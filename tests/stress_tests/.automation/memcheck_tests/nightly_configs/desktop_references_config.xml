<?xml version="1.0"?>
<attributes>
    <models>
        # Values from {"commit_id": "a92a737ba0ce5afd6df2da8d80c8deec7e11b1d9", "commit_date": "2021-07-16 13:30"} and *= 1.3
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="904274" vmpeak="904274" vmrss="33446" vmhwm="33446" />
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1843992" vmpeak="1869348" vmrss="272173" vmhwm="444511" />
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="904295" vmpeak="904295" vmrss="33576" vmhwm="33576" />
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="1928664" vmpeak="2013861" vmrss="272994" vmhwm="444870" />
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1012648" vmpeak="1012648" vmrss="38703" vmhwm="38703" />
        <model path="intel/age-gender-recognition-retail-0013/FP16-INT8/age-gender-recognition-retail-0013.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2025036" vmpeak="2025036" vmrss="275095" vmhwm="444948" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="953472" vmpeak="953472" vmrss="46971" vmhwm="46971" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1922746" vmpeak="1923162" vmrss="593216" vmhwm="972977" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="957413" vmpeak="957413" vmrss="53492" vmhwm="53492" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2018380" vmpeak="2103576" vmrss="583185" vmhwm="947455" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1108400" vmpeak="1112337" vmrss="68260" vmhwm="68260" />
        <model path="intel/face-detection-adas-0001/FP16-INT8/face-detection-adas-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2153018" vmpeak="2238215" vmrss="682832" vmhwm="975738" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="2671302" vmpeak="2671302" vmrss="204802" vmhwm="204802" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="4467044" vmpeak="4467044" vmrss="3262246" vmhwm="3262246" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="2869167" vmpeak="2953158" vmrss="1736451" vmhwm="1736451" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="4556806" vmpeak="4642003" vmrss="3289228" vmhwm="3289228" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="4667946" vmpeak="4750439" vmrss="1837128" vmhwm="1837128" />
        <model path="intel/faster-rcnn-resnet101-coco-sparse-60-0001/FP16-INT8/faster-rcnn-resnet101-coco-sparse-60-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="6565946" vmpeak="6651143" vmrss="5227102" vmhwm="5227102" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="939816" vmpeak="984178" vmrss="46618" vmhwm="46618" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1939158" vmpeak="1943188" vmrss="485128" vmhwm="767988" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="941621" vmpeak="941621" vmrss="51168" vmhwm="51168" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2018276" vmpeak="2103472" vmrss="477120" vmhwm="744447" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1259580" vmpeak="1344184" vmrss="66222" vmhwm="66222" />
        <model path="intel/human-pose-estimation-0001/FP16-INT8/human-pose-estimation-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2139228" vmpeak="2224424" vmrss="507405" vmhwm="764634" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="938875" vmpeak="938875" vmrss="44480" vmhwm="44480" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1925804" vmpeak="1949838" vmrss="597656" vmhwm="1006558" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="939666" vmpeak="939666" vmrss="45905" vmhwm="45905" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2010569" vmpeak="2095766" vmrss="603959" vmhwm="982862" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1077174" vmpeak="1077944" vmrss="57236" vmhwm="57236" />
        <model path="intel/image-retrieval-0001/FP16-INT8/image-retrieval-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2110050" vmpeak="2195247" vmrss="605082" vmhwm="977121" />
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="905855" vmpeak="991052" vmrss="35001" vmhwm="35001" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1845022" vmpeak="1871490" vmrss="229418" vmhwm="344151" />
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="903156" vmpeak="974376" vmrss="27279" vmhwm="27279" />
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="1930765" vmpeak="2015962" vmrss="230198" vmhwm="344370" />
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1002034" vmpeak="1002034" vmrss="28922" vmhwm="28922" />
        <model path="intel/landmarks-regression-retail-0009/FP16-INT8/landmarks-regression-retail-0009.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2032295" vmpeak="2117492" vmrss="230531" vmhwm="345342" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="913338" vmpeak="981167" vmrss="36415" vmhwm="36415" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1899773" vmpeak="1904687" vmrss="354510" vmhwm="555734" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="913738" vmpeak="981167" vmrss="41033" vmhwm="41033" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="1984772" vmpeak="1984772" vmrss="379402" vmhwm="580434" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1030702" vmpeak="1030702" vmrss="46139" vmhwm="46139" />
        <model path="intel/license-plate-recognition-barrier-0001/FP16-INT8/license-plate-recognition-barrier-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2059610" vmpeak="2144807" vmrss="402916" vmhwm="579800" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="961485" vmpeak="981120" vmrss="42525" vmhwm="42525" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1934670" vmpeak="1957597" vmrss="642756" vmhwm="1140750" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="961500" vmpeak="961500" vmrss="44392" vmhwm="44392" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2019097" vmpeak="2019097" vmrss="619580" vmhwm="1136980" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1122565" vmpeak="1122565" vmrss="54464" vmhwm="54464" />
        <model path="intel/person-attributes-recognition-crossroad-0230/FP16-INT8/person-attributes-recognition-crossroad-0230.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2115266" vmpeak="2115266" vmrss="647426" vmhwm="1134718" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1074814" vmpeak="1074814" vmrss="88452" vmhwm="88452" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2743062" vmpeak="2763290" vmrss="1681264" vmhwm="2056142" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1078911" vmpeak="1078911" vmrss="93225" vmhwm="93225" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2829013" vmpeak="2914210" vmrss="1684462" vmhwm="2072168" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1331330" vmpeak="1335474" vmrss="126781" vmhwm="126781" />
        <model path="intel/person-detection-action-recognition-0005/FP16-INT8/person-detection-action-recognition-0005.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2959158" vmpeak="3044355" vmrss="1721330" vmhwm="2066324" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1093856" vmpeak="1093856" vmrss="91852" vmhwm="91852" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="3026670" vmpeak="3046903" vmrss="1968860" vmhwm="2354736" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1098016" vmpeak="1098016" vmrss="102190" vmhwm="102190" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="3112090" vmpeak="3197287" vmrss="1981917" vmhwm="2363608" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1367927" vmpeak="1372072" vmrss="137696" vmhwm="137696" />
        <model path="intel/person-detection-action-recognition-0006/FP16-INT8/person-detection-action-recognition-0006.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="3240198" vmpeak="3240198" vmrss="2008375" vmhwm="2362521" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1074772" vmpeak="1074772" vmrss="87604" vmhwm="87604" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2742277" vmpeak="2762510" vmrss="1678019" vmhwm="2070738" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1078958" vmpeak="1078958" vmrss="93584" vmhwm="93584" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2822476" vmpeak="2907673" vmrss="1663017" vmhwm="2058227" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1512357" vmpeak="1597346" vmrss="126666" vmhwm="126666" />
        <model path="intel/person-detection-action-recognition-teacher-0002/FP16-INT8/person-detection-action-recognition-teacher-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2953698" vmpeak="3038895" vmrss="1709531" vmhwm="2073838" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1069603" vmpeak="1069603" vmrss="79658" vmhwm="79658" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2425540" vmpeak="2445773" vmrss="1316775" vmhwm="1781587" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1069629" vmpeak="1069629" vmrss="80371" vmhwm="80371" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2511896" vmpeak="2597093" vmrss="1351994" vmhwm="1781400" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1316905" vmpeak="1316905" vmrss="104332" vmhwm="104332" />
        <model path="intel/person-detection-asl-0001/FP16-INT8/person-detection-asl-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2607451" vmpeak="2607451" vmrss="1374937" vmhwm="1782138" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1079280" vmpeak="1079280" vmrss="87469" vmhwm="87469" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2739271" vmpeak="2759499" vmrss="1667624" vmhwm="2055014" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1083425" vmpeak="1083425" vmrss="92372" vmhwm="92372" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2819606" vmpeak="2819606" vmrss="1679277" vmhwm="2052653" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1341215" vmpeak="1345359" vmrss="127389" vmhwm="127389" />
        <model path="intel/person-detection-raisinghand-recognition-0001/FP16-INT8/person-detection-raisinghand-recognition-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2951806" vmpeak="3037002" vmrss="1709702" vmhwm="2068190" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1016345" vmpeak="1016345" vmrss="65041" vmhwm="65041" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2129472" vmpeak="2134132" vmrss="973185" vmhwm="1436812" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1024571" vmpeak="1024571" vmrss="84791" vmhwm="84791" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2228007" vmpeak="2228007" vmrss="987209" vmhwm="1437711" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1415507" vmpeak="1500704" vmrss="109579" vmhwm="109636" />
        <model path="intel/person-detection-retail-0002/FP16-INT8/person-detection-retail-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2375094" vmpeak="2460291" vmrss="1038752" vmhwm="1435631" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="984973" vmpeak="988312" vmrss="57933" vmhwm="57933" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2046600" vmpeak="2070213" vmrss="776110" vmhwm="1239456" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="987630" vmpeak="987630" vmrss="60923" vmhwm="60923" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2131043" vmpeak="2131043" vmrss="773370" vmhwm="1232680" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1164971" vmpeak="1167628" vmrss="80022" vmhwm="80022" />
        <model path="intel/person-detection-retail-0013/FP16-INT8/person-detection-retail-0013.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2235350" vmpeak="2235350" vmrss="778523" vmhwm="1235161" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1016012" vmpeak="1016012" vmrss="84682" vmhwm="84682" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2086245" vmpeak="2088216" vmrss="855353" vmhwm="1342120" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1031976" vmpeak="1031976" vmrss="105632" vmhwm="105632" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2190994" vmpeak="2276190" vmrss="866829" vmhwm="1328501" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1235972" vmpeak="1251952" vmrss="149141" vmhwm="149141" />
        <model path="intel/person-vehicle-bike-detection-crossroad-0078/FP16-INT8/person-vehicle-bike-detection-crossroad-0078.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2334618" vmpeak="2334618" vmrss="939302" vmhwm="1331090" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="958713" vmpeak="958713" vmrss="55172" vmhwm="55172" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1970654" vmpeak="1970654" vmrss="580944" vmhwm="847376" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="962722" vmpeak="962722" vmrss="65306" vmhwm="65306" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2060442" vmpeak="2145639" vmrss="583564" vmhwm="850070" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1115462" vmpeak="1119461" vmrss="83964" vmhwm="83964" />
        <model path="intel/person-vehicle-bike-detection-crossroad-1016/FP16-INT8/person-vehicle-bike-detection-crossroad-1016.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2205117" vmpeak="2205117" vmrss="637000" vmhwm="843008" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1106752" vmpeak="1106752" vmrss="61453" vmhwm="61453" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2191248" vmpeak="2191248" vmrss="620916" vmhwm="620916" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1171903" vmpeak="1171903" vmrss="293108" vmhwm="293108" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2341128" vmpeak="2426325" vmrss="675906" vmhwm="675906" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1664858" vmpeak="1750054" vmrss="328120" vmhwm="329108" />
        <model path="intel/single-image-super-resolution-1032/FP16-INT8/single-image-super-resolution-1032.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2808405" vmpeak="2862012" vmrss="1053499" vmhwm="1053499" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1313826" vmpeak="1399023" vmrss="142360" vmhwm="142360" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2452366" vmpeak="2452366" vmrss="868036" vmhwm="868036" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1327289" vmpeak="1456114" vmrss="268013" vmhwm="268013" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2532270" vmpeak="2617466" vmrss="867344" vmhwm="867344" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1587289" vmpeak="1740538" vmrss="304246" vmhwm="304480" />
        <model path="intel/unet-camvid-onnx-0001/FP16-INT8/unet-camvid-onnx-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="3006556" vmpeak="3083678" vmrss="1228546" vmhwm="1228546" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="900432" vmpeak="975312" vmrss="28756" vmhwm="28756" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1859228" vmpeak="1882197" vmrss="244883" vmhwm="410867" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="900432" vmpeak="900432" vmrss="29312" vmhwm="29312" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="1940302" vmpeak="2025498" vmrss="247665" vmhwm="413498" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1006241" vmpeak="1006241" vmrss="30877" vmhwm="30877" />
        <model path="intel/vehicle-attributes-recognition-barrier-0039/FP16-INT8/vehicle-attributes-recognition-barrier-0039.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2040506" vmpeak="2040506" vmrss="248866" vmhwm="411886" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="946394" vmpeak="946394" vmrss="46238" vmhwm="46238" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1912024" vmpeak="1912497" vmrss="573518" vmhwm="914555" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="950320" vmpeak="982436" vmrss="54329" vmhwm="54329" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2004069" vmpeak="2089266" vmrss="560768" vmhwm="867037" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1094984" vmpeak="1098921" vmrss="68026" vmhwm="68026" />
        <model path="intel/vehicle-detection-adas-0002/FP16-INT8/vehicle-detection-adas-0002.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2144334" vmpeak="2229531" vmrss="605508" vmhwm="884655" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1045829" vmpeak="1045829" vmrss="161018" vmhwm="161018" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2375412" vmpeak="2411536" vmrss="751181" vmhwm="1053364" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1229524" vmpeak="1229524" vmrss="169993" vmhwm="169993" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2460582" vmpeak="2545779" vmrss="787394" vmhwm="1060560" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1416365" vmpeak="1501562" vmrss="241025" vmhwm="241025" />
        <model path="intel/yolo-v2-ava-0001/FP16-INT8/yolo-v2-ava-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2556434" vmpeak="2641631" vmrss="810035" vmhwm="1051336" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="1045834" vmpeak="1045834" vmrss="160570" vmhwm="160570" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="2375386" vmpeak="2411510" vmrss="777675" vmhwm="1048928" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1048491" vmpeak="1120397" vmrss="169426" vmhwm="169426" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2460619" vmpeak="2545816" vmrss="778388" vmhwm="1051492" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1416521" vmpeak="1501718" vmrss="240994" vmhwm="240994" />
        <model path="intel/yolo-v2-ava-sparse-35-0001/FP16-INT8/yolo-v2-ava-sparse-35-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2556470" vmpeak="2641667" vmrss="793582" vmhwm="1049058" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="CPU" vmsize="941803" vmpeak="941803" vmrss="69201" vmhwm="69201" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="create_exenetwork" device="GPU" vmsize="1983768" vmpeak="2007907" vmrss="381274" vmhwm="472711" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="CPU" vmsize="1125498" vmpeak="1210695" vmrss="73169" vmhwm="73169" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="infer_request_inference" device="GPU" vmsize="2068965" vmpeak="2068965" vmrss="382564" vmhwm="473543" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="CPU" vmsize="1072890" vmpeak="1075531" vmrss="98389" vmhwm="98389" />
        <model path="intel/yolo-v2-tiny-ava-0001/FP16-INT8/yolo-v2-tiny-ava-0001.xml" precision="FP16-INT8" test="inference_with_streams" device="GPU" vmsize="2164817" vmpeak="2250014" vmrss="385455" vmhwm="473413" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1287322" vmpeak="1287322" vmrss="405485" vmhwm="405485" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1996836" vmpeak="2081289" vmrss="426134" vmhwm="592046" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1287722" vmpeak="1287722" vmrss="408948" vmhwm="408948" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2086104" vmpeak="2171301" vmrss="448952" vmhwm="617328" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1606805" vmpeak="1691601" vmrss="433867" vmhwm="433867" />
        <model path="public/Sphereface/FP16/Sphereface.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2175836" vmpeak="2261032" vmrss="406463" vmhwm="568890" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1138176" vmpeak="1138176" vmrss="255710" vmhwm="255710" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2148281" vmpeak="2311311" vmrss="564454" vmhwm="796863" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1319630" vmpeak="1404426" vmrss="259636" vmhwm="259636" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2235058" vmpeak="2320255" vmrss="543436" vmhwm="767738" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1554867" vmpeak="1639664" vmrss="376485" vmhwm="376485" />
        <model path="public/Sphereface/FP32/Sphereface.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2330010" vmpeak="2415207" vmrss="545615" vmhwm="771565" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1866129" vmpeak="1866129" vmrss="994520" vmhwm="994520" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2319610" vmpeak="2480446" vmrss="708827" vmhwm="848853" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1867335" vmpeak="1867335" vmrss="998124" vmhwm="998124" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2404807" vmpeak="2490004" vmrss="709701" vmhwm="849154" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1980446" vmpeak="1981231" vmrss="1009444" vmhwm="1009444" />
        <model path="public/alexnet/FP16/alexnet.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2596864" vmpeak="2682061" vmrss="807097" vmhwm="852155" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1410427" vmpeak="1410427" vmrss="538699" vmhwm="538699" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2710333" vmpeak="3100749" vmrss="1079306" vmhwm="1440296" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1592687" vmpeak="1677483" vmrss="542739" vmhwm="542739" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2794318" vmpeak="3100702" vmrss="1079868" vmhwm="1441736" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2097404" vmpeak="2098189" vmrss="855550" vmhwm="855550" />
        <model path="public/alexnet/FP32/alexnet.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3103984" vmpeak="3196575" vmrss="1297790" vmhwm="1441346" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="2756067" vmpeak="2756067" vmrss="728400" vmhwm="728400" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="3656920" vmpeak="3656920" vmrss="2576542" vmhwm="2576542" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="3029384" vmpeak="3029384" vmrss="1885780" vmhwm="1885780" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="3741670" vmpeak="3826867" vmrss="2550085" vmhwm="2633061" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="4408804" vmpeak="4494011" vmrss="2004210" vmhwm="2004210" />
        <model path="public/brain-tumor-segmentation-0001/FP16/brain-tumor-segmentation-0001.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="5380268" vmpeak="5465480" vmrss="4111598" vmhwm="4196514" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="2514335" vmpeak="2514335" vmrss="485040" vmhwm="485040" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="5519477" vmpeak="5519477" vmrss="4423853" vmhwm="4423853" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="2787652" vmpeak="2872849" vmrss="1642518" vmhwm="1642518" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="5608704" vmpeak="5693901" vmrss="4406594" vmhwm="4482212" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="4765904" vmpeak="4851111" vmrss="1908644" vmhwm="1908644" />
        <model path="public/brain-tumor-segmentation-0001/FP32/brain-tumor-segmentation-0001.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="8912529" vmpeak="8997742" vmrss="7661456" vmhwm="7746377" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1910786" vmpeak="1910786" vmrss="146354" vmhwm="146354" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2791230" vmpeak="2791230" vmrss="1513553" vmhwm="1513553" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="2173454" vmpeak="2258651" vmrss="1042849" vmhwm="1042849" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2878990" vmpeak="2964187" vmrss="1505805" vmhwm="1590258" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="3238690" vmpeak="3323886" vmrss="1112503" vmhwm="1112836" />
        <model path="public/brain-tumor-segmentation-0002/FP16/brain-tumor-segmentation-0002.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="3918012" vmpeak="4003225" vmrss="2457884" vmhwm="2542716" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1894162" vmpeak="1894162" vmrss="128939" vmhwm="128939" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="3650873" vmpeak="3650873" vmrss="2354716" vmhwm="2354716" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="2156814" vmpeak="2245588" vmrss="1026880" vmhwm="1026880" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="3731681" vmpeak="3816878" vmrss="2366618" vmhwm="2449782" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="3228763" vmpeak="3313960" vmrss="1102192" vmhwm="1102805" />
        <model path="public/brain-tumor-segmentation-0002/FP32/brain-tumor-segmentation-0002.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="5584758" vmpeak="5669970" vmrss="4130126" vmhwm="4215234" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1863622" vmpeak="1863622" vmrss="992560" vmhwm="992560" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2320214" vmpeak="2481050" vmrss="708780" vmhwm="851458" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1864834" vmpeak="1864834" vmrss="996335" vmhwm="996335" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2405312" vmpeak="2490508" vmrss="707028" vmhwm="850777" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1979062" vmpeak="2147272" vmrss="1008898" vmhwm="1008898" />
        <model path="public/caffenet/FP16/caffenet.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2596859" vmpeak="2682056" vmrss="806967" vmhwm="850548" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1409730" vmpeak="1409730" vmrss="539401" vmhwm="539401" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2708596" vmpeak="3099012" vmrss="1056333" vmhwm="1442147" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1410942" vmpeak="1410942" vmrss="542209" vmhwm="542209" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2794235" vmpeak="3100619" vmrss="1080539" vmhwm="1441460" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1824440" vmpeak="1825226" vmrss="854365" vmhwm="854365" />
        <model path="public/caffenet/FP32/caffenet.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3104025" vmpeak="3196616" vmrss="1295985" vmhwm="1441315" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1240943" vmpeak="1240943" vmrss="304064" vmhwm="304064" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2038566" vmpeak="2087264" vmrss="680274" vmhwm="1115956" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1432990" vmpeak="1432990" vmrss="355217" vmhwm="355217" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2132109" vmpeak="2217306" vmrss="693284" vmhwm="1114786" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1667463" vmpeak="1752327" vmrss="421470" vmhwm="421470" />
        <model path="public/ctdet_coco_dlav0_512/FP16/ctdet_coco_dlav0_512.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2342012" vmpeak="2420553" vmrss="805828" vmhwm="1105780" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1157894" vmpeak="1157894" vmrss="220766" vmhwm="220766" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2254428" vmpeak="2254428" vmrss="912740" vmhwm="1246185" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1349608" vmpeak="1481355" vmrss="270696" vmhwm="270696" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2345886" vmpeak="2345886" vmrss="888997" vmhwm="1246304" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1622353" vmpeak="1707550" vmrss="375980" vmhwm="375980" />
        <model path="public/ctdet_coco_dlav0_512/FP32/ctdet_coco_dlav0_512.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2678161" vmpeak="2678161" vmrss="1159002" vmhwm="1246637" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1704944" vmpeak="1704944" vmrss="344167" vmhwm="344167" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2239671" vmpeak="2239671" vmrss="1230164" vmhwm="1509679" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1899924" vmpeak="1985120" vmrss="603870" vmhwm="603870" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2347129" vmpeak="2432326" vmrss="1235364" vmhwm="1524920" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="2518651" vmpeak="2601591" vmrss="628570" vmhwm="628570" />
        <model path="public/ctpn/FP16/ctpn.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2653326" vmpeak="2738522" vmrss="1462136" vmhwm="1506434" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1579780" vmpeak="1579780" vmrss="218894" vmhwm="218894" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2561416" vmpeak="2561416" vmrss="1534660" vmhwm="1656262" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1774312" vmpeak="1851512" vmrss="478093" vmhwm="478093" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2645666" vmpeak="2730863" vmrss="1534322" vmhwm="1645165" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2383867" vmpeak="2454660" vmrss="582306" vmhwm="582306" />
        <model path="public/ctpn/FP32/ctpn.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3192025" vmpeak="3276665" vmrss="2009014" vmhwm="2009014" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1136496" vmpeak="1136496" vmrss="174028" vmhwm="174028" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1970009" vmpeak="2004932" vmrss="781055" vmhwm="1310488" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1137245" vmpeak="1137245" vmrss="185198" vmhwm="185198" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2057889" vmpeak="2143086" vmrss="762236" vmhwm="1319926" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1395742" vmpeak="1396512" vmrss="223568" vmhwm="223589" />
        <model path="public/densenet-121/FP16/densenet-121.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2188004" vmpeak="2188004" vmrss="816322" vmhwm="1322646" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1103949" vmpeak="1103949" vmrss="129329" vmhwm="129329" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2045581" vmpeak="2069397" vmrss="834246" vmhwm="1364038" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1285767" vmpeak="1370964" vmrss="140966" vmhwm="140966" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2131896" vmpeak="2217092" vmrss="850460" vmhwm="1368036" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1370922" vmpeak="1371692" vmrss="202976" vmhwm="203013" />
        <model path="public/densenet-121/FP32/densenet-121.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2300828" vmpeak="2386025" vmrss="909090" vmhwm="1382570" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1019512" vmpeak="1019512" vmrss="108758" vmhwm="108758" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1955226" vmpeak="1968257" vmrss="672531" vmhwm="1135612" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1020682" vmpeak="1020682" vmrss="116802" vmhwm="116802" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2016086" vmpeak="2101283" vmrss="669765" vmhwm="1141275" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1200732" vmpeak="1201501" vmrss="156202" vmhwm="156202" />
        <model path="public/efficientnet-b0/FP16/efficientnet-b0.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2135697" vmpeak="2220894" vmrss="657997" vmhwm="1135056" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1004827" vmpeak="1088048" vmrss="93022" vmhwm="93022" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1967732" vmpeak="2008494" vmrss="698916" vmhwm="1172537" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1005960" vmpeak="1005960" vmrss="103516" vmhwm="103516" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2066838" vmpeak="2152035" vmrss="723502" vmhwm="1159958" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1188699" vmpeak="1189468" vmrss="143130" vmhwm="143130" />
        <model path="public/efficientnet-b0/FP32/efficientnet-b0.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2172851" vmpeak="2258048" vmrss="770874" vmhwm="1188517" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="2888724" vmpeak="2888724" vmrss="965489" vmhwm="965489" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="5872651" vmpeak="5872651" vmrss="4717414" vmhwm="4717414" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="3086844" vmpeak="3154975" vmrss="1942470" vmhwm="1942470" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="5979355" vmpeak="6064552" vmrss="4755571" vmhwm="4755571" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="4607044" vmpeak="4610559" vmrss="2155514" vmhwm="2155514" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP16/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="9583225" vmpeak="9668422" vmrss="8279850" vmhwm="8279850" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="2632042" vmpeak="2632042" vmrss="670566" vmhwm="670566" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="9593402" vmpeak="9593402" vmrss="8055205" vmhwm="8055205" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="3011106" vmpeak="3088722" vmrss="1648244" vmhwm="1648244" />
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="9599023" vmpeak="9599023" vmrss="8026049" vmhwm="8026049" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32/faster_rcnn_inception_resnet_v2_atrous_coco.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="4363980" vmpeak="4435730" vmrss="2004262" vmhwm="2004262" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1020562" vmpeak="1024306" vmrss="119329" vmhwm="119329" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1904156" vmpeak="1934992" vmrss="474572" vmhwm="807986" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1202307" vmpeak="1202307" vmrss="124997" vmhwm="124997" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1988771" vmpeak="2073968" vmrss="482253" vmhwm="819291" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1194705" vmpeak="1195474" vmrss="166561" vmhwm="166561" />
        <model path="public/googlenet-v1-tf/FP16/googlenet-v1-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2087311" vmpeak="2087311" vmrss="458333" vmhwm="782272" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1003256" vmpeak="1008779" vmrss="101306" vmhwm="101306" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1948325" vmpeak="1987351" vmrss="546041" vmhwm="854984" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1185074" vmpeak="1270271" vmrss="107614" vmhwm="107614" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2030236" vmpeak="2115432" vmrss="523603" vmhwm="833918" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1179198" vmpeak="1179968" vmrss="151647" vmhwm="151647" />
        <model path="public/googlenet-v1-tf/FP32/googlenet-v1-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2143554" vmpeak="2143554" vmrss="514124" vmhwm="825801" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1027041" vmpeak="1027041" vmrss="123994" vmhwm="123994" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2018848" vmpeak="2139269" vmrss="509132" vmhwm="975197" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1028274" vmpeak="1028274" vmrss="133806" vmhwm="133806" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2102547" vmpeak="2187744" vmrss="519095" vmhwm="977927" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1204424" vmpeak="1238369" vmrss="176784" vmhwm="176784" />
        <model path="public/googlenet-v1/FP16/googlenet-v1.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2203011" vmpeak="2288208" vmrss="598343" vmhwm="1051741" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1008170" vmpeak="1008170" vmrss="106527" vmhwm="106527" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1970997" vmpeak="2018832" vmrss="560450" vmhwm="921700" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1099888" vmpeak="1183915" vmrss="114956" vmhwm="114956" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2056111" vmpeak="2141308" vmrss="578531" vmhwm="938085" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1368848" vmpeak="1453644" vmrss="159276" vmhwm="159276" />
        <model path="public/googlenet-v1/FP32/googlenet-v1.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2170147" vmpeak="2255344" vmrss="600688" vmhwm="922636" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1090684" vmpeak="1090684" vmrss="181376" vmhwm="181376" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1970612" vmpeak="2014734" vmrss="579800" vmhwm="950300" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1091750" vmpeak="1091750" vmrss="189462" vmhwm="189462" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2054967" vmpeak="2140164" vmrss="548147" vmhwm="932672" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1296131" vmpeak="1323472" vmrss="252777" vmhwm="252777" />
        <model path="public/googlenet-v2/FP16/googlenet-v2.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2149940" vmpeak="2235136" vmrss="566914" vmhwm="933862" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1059411" vmpeak="1059411" vmrss="149765" vmhwm="149765" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2039741" vmpeak="2109894" vmrss="645907" vmhwm="1041768" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1060560" vmpeak="1060560" vmrss="156156" vmhwm="156156" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2124506" vmpeak="2209703" vmrss="647535" vmhwm="1032413" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1449567" vmpeak="1534364" vmrss="226522" vmhwm="226522" />
        <model path="public/googlenet-v2/FP32/googlenet-v2.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2226884" vmpeak="2312081" vmrss="659822" vmhwm="1061918" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1277603" vmpeak="1277603" vmrss="354759" vmhwm="354759" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2023829" vmpeak="2100358" vmrss="709248" vmhwm="1212140" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1460087" vmpeak="1545284" vmrss="364660" vmhwm="364660" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2108137" vmpeak="2193334" vmrss="741785" vmhwm="1220689" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1559298" vmpeak="1560660" vmrss="489840" vmhwm="489840" />
        <model path="public/googlenet-v3/FP16/googlenet-v3.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2204576" vmpeak="2289773" vmrss="728457" vmhwm="1216612" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1207533" vmpeak="1207533" vmrss="284746" vmhwm="284746" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2204545" vmpeak="2340260" vmrss="852732" vmhwm="1390688" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1389928" vmpeak="1475125" vmrss="295583" vmhwm="295583" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2289170" vmpeak="2374366" vmrss="848369" vmhwm="1366565" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1680666" vmpeak="1770605" vmrss="431324" vmhwm="431324" />
        <model path="public/googlenet-v3/FP32/googlenet-v3.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2383622" vmpeak="2434140" vmrss="879502" vmhwm="1386756" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1644281" vmpeak="1644281" vmrss="694304" vmhwm="694304" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2247502" vmpeak="2375058" vmrss="979690" vmhwm="1670812" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1827103" vmpeak="1911899" vmrss="706139" vmhwm="706139" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2313381" vmpeak="2398578" vmrss="991702" vmhwm="1650662" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="2143013" vmpeak="2227810" vmrss="837517" vmhwm="837517" />
        <model path="public/googlenet-v4-tf/FP16/googlenet-v4-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2421229" vmpeak="2506426" vmrss="1040390" vmhwm="1675710" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1429204" vmpeak="1429204" vmrss="476585" vmhwm="476585" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2557022" vmpeak="2771116" vmrss="1247350" vmhwm="1980596" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1612015" vmpeak="1695449" vmrss="489678" vmhwm="489678" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2641761" vmpeak="2770398" vmrss="1257432" vmhwm="2000018" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2034879" vmpeak="2036242" vmrss="729263" vmhwm="729263" />
        <model path="public/googlenet-v4-tf/FP32/googlenet-v4-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2742147" vmpeak="2870784" vmrss="1351552" vmhwm="1997351" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1333212" vmpeak="1333212" vmrss="270680" vmhwm="270680" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2270444" vmpeak="2270444" vmrss="920306" vmhwm="920306" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1484132" vmpeak="1508936" vmrss="494598" vmhwm="494598" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2364554" vmpeak="2449751" vmrss="870495" vmhwm="990854" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1982812" vmpeak="2068008" vmrss="611743" vmhwm="611894" />
        <model path="public/i3d-rgb-tf/FP16/i3d-rgb-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2796128" vmpeak="2916934" vmrss="1274873" vmhwm="1395160" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1284634" vmpeak="1284634" vmrss="221514" vmhwm="221514" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2632203" vmpeak="2632203" vmrss="1283084" vmhwm="1283084" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1526080" vmpeak="1611277" vmrss="446622" vmhwm="446622" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2712845" vmpeak="2798042" vmrss="1197237" vmhwm="1316988" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2132785" vmpeak="2217982" vmrss="579602" vmhwm="579852" />
        <model path="public/i3d-rgb-tf/FP32/i3d-rgb-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3444448" vmpeak="3565255" vmrss="1943104" vmhwm="2063417" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="3312493" vmpeak="3312493" vmrss="1124588" vmhwm="1124588" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="3757936" vmpeak="3757936" vmrss="2413398" vmhwm="2413398" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="3567012" vmpeak="3645153" vmrss="2452902" vmhwm="2452902" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="3867323" vmpeak="3952520" vmrss="2464243" vmhwm="2464243" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="5164328" vmpeak="5231038" vmrss="2626348" vmhwm="2626348" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP16/mask_rcnn_resnet101_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="5571659" vmpeak="5656856" vmrss="4054133" vmhwm="4054138" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP32/mask_rcnn_resnet101_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="2876744" vmpeak="2876744" vmrss="685786" vmhwm="685786" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP32/mask_rcnn_resnet101_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="4934425" vmpeak="5609838" vmrss="3672682" vmhwm="4177638" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP32/mask_rcnn_resnet101_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="3493204" vmpeak="3571344" vmrss="2016596" vmhwm="2016596" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP32/mask_rcnn_resnet101_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="5044150" vmpeak="5617721" vmrss="3713075" vmhwm="4196649" />
        <model path="public/mask_rcnn_resnet101_atrous_coco/FP32/mask_rcnn_resnet101_atrous_coco.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="4951252" vmpeak="5017812" vmrss="2413538" vmhwm="2413538" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1035829" vmpeak="1035829" vmrss="132184" vmhwm="132184" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1901702" vmpeak="1926574" vmrss="480537" vmhwm="811179" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1037186" vmpeak="1037186" vmrss="145537" vmhwm="145537" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1994184" vmpeak="2079381" vmrss="458562" vmhwm="799078" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1366986" vmpeak="1401233" vmrss="158750" vmhwm="158750" />
        <model path="public/mobilenet-ssd/FP16/mobilenet-ssd.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2089396" vmpeak="2174593" vmrss="471005" vmhwm="804117" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1021680" vmpeak="1021680" vmrss="117878" vmhwm="117878" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1947420" vmpeak="1970098" vmrss="487349" vmhwm="796473" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1023115" vmpeak="1023115" vmrss="131268" vmhwm="131268" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2028780" vmpeak="2113976" vmrss="477536" vmhwm="795938" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1353918" vmpeak="1473362" vmrss="145662" vmhwm="145662" />
        <model path="public/mobilenet-ssd/FP32/mobilenet-ssd.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2149472" vmpeak="2234668" vmrss="522152" vmhwm="796832" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="966716" vmpeak="966716" vmrss="80896" vmhwm="80896" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1866378" vmpeak="1891947" vmrss="354161" vmhwm="524466" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="967844" vmpeak="1005581" vmrss="89518" vmhwm="89518" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1951846" vmpeak="1951846" vmrss="326367" vmhwm="496823" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1109695" vmpeak="1110465" vmrss="115767" vmhwm="115767" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP16/mobilenet-v1-1.0-224-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2052065" vmpeak="2052065" vmrss="332602" vmhwm="494494" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="956857" vmpeak="956857" vmrss="70714" vmhwm="70714" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1896674" vmpeak="1930167" vmrss="359964" vmhwm="548329" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="957647" vmpeak="957647" vmrss="78270" vmhwm="78270" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="1982448" vmpeak="1982448" vmrss="335602" vmhwm="519755" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1099836" vmpeak="1132752" vmrss="105830" vmhwm="105830" />
        <model path="public/mobilenet-v1-1.0-224-tf/FP32/mobilenet-v1-1.0-224-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2096936" vmpeak="2182133" vmrss="353860" vmhwm="522808" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1016121" vmpeak="1016121" vmrss="114899" vmhwm="114899" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1903532" vmpeak="1928175" vmrss="423872" vmhwm="705432" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1016891" vmpeak="1016891" vmrss="126323" vmhwm="126323" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1986561" vmpeak="1986561" vmrss="428298" vmhwm="708520" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1182480" vmpeak="1183249" vmrss="160056" vmhwm="160056" />
        <model path="public/mobilenet-v2-1.4-224/FP16/mobilenet-v2-1.4-224.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2095542" vmpeak="2180739" vmrss="431589" vmhwm="724750" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="996309" vmpeak="1005633" vmrss="94083" vmhwm="94083" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1933781" vmpeak="1964430" vmrss="456898" vmhwm="702426" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="997084" vmpeak="997084" vmrss="107166" vmhwm="107166" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2017215" vmpeak="2102412" vmrss="454734" vmhwm="708734" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1168440" vmpeak="1169209" vmrss="145371" vmhwm="145371" />
        <model path="public/mobilenet-v2-1.4-224/FP32/mobilenet-v2-1.4-224.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2142561" vmpeak="2142561" vmrss="492315" vmhwm="703684" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="989924" vmpeak="989924" vmrss="88706" vmhwm="88706" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1882920" vmpeak="1908389" vmrss="447792" vmhwm="701485" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="990797" vmpeak="990797" vmrss="97021" vmhwm="97021" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1970982" vmpeak="2056178" vmrss="403280" vmhwm="699930" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1132965" vmpeak="1133735" vmrss="108435" vmhwm="108435" />
        <model path="public/mobilenet-v2/FP16/mobilenet-v2.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2069516" vmpeak="2154713" vmrss="465301" vmhwm="732950" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="968162" vmpeak="1077648" vmrss="66461" vmhwm="66461" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1912648" vmpeak="1934119" vmrss="476044" vmhwm="718510" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="968910" vmpeak="992430" vmrss="75238" vmhwm="75238" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="1999504" vmpeak="2084700" vmrss="457693" vmhwm="690658" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1125358" vmpeak="1164862" vmrss="101790" vmhwm="101790" />
        <model path="public/mobilenet-v2/FP32/mobilenet-v2.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2113342" vmpeak="2198539" vmrss="474572" vmhwm="694558" />
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="904685" vmpeak="904685" vmrss="38922" vmhwm="38922" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1858521" vmpeak="1884745" vmrss="294855" vmhwm="439420" />
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="900504" vmpeak="900504" vmrss="30908" vmhwm="30908" />
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1943931" vmpeak="2029128" vmrss="293878" vmhwm="437933" />
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1010781" vmpeak="1010781" vmrss="42900" vmhwm="42900" /> # Values from {"commit_id": "d6bcfb7b8f3b65b79f5016d4c8351962dd03bc9e", "commit_date": "2022-01-21 14:12"} and *= 1.3
        <model path="public/mtcnn/mtcnn-o/FP16/mtcnn-o.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2039954" vmpeak="2125151" vmrss="294184" vmhwm="437283" />
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="906594" vmpeak="906594" vmrss="36088" vmhwm="36088" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1877002" vmpeak="1899934" vmrss="273171" vmhwm="373313" />
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="907124" vmpeak="907124" vmrss="38651" vmhwm="38651" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="1962100" vmpeak="2047297" vmrss="297538" vmhwm="397456" />
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1007151" vmpeak="1007151" vmrss="42458" vmhwm="42458" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="public/mtcnn/mtcnn-o/FP32/mtcnn-o.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2057952" vmpeak="2143148" vmrss="296623" vmhwm="396120" />
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="901539" vmpeak="901539" vmrss="34528" vmhwm="34528" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1871490" vmpeak="1895717" vmrss="286785" vmhwm="382896" />
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="893791" vmpeak="893791" vmrss="27773" vmhwm="27773" />
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1954758" vmpeak="2039954" vmrss="286015" vmhwm="382257" />
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1001962" vmpeak="1001962" vmrss="36218" vmhwm="36218" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mtcnn/mtcnn-r/FP16/mtcnn-r.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2052585" vmpeak="2052585" vmrss="287752" vmhwm="383385" />
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="892860" vmpeak="973684" vmrss="26197" vmhwm="26197" />
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1878266" vmpeak="1898499" vmrss="262189" vmhwm="337875" />
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="901732" vmpeak="901732" vmrss="34086" vmhwm="34086" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="1963353" vmpeak="1963353" vmrss="260572" vmhwm="336034" />
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1456457" vmpeak="1541654" vmrss="28574" vmhwm="28574" />
        <model path="public/mtcnn/mtcnn-r/FP32/mtcnn-r.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2059200" vmpeak="2144396" vmrss="263166" vmhwm="337578" />
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="2793830" vmpeak="2793830" vmrss="685063" vmhwm="685063" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="3952738" vmpeak="3952738" vmrss="2415951" vmhwm="2415951" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="4760704" vmpeak="4837279" vmrss="2300220" vmhwm="2300220" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="5623264" vmpeak="5722886" vmrss="4014436" vmhwm="4113943" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="3049919" vmpeak="3126494" vmrss="2015208" vmhwm="2015208" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP16/mask_rcnn_resnet50_atrous_coco.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="3984978" vmpeak="4084600" vmrss="2446657" vmhwm="2546122" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="2641449" vmpeak="2641449" vmrss="500333" vmhwm="500333" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="4967180" vmpeak="5213473" vmrss="3420466" vmhwm="3666821" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="4566224" vmpeak="4632654" vmrss="2124101" vmhwm="2124101" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="7386906" vmpeak="7610553" vmrss="5782452" vmhwm="5999718" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="2896103" vmpeak="2974114" vmrss="1829848" vmhwm="1829848" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/mask_rcnn_resnet50_atrous_coco/FP32/mask_rcnn_resnet50_atrous_coco.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="4985754" vmpeak="5215402" vmrss="3446508" vmhwm="3675848" /> # Values from {"commit_id": "403339f8f470c90dee6f6d94ed58644b2787f66b", "commit_date": "2022-01-19 14:13"} and *= 1.3
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1114391" vmpeak="1114391" vmrss="202155" vmhwm="202155" />
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1995806" vmpeak="2045851" vmrss="683181" vmhwm="1193899" />
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1115426" vmpeak="1115426" vmrss="208135" vmhwm="208135" />
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2067858" vmpeak="2153054" vmrss="676041" vmhwm="1189921" />
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1329666" vmpeak="1330435" vmrss="274991" vmhwm="274991" />
        <model path="public/se-inception/FP16/se-inception.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2166060" vmpeak="2251256" vmrss="692946" vmhwm="1194798" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1078386" vmpeak="1078386" vmrss="165084" vmhwm="165084" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2062039" vmpeak="2137465" vmrss="762621" vmhwm="1302787" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1079535" vmpeak="1079535" vmrss="169436" vmhwm="169436" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2143580" vmpeak="2228777" vmrss="776235" vmhwm="1301762" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1300665" vmpeak="1301435" vmrss="245502" vmhwm="245580" />
        <model path="public/se-inception/FP32/se-inception.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2243238" vmpeak="2243238" vmrss="790270" vmhwm="1304394" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1383584" vmpeak="1383584" vmrss="457600" vmhwm="457600" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2084934" vmpeak="2169668" vmrss="758685" vmhwm="1308013" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1384640" vmpeak="1384640" vmrss="470329" vmhwm="470329" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2170022" vmpeak="2255219" vmrss="766880" vmhwm="1303192" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1637667" vmpeak="1638436" vmrss="565999" vmhwm="565999" />
        <model path="public/se-resnet-50/FP16/se-resnet-50.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2265785" vmpeak="2350982" vmrss="777212" vmhwm="1303208" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1252508" vmpeak="1252508" vmrss="325702" vmhwm="325702" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2297734" vmpeak="2433704" vmrss="961001" vmhwm="1588095" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1253657" vmpeak="1253657" vmrss="340074" vmhwm="340074" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2396368" vmpeak="2481564" vmrss="916255" vmhwm="1553744" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1748786" vmpeak="1749555" vmrss="496828" vmhwm="496828" />
        <model path="public/se-resnet-50/FP32/se-resnet-50.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2478444" vmpeak="2563641" vmrss="946140" vmhwm="1581554" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1380340" vmpeak="1380340" vmrss="441625" vmhwm="441625" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2151588" vmpeak="2165165" vmrss="822603" vmhwm="1296219" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1388145" vmpeak="1388145" vmrss="471348" vmhwm="471348" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2236046" vmpeak="2321243" vmrss="829977" vmhwm="1291544" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1669938" vmpeak="1730118" vmrss="572187" vmhwm="572187" />
        <model path="public/se-resnext-50/FP16/se-resnext-50.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2403351" vmpeak="2488548" vmrss="896272" vmhwm="1287598" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1264551" vmpeak="1264551" vmrss="320252" vmhwm="320252" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2431223" vmpeak="2431223" vmrss="1133163" vmhwm="1592250" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1453410" vmpeak="1453410" vmrss="349934" vmhwm="349934" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2516072" vmpeak="2601268" vmrss="1149501" vmhwm="1568543" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1782232" vmpeak="1867429" vmrss="501914" vmhwm="501914" />
        <model path="public/se-resnext-50/FP32/se-resnext-50.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2721986" vmpeak="2721986" vmrss="1246362" vmhwm="1540531" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1403105" vmpeak="1403105" vmrss="454693" vmhwm="454693" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2007262" vmpeak="2137163" vmrss="554346" vmhwm="885066" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1592588" vmpeak="1670728" vmrss="517888" vmhwm="517888" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2092875" vmpeak="2221367" vmrss="548860" vmhwm="883859" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1634895" vmpeak="1636268" vmrss="564491" vmhwm="564491" />
        <model path="public/ssd300/FP16/ssd300.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2219001" vmpeak="2304198" vmrss="606065" vmhwm="885196" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1237797" vmpeak="1237797" vmrss="283935" vmhwm="283935" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2266238" vmpeak="2356536" vmrss="805599" vmhwm="1104038" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1421955" vmpeak="1500096" vmrss="348904" vmhwm="348904" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2347144" vmpeak="2432341" vmrss="808626" vmhwm="1101146" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1749966" vmpeak="1835163" vmrss="496121" vmhwm="496121" />
        <model path="public/ssd300/FP32/ssd300.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2549684" vmpeak="2634881" vmrss="926489" vmhwm="1127599" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1550010" vmpeak="1550010" vmrss="492325" vmhwm="492325" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2216978" vmpeak="2246852" vmrss="688391" vmhwm="973408" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1742150" vmpeak="1742150" vmrss="667882" vmhwm="667882" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2301826" vmpeak="2387023" vmrss="697866" vmhwm="974745" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="2084888" vmpeak="2170084" vmrss="726590" vmhwm="726590" />
        <model path="public/ssd512/FP16/ssd512.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2564936" vmpeak="2650133" vmrss="878004" vmhwm="978021" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1369648" vmpeak="1369648" vmrss="311677" vmhwm="311677" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2491642" vmpeak="2491642" vmrss="1031742" vmhwm="1177186" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1561757" vmpeak="1639898" vmrss="487650" vmhwm="487650" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2582772" vmpeak="2667969" vmrss="1054612" vmhwm="1180384" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2018057" vmpeak="2096198" vmrss="659510" vmhwm="659510" />
        <model path="public/ssd512/FP32/ssd512.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3018490" vmpeak="3103687" vmrss="1409787" vmhwm="1409787" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1044373" vmpeak="1044373" vmrss="138741" vmhwm="138741" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1909330" vmpeak="1939194" vmrss="533468" vmhwm="874822" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1045746" vmpeak="1045746" vmrss="150820" vmhwm="150820" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="1997382" vmpeak="2082579" vmrss="522485" vmhwm="853268" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1400115" vmpeak="1401488" vmrss="187132" vmhwm="187132" />
        <model path="public/ssd_mobilenet_v1_coco/FP16/ssd_mobilenet_v1_coco.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2099359" vmpeak="2184556" vmrss="509438" vmhwm="824829" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1015742" vmpeak="1015742" vmrss="109252" vmhwm="109252" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1957919" vmpeak="1989608" vmrss="508986" vmhwm="833970" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1017114" vmpeak="1095520" vmrss="121321" vmhwm="121321" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2053833" vmpeak="2139030" vmrss="524711" vmhwm="830689" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1203155" vmpeak="1204528" vmrss="172775" vmhwm="172775" />
        <model path="public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2162082" vmpeak="2162082" vmrss="531549" vmhwm="830174" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="3373182" vmpeak="3373182" vmrss="2465569" vmhwm="2465569" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2583620" vmpeak="3276847" vmrss="1036204" vmhwm="1596748" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="3736839" vmpeak="3736839" vmrss="2498802" vmhwm="2498802" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2669076" vmpeak="3275610" vmrss="1036926" vmhwm="1598308" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="3525449" vmpeak="3562546" vmrss="2511381" vmhwm="2511381" />
        <model path="public/vgg19/FP16/vgg19.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2785593" vmpeak="3371560" vmrss="1051133" vmhwm="1599936" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="2288202" vmpeak="2288202" vmrss="1379575" vmhwm="1379575" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="3432826" vmpeak="4772851" vmrss="1844731" vmhwm="3145084" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="2560953" vmpeak="2560953" vmrss="1413224" vmhwm="1413224" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="3525412" vmpeak="4772742" vmrss="1807083" vmhwm="3145454" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="3342263" vmpeak="3343033" vmrss="2148088" vmhwm="2148088" />
        <model path="public/vgg19/FP32/vgg19.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="3642854" vmpeak="4868640" vmrss="1896221" vmhwm="3144700" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1169786" vmpeak="1169786" vmrss="282750" vmhwm="282750" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1939444" vmpeak="1992348" vmrss="304751" vmhwm="466450" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1172428" vmpeak="1172428" vmrss="300622" vmhwm="300622" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2024640" vmpeak="2109837" vmrss="307346" vmhwm="466934" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1319708" vmpeak="1435657" vmrss="330574" vmhwm="330574" />
        <model path="public/yolo-v1-tiny-tf/FP16/yolo-v1-tiny-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2127517" vmpeak="2212714" vmrss="317777" vmhwm="466960" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1075027" vmpeak="1075027" vmrss="187548" vmhwm="187548" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2028556" vmpeak="2145187" vmrss="418662" vmhwm="562749" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1077668" vmpeak="1077668" vmrss="205644" vmhwm="205644" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2113649" vmpeak="2198846" vmrss="422427" vmhwm="564241" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1284852" vmpeak="1287494" vmrss="290934" vmhwm="290934" />
        <model path="public/yolo-v1-tiny-tf/FP32/yolo-v1-tiny-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2232469" vmpeak="2240950" vmrss="448557" vmhwm="563076" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1850664" vmpeak="1850664" vmrss="901706" vmhwm="901706" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2082522" vmpeak="2247648" vmrss="461983" vmhwm="763126" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="2038134" vmpeak="2123331" vmrss="979648" vmhwm="979648" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2177286" vmpeak="2262483" vmrss="497452" vmhwm="787300" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="2080462" vmpeak="2086879" vmrss="1023526" vmhwm="1023526" />
        <model path="public/yolo-v2-tf/FP16/yolo-v2-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2315367" vmpeak="2400564" vmrss="527820" vmhwm="764706" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1498515" vmpeak="1498515" vmrss="549926" vmhwm="549926" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2471450" vmpeak="2847457" vmrss="866907" vmhwm="1178996" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1504932" vmpeak="1504932" vmrss="626033" vmhwm="626033" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2563236" vmpeak="2846100" vmrss="873059" vmhwm="1177664" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2313610" vmpeak="2398806" vmrss="893921" vmhwm="893958" />
        <model path="public/yolo-v2-tf/FP32/yolo-v2-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2767268" vmpeak="2941967" vmrss="1008571" vmhwm="1181902" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1110751" vmpeak="1110751" vmrss="223412" vmhwm="223412" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="1923818" vmpeak="1950629" vmrss="282828" vmhwm="429265" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1113762" vmpeak="1113762" vmrss="242008" vmhwm="242008" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2005879" vmpeak="2091076" vmrss="280706" vmhwm="427741" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1237802" vmpeak="1268186" vmrss="249511" vmhwm="249511" />
        <model path="public/yolo-v2-tiny-tf/FP16/yolo-v2-tiny-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2107586" vmpeak="2192782" vmrss="293467" vmhwm="428490" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1028014" vmpeak="1030369" vmrss="139328" vmhwm="139328" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="1983238" vmpeak="2050245" vmrss="357052" vmhwm="495908" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1212078" vmpeak="1212078" vmrss="158246" vmhwm="158246" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2068710" vmpeak="2153907" vmrss="357437" vmhwm="495237" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1390428" vmpeak="1475624" vmrss="220927" vmhwm="220927" />
        <model path="public/yolo-v2-tiny-tf/FP32/yolo-v2-tiny-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2187723" vmpeak="2187723" vmrss="404102" vmhwm="495456" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="2047505" vmpeak="2047505" vmrss="1096638" vmhwm="1096638" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2301114" vmpeak="2451264" vmrss="806754" vmhwm="1364896" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="2054748" vmpeak="2054748" vmrss="1144384" vmhwm="1144384" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2386014" vmpeak="2471211" vmrss="862357" vmhwm="1380303" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="2472865" vmpeak="2557843" vmrss="1200310" vmhwm="1200310" />
        <model path="public/yolo-v3-tf/FP16/yolo-v3-tf.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2510856" vmpeak="2596053" vmrss="952666" vmhwm="1439838" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1619061" vmpeak="1619061" vmrss="668423" vmhwm="668423" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2627606" vmpeak="3083823" vmrss="1242113" vmhwm="1780994" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1807140" vmpeak="1885312" vmrss="715691" vmhwm="715691" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2712080" vmpeak="3083100" vmrss="1226342" vmhwm="1802039" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="2497045" vmpeak="2504070" vmrss="1042381" vmhwm="1042381" />
        <model path="public/yolo-v3-tf/FP32/yolo-v3-tf.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2905910" vmpeak="3180080" vmrss="1353804" vmhwm="1800713" />
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1001400" vmpeak="1001400" vmrss="146021" vmhwm="146021" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2265411" vmpeak="2265411" vmrss="433903" vmhwm="830575" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1112098" vmpeak="1112098" vmrss="162874" vmhwm="162874" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2371928" vmpeak="2371928" vmrss="452197" vmhwm="849206" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1008586" vmpeak="1008586" vmrss="153077" vmhwm="153077" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP16/action-recognition-0001-decoder.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2265421" vmpeak="2265421" vmrss="432156" vmhwm="830507" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="977522" vmpeak="977522" vmrss="112351" vmhwm="112351" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2399134" vmpeak="2420444" vmrss="581666" vmhwm="994869" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1106778" vmpeak="1106778" vmrss="143676" vmhwm="143676" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2500071" vmpeak="2521381" vmrss="581505" vmhwm="993200" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="984568" vmpeak="984568" vmrss="118690" vmhwm="118690" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-decoder/FP32/action-recognition-0001-decoder.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2404298" vmpeak="2425607" vmrss="602420" vmhwm="993142" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1263880" vmpeak="1263880" vmrss="409588" vmhwm="409588" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2303709" vmpeak="2303709" vmrss="444724" vmhwm="709300" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1370382" vmpeak="1455578" vmrss="453377" vmhwm="453377" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2399571" vmpeak="2439184" vmrss="512668" vmhwm="708047" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1263880" vmpeak="1263880" vmrss="415953" vmhwm="415953" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP16/action-recognition-0001-encoder.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2309257" vmpeak="2309257" vmrss="467656" vmhwm="707699" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="1221006" vmpeak="1295221" vmrss="399344" vmhwm="399344" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2535899" vmpeak="2535899" vmrss="671195" vmhwm="920743" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1418029" vmpeak="1418029" vmrss="495799" vmhwm="495799" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2637247" vmpeak="2637247" vmrss="662958" vmhwm="917633" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="1221194" vmpeak="1295408" vmrss="406905" vmhwm="406905" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/action-recognition-0001/action-recognition-0001-encoder/FP32/action-recognition-0001-encoder.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2536159" vmpeak="2536159" vmrss="671595" vmhwm="920571" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="create_exenetwork" device="CPU" vmsize="1002435" vmpeak="1002435" vmrss="153223" vmhwm="153223" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="create_exenetwork" device="GPU" vmsize="2272504" vmpeak="2272504" vmrss="470667" vmhwm="967184" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="inference_with_streams" device="CPU" vmsize="1110137" vmpeak="1183156" vmrss="162604" vmhwm="162604" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="inference_with_streams" device="GPU" vmsize="2373690" vmpeak="2373690" vmrss="489200" vmhwm="968448" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="infer_request_inference" device="CPU" vmsize="1009621" vmpeak="1009621" vmrss="160040" vmhwm="160040" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP16/driver-action-recognition-adas-0002-decoder.xml" precision="FP16" test="infer_request_inference" device="GPU" vmsize="2267184" vmpeak="2267184" vmrss="450502" vmhwm="967954" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="create_exenetwork" device="CPU" vmsize="987246" vmpeak="987246" vmrss="121742" vmhwm="121742" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="create_exenetwork" device="GPU" vmsize="2391490" vmpeak="2412800" vmrss="569114" vmhwm="1107033" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="inference_with_streams" device="CPU" vmsize="1106133" vmpeak="1184144" vmrss="140264" vmhwm="140264" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="inference_with_streams" device="GPU" vmsize="2492755" vmpeak="2514064" vmrss="571006" vmhwm="1106617" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="infer_request_inference" device="CPU" vmsize="994406" vmpeak="1020219" vmrss="128960" vmhwm="128960" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
        <model path="intel/driver-action-recognition-adas-0002/driver-action-recognition-adas-0002-decoder/FP32/driver-action-recognition-adas-0002-decoder.xml" precision="FP32" test="infer_request_inference" device="GPU" vmsize="2391812" vmpeak="2413122" vmrss="570486" vmhwm="1085520" /> # Values from {"commit_id": "8ca1abed0d309d4c4981f00786adbe6106ca46b3", "commit_date": "2022-03-01 14:04"} and *= 1.3
    </models>
</attributes>
