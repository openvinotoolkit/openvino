# 性能优化简介 {#openvino_docs_optimization_guide_dldt_optimization_guide_zh_CN}

虽然推理性能应定义为许多因素（包括精度和效率）的组合，但多数情况下会将其描述为执行速度。作为模型处理实时数据的速率，从根本上说，它基于两个相互关联的指标：延迟和吞吐量。

![](../../img/LATENCY_VS_THROUGHPUT.svg)

**延迟**测量处理单一输入所需的推理时间（毫秒）。如果同时执行多个输入（如通过批处理），那么整体吞吐量（在特定视觉处理任务中，则是每秒的推理次数或每秒帧数，即帧速率）通常更为重要。
**吞吐量**通过用所处理的输入数量除以处理时间计算得出。

## 端到端应用性能
区分神经网络的“纯”推理时间与端到端应用性能至关重要。例如，当在 dGPU 等加速器上处理主机输入张量时，主机和设备之间的数据传输可能会意外地影响性能。

同样，输入预处理也会显著增加推理时间。如[获得性能数据](../MO_DG/prepare_model/Getting_performance_numbers_zh_CN.md)一节中所述，在评估*推理*性能时，一种方案是单独测量所有这些项目。
但是，对于**端到端场景**，应考虑通过 OpenVINO™ 和异步执行进行图像预处理，作为一种分摊通信成本（如数据传输）的方式。有关更多详细信息，请参阅[一般优化指南](../../optimization_guide/dldt_deployment_optimization_common.md)。

另一个特定用例是**首次推理延迟**（例如，需要快速启动应用时），由此产生的性能可能主要受模型加载时间的影响。可以将[模型缓存](../../OV_Runtime_UG/Model_caching_overview.md)当作一种缩短模型加载/编译时间的方法。

最后，**内存占用空间**限制可能是设计应用时的另一个考虑因素。虽然这为使用*模型*优化技术提供了动机，但请记住，面向吞吐量的执行通常会占用更多内存。有关更多详细信息，请参阅[运行时推理优化指南](./dldt_deployment_optimization_guide_zh_CN.md)。


> **NOTE**: 要获取 OpenVINO™ 的性能数据以及有关如何测量该数据并与原生框架进行比较的提示，请参阅[获得性能数据](../MO_DG/prepare_model/Getting_performance_numbers_zh_CN.md)一文。
 
## 提高性能：模型与运行时优化对比

> **NOTE**: 首先确保您的模型可以通过 OpenVINO™ 运行时成功进行推理。

有两种主要优化方法可提升 OpenVINO™ 的推理性能：模型级和运行时级优化。它们**完全兼容**并且可单独执行。

- **模型优化**包括模型修改，如量化、修剪、预处理优化等。有关更多详细信息，请参阅本[文档](./model_optimization_guide_zh_CN.md)。
   - 即使不调优运行时参数（如下所述），模型优化也会直接缩短推理时间。

- **运行时（部署）优化**包括调优模型*执行*参数。有关更多详细信息，请参阅[运行时推理优化指南](./dldt_deployment_optimization_guide_zh_CN.md)。

## 性能基准测试
[性能基准测试一节](../benchmarks/performance_benchmarks_zh_CN.md)中提供了一系列用于评估性能和比较数据（在各种支持的设备上测量得出）的公共模型。