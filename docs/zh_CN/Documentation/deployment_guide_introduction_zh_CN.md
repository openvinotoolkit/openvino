# OpenVINO™ 部署简介 {#openvino_docs_deployment_guide_introduction_zh_CN}


构建符合 OpenVINO™ 和您的要求的模型后，即可选择通过以下几种方法部署该模型与应用：

* [在本地部署应用](../OV_Runtime_UG/deployment/deployment_intro_zh_CN.md)。
* [通过 OpenVINO™ 模型服务器部署模型](@ref ovms_what_is_openvino_model_server_zh_CN)。
* [部署应用以便集成 TensorFlow 框架与 OpenVINO](./openvino_ecosystem_ovtf_zh_CN.md)。


> **NOTE**: 请注意，[在 OpenVINO™ 运行时运行推理](../OV_Runtime_UG/openvino_intro_zh_CN.md)是最基本的部署形式。在继续之前，请确保了解如何创建正确的推理配置。