diff --git a/src/frontends/paddle/include/openvino/frontend/paddle/decoder.hpp b/src/frontends/paddle/include/openvino/frontend/paddle/decoder.hpp
index 34f2549fd0..6903826a77 100644
--- a/src/frontends/paddle/include/openvino/frontend/paddle/decoder.hpp
+++ b/src/frontends/paddle/include/openvino/frontend/paddle/decoder.hpp
@@ -59,6 +59,14 @@ public:
     /// \brief Get the type of the operation
     virtual std::string get_op_type() const = 0;
 
+    virtual std::map<std::string, std::vector<ov::element::Type>> get_output_type_map() const = 0;
+    virtual std::map<std::string, OutputVector> map_for_each_input(
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const = 0;
+
+    virtual std::map<std::string, OutputVector> map_for_each_output(
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const = 0;
+
+
     /// \brief Destructor
     virtual ~DecoderBase();
 };
diff --git a/src/frontends/paddle/include/openvino/frontend/paddle/frontend.hpp b/src/frontends/paddle/include/openvino/frontend/paddle/frontend.hpp
index 77b59aa4ca..c2242214f5 100644
--- a/src/frontends/paddle/include/openvino/frontend/paddle/frontend.hpp
+++ b/src/frontends/paddle/include/openvino/frontend/paddle/frontend.hpp
@@ -18,8 +18,8 @@ namespace ov {
 namespace frontend {
 namespace paddle {
 
-class OpPlace;
-class TensorPlace;
+class BaseOpPlace;
+class BaseTensorPlace;
 
 class PADDLE_FRONTEND_API FrontEnd : public ov::frontend::FrontEnd {
 public:
@@ -83,14 +83,14 @@ protected:
     static std::vector<std::shared_ptr<Model>> convert_each_node(
         const std::shared_ptr<InputModel>& frontend_model,
         std::function<std::map<std::string, OutputVector>(const std::map<std::string, Output<Node>>&,
-                                                          const std::shared_ptr<OpPlace>&)> func);
+                                                          const std::shared_ptr<BaseOpPlace>&)> func);
     static std::map<int32_t, std::shared_ptr<Model>> convert_each_node_recursive(
         const std::shared_ptr<InputModel>& frontend_model,
         const int32_t block_idx,
-        const std::vector<std::shared_ptr<TensorPlace>>& input_tensors,
-        const std::vector<std::shared_ptr<TensorPlace>>& output_tensors,
+        const std::vector<std::shared_ptr<BaseTensorPlace>>& input_tensors,
+        const std::vector<std::shared_ptr<BaseTensorPlace>>& output_tensors,
         std::function<std::map<std::string, OutputVector>(const std::map<std::string, Output<Node>>&,
-                                                          const std::shared_ptr<OpPlace>&)> func);
+                                                          const std::shared_ptr<BaseOpPlace>&)> func);
 
     TelemetryExtension::Ptr m_telemetry;
     std::vector<DecoderTransformationExtension::Ptr> m_transformation_extensions;
diff --git a/src/frontends/paddle/include/openvino/frontend/paddle/node_context.hpp b/src/frontends/paddle/include/openvino/frontend/paddle/node_context.hpp
index 91dfaa9af5..aa3a587a96 100644
--- a/src/frontends/paddle/include/openvino/frontend/paddle/node_context.hpp
+++ b/src/frontends/paddle/include/openvino/frontend/paddle/node_context.hpp
@@ -22,10 +22,11 @@ using NamedInputs = std::map<InPortName, OutputVector>;
 class NodeContext : public ov::frontend::NodeContext {
 public:
     using Ptr = std::shared_ptr<NodeContext>;
-    NodeContext(const std::shared_ptr<DecoderBase>& _decoder, const NamedInputs& _name_map)
+    NodeContext(const std::shared_ptr<DecoderBase>& _decoder, const NamedInputs& _name_map, bool _is_json = false)
         : ov::frontend::NodeContext(_decoder->get_op_type()),
           decoder(_decoder),
-          name_map(_name_map) {}
+          name_map(_name_map),
+          is_json(_is_json){}
 
     /// Detects if there is at least one input attached with a given name
     bool has_input(const std::string& name) const {
@@ -38,13 +39,18 @@ public:
     /// Returns exactly one input with a given name; throws if there is no inputs or
     /// there are more than one input
     Output<Node> get_input(const std::string& name) const override {
-        FRONT_END_GENERAL_CHECK(name_map.at(name).size() == 1);
-        return name_map.at(name).at(0);
+        auto it = name_map.find(name);
+        FRONT_END_GENERAL_CHECK(it != name_map.end(), "can't find input name:", name );
+        auto& input = it->second;
+        FRONT_END_GENERAL_CHECK(input.size() == 1);
+        return input.at(0);
     }
 
     /// Returns all inputs with a given name
     OutputVector get_ng_inputs(const std::string& name) const {
-        return name_map.at(name);
+        auto it = name_map.find(name);
+        FRONT_END_GENERAL_CHECK(it != name_map.end(), "can't find input name:", name );
+        return it->second;
     }
 
     /// Returns all inputs in order they appear in map. This is used for FrameworkNode
@@ -102,6 +108,10 @@ public:
         return decoder->get_version();
     }
 
+    bool is_json_format() const {
+        return is_json;
+    }
+
 private:
     ov::Any apply_additional_conversion_rules(const ov::Any& any, const std::type_info& type_info) const override {
         auto res = decoder->convert_attribute(any, type_info);
@@ -110,6 +120,7 @@ private:
 
     const std::shared_ptr<DecoderBase> decoder;
     const NamedInputs& name_map;
+    const bool is_json;
 };
 
 inline NamedOutputs NodeContext::default_single_output_mapping(
diff --git a/src/frontends/paddle/src/decoder_json.cpp b/src/frontends/paddle/src/decoder_json.cpp
new file mode 100644
index 0000000000..6b820414c5
--- /dev/null
+++ b/src/frontends/paddle/src/decoder_json.cpp
@@ -0,0 +1,282 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "decoder_json.hpp"
+#include "op_table.hpp"
+
+#include <algorithm>
+#include <chrono>
+#include <fstream>
+#include <map>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+
+using namespace ::paddle::framework;
+
+ov::Any DecoderJson::get_attribute(const std::string& name) const {
+    // some attribute name is already changed in 3.0, so use new name to get attributes;
+    static const std::map<std::string, std::map<std::string, std::string>> attr_name_map = {
+        {"batch_norm_", {{"data_layout", "data_format"}}},
+        {"group_norm", {{"data_layout", "data_format"}}},
+        {"cast", {{"out_dtype", "dtype"}}},
+        {"pool3d", {{"ksize", "kernel_size"}}},
+        {"dropout", {{"dropout_implementation", "mode"}}},
+        {"index_select", {{"dim", "axis"}}},
+        {"leaky_relu", {{"alpha", "negative_slope"}}},
+        {"matmul", {{"transpose_X", "transpose_x"}, {"transpose_Y", "transpose_y"}}},
+        {"max_pool2d_with_index", {{"ksize", "kernel_size"}}},
+        {"max_pool3d_with_index", {{"ksize", "kernel_size"}}},
+        {"pad3d", {{"value", "pad_value"}}},
+        {"all", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"max", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"mean", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"min", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"prod", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"any", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        // the type name sum already replaced by reduce_sum when decode json
+        {"reduce_sum", {{"keep_dim", "keepdim"}, {"dim", "axis"}}},
+        {"pow", {{"factor", "y"}}},
+        {"transpose", {{"axis", "perm"}}},
+        {"softshrink", {{"lambda", "threshold"}}},
+        };
+    auto& op = op_place.lock()->get_op();
+    // workaroud for new tril and triu in paddle 3.0
+    if (op.type == "tril" && name == "lower") {
+        return ov::Any(true);
+    } else if (op.type == "triu" && name == "lower") {
+        return ov::Any(false);
+    }
+    std::string new_name = name;
+    auto op_it = attr_name_map.find(op.type);
+    if (op_it != attr_name_map.end()) {
+        const auto attr_names = op_it->second;
+        auto at_it = attr_names.find(name);
+        if (at_it != attr_names.end()) {
+            new_name = at_it->second;
+        }
+    }
+    auto& attrs = op.json_data.at("A");
+    for (auto& attr : attrs) {
+        std::string attr_name = attr.at("N").template get<std::string>();
+        if (attr_name == new_name) {
+            auto result = json::decode_attr(attr);
+            return result;
+            // if (result.empty() && attr_name == "shape") {
+            //     std::cout << "warining the shape attr is null" << std::endl;
+            //     return ov::Any(std::vector<int32_t>{});
+            // } else {
+            //     return result;
+            // }
+        }
+    }
+    return {};
+}
+
+int64_t DecoderJson::get_version() const {
+    return 1;
+}
+
+ov::Any DecoderJson::convert_attribute(const Any& data, const std::type_info& type_info) const {
+    // if (data.is<int32_t>() && type_info == typeid(ov::element::Type)) {
+    //     return get_ov_type(static_cast<proto::VarType_Type>(data.as<int32_t>()));
+    // } else if (data.is<std::vector<int32_t>>() && type_info == typeid(std::vector<ov::element::Type>)) {
+    //     const auto& casted = data.as<std::vector<int32_t>>();
+    //     std::vector<ov::element::Type> types(casted.size());
+    //     for (size_t i = 0; i < casted.size(); ++i) {
+    //         types[i] = get_ov_type(static_cast<proto::VarType_Type>(casted[i]));
+    //     }
+    //     return types;
+    // }
+    // no conversion rules found.
+    return data;
+}
+
+std::vector<paddle::OutPortName> DecoderJson::get_output_names() const {
+    std::vector<std::string> output_names;
+    auto& op = op_place.lock()->get_op();
+    auto fix_output_name = get_output_name_by_op_type(op.type);
+    for (auto& name : fix_output_name) {
+        output_names.push_back(name);
+    }
+
+    return output_names;
+}
+// ?
+std::vector<paddle::TensorName> DecoderJson::get_output_var_names(const std::string& var_name) const {
+    // std::vector<std::string> output_names;
+    // for (const auto& output : get_place()->get_desc().outputs()) {
+    //     if (output.parameter() == var_name) {
+    //         for (int idx = 0; idx < output.arguments_size(); ++idx) {
+    //             output_names.push_back(output.arguments()[idx]);
+    //         }
+    //     }
+    // }
+    // return output_names;
+    return {var_name};
+}
+// ?
+std::vector<paddle::TensorName> DecoderJson::get_input_var_names(const std::string& var_name) const {
+    // std::vector<std::string> input_names;
+    // for (const auto& input : get_place()->get_desc().inputs()) {
+    //     if (input.parameter() == var_name) {
+    //         for (int idx = 0; idx < input.arguments_size(); ++idx) {
+    //             input_names.push_back(input.arguments()[idx]);
+    //         }
+    //     }
+    // }
+    // return input_names;
+    return {var_name};
+}
+
+size_t DecoderJson::get_output_size(const std::string& port_name) const {
+    // const auto out_port = get_place()->get_output_ports().at(port_name);
+    // return out_port.size();
+    return 1;
+}
+
+size_t DecoderJson::get_output_size() const {
+    auto& op = get_place()->get_op();
+    return op.outputPorts.size();
+}
+
+std::map<std::string, std::vector<ov::element::Type>> DecoderJson::get_output_type_map() const {
+    auto& op = get_place()->get_op();
+    auto fix_output_name = get_output_name_by_op_type(op.type);
+    std::map<std::string, std::vector<ov::element::Type>> output_types;
+    size_t index = 0;
+    for (auto& output_name : fix_output_name) {
+        FRONT_END_GENERAL_CHECK(index < op.outputPorts.size(), "output name num is not match port num");
+        auto& outputport = op.outputPorts[index];
+        output_types.insert({output_name, {json::convert_to_ov_type(outputport.get_precision())}});
+        index++;
+    }
+    return output_types;
+}
+
+std::vector<std::pair<ov::element::Type, ov::PartialShape>> DecoderJson::get_output_port_infos(
+    const std::string& port_name) const {
+    auto& op = get_place()->get_op();
+    auto fix_output_name = get_output_name_by_op_type(op.type);
+    std::vector<std::pair<ov::element::Type, ov::PartialShape>> output_types;
+    size_t index = 0;
+    for (auto& output_name : fix_output_name) {
+        if (output_name == port_name) {
+            break;
+        } else {
+            index++;
+        }
+    }
+    for (const auto& outputport : op.outputPorts) {
+        output_types.push_back({json::convert_to_ov_type(outputport.get_precision()),
+                ov::PartialShape(outputport.get_shapes())});
+    }
+    // add a fake bool output for while
+    if (op.type == "while") {
+        output_types.push_back({ov::element::boolean, ov::PartialShape({1})});
+    }
+    if (index > output_types.size() -1 ) {
+       FRONT_END_GENERAL_CHECK(false, "can't find ouput name ", port_name);
+       return {};
+    } else if (index < fix_output_name.size() - 1) {
+        return {output_types[index]};
+    } else {
+        std::vector<std::pair<ov::element::Type, ov::PartialShape>> all_left_output_types;
+        while (index < output_types.size()) {
+            all_left_output_types.push_back(output_types[index]);
+            index++;
+        }
+        return all_left_output_types;
+    }
+}
+
+ov::element::Type DecoderJson::get_out_port_type(const std::string& port_name) const {
+    auto map = get_output_type_map();
+    auto iter = map.find(port_name);
+    if(iter != map.end()) {
+       return iter->second[0];
+    } else {
+       FRONT_END_GENERAL_CHECK(false, "get port precision failed", port_name);
+       return ov::element::undefined;
+    }
+}
+
+std::string DecoderJson::get_op_type() const {
+    return get_place()->get_op().type;
+}
+/*
+std::vector<proto::OpDesc_Attr> DecoderJson::decode_attribute_helper(const std::string& name) const {
+    std::vector<proto::OpDesc_Attr> attrs;
+    for (const auto& attr : get_place()->get_desc().attrs()) {
+        if (attr.name() == name)
+            attrs.push_back(attr);
+    }
+    FRONT_END_GENERAL_CHECK(attrs.size() <= 1,
+                            "An error occurred while parsing the ",
+                            name,
+                            " attribute of ",
+                            get_place()->get_desc().type(),
+                            "node. Unsupported number of attributes. Current number: ",
+                            attrs.size(),
+                            " Expected number: 0 or 1");
+    return attrs;
+}
+
+namespace {
+inline std::map<std::string, OutputVector> map_for_each_input_impl(
+    const google::protobuf::RepeatedPtrField<::paddle::framework::proto::OpDesc_Var>& c,
+    const std::function<Output<Node>(const std::string&, size_t)>& func) {
+    size_t idx = 0;
+    std::map<std::string, OutputVector> res;
+    for (const auto& port : c) {
+        std::vector<Output<Node>> v;
+        v.reserve(port.arguments_size());
+        for (const auto& inp : port.arguments()) {
+            v.push_back(func(inp, idx++));
+        }
+        res.emplace(std::make_pair(port.parameter(), v));
+    }
+    return res;
+}
+}  // namespace
+*/
+
+std::map<std::string, OutputVector> DecoderJson::map_for_each_input(
+    const std::function<Output<Node>(const std::string&, size_t)>& func) const {
+    auto& op = get_place()->get_op();
+    size_t idx = 0;
+    std::map<std::string, OutputVector> res;
+    for (const auto& inputId : op.inputIds) {
+        auto input_name = get_input_name_by_op_type(op.type, idx);
+        std::vector<Output<Node>> v;
+        v.push_back(func(input_name, idx++));
+        res.emplace(std::make_pair(input_name, v));
+    }
+    return res;
+}
+
+std::map<std::string, OutputVector> DecoderJson::map_for_each_output(
+    const std::function<Output<Node>(const std::string&, size_t)>& func) const {
+    auto& op = get_place()->get_op();
+    size_t idx = 0;
+    std::map<std::string, OutputVector> res;
+    auto outputName = get_output_names();
+    for (const auto& port : op.outputPorts) {
+        std::vector<Output<Node>> v;
+        v.push_back(func(std::to_string(port.id), idx));
+        res.emplace(std::make_pair(outputName[idx], v));
+        idx++;
+    }
+    return res;
+}
+
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/decoder_json.hpp b/src/frontends/paddle/src/decoder_json.hpp
new file mode 100644
index 0000000000..aa24baba6f
--- /dev/null
+++ b/src/frontends/paddle/src/decoder_json.hpp
@@ -0,0 +1,69 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+
+#include <algorithm>
+#include <chrono>
+#include <fstream>
+#include <map>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "framework.pb.h"
+#include "openvino/core/any.hpp"
+#include "openvino/frontend/paddle/frontend.hpp"
+#include "openvino/frontend/paddle/node_context.hpp"
+#include "place.hpp"
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+class DecoderJson : public paddle::DecoderBase {
+public:
+    explicit DecoderJson(const std::shared_ptr<JsonOpPlace>& op) : op_place(op) {}
+
+    ov::Any get_attribute(const std::string& name) const override;
+
+    std::vector<TensorName> get_output_var_names(const std::string& var_name) const override;
+    std::vector<TensorName> get_input_var_names(const std::string& var_name) const override;
+
+    ov::Any convert_attribute(const ov::Any& data, const std::type_info& type_info) const override;
+
+    std::vector<paddle::OutPortName> get_output_names() const override;
+
+    size_t get_output_size() const override;
+    size_t get_output_size(const std::string& port_name) const override;
+
+    ov::element::Type get_out_port_type(const std::string& port_name) const override;
+
+    std::string get_op_type() const override;
+
+    std::map<std::string, std::vector<ov::element::Type>> get_output_type_map() const override;
+    std::vector<std::pair<ov::element::Type, ov::PartialShape>> get_output_port_infos(
+        const std::string& port_name) const override;
+
+    std::map<std::string, OutputVector> map_for_each_input(
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const override;
+
+    std::map<std::string, OutputVector> map_for_each_output(
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const override;
+
+    int64_t get_version() const override;
+private:
+    std::weak_ptr<JsonOpPlace> op_place;
+
+    const std::shared_ptr<JsonOpPlace> get_place() const {
+        auto place = op_place.lock();
+        if (!place)
+            FRONT_END_THROW("This json decoder contains empty op place.");
+        return place;
+    }
+};
+
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/decoder_proto.hpp b/src/frontends/paddle/src/decoder_proto.hpp
index d6f665be60..3d88e78c51 100644
--- a/src/frontends/paddle/src/decoder_proto.hpp
+++ b/src/frontends/paddle/src/decoder_proto.hpp
@@ -27,7 +27,7 @@ ov::element::Type get_ov_type(const ::paddle::framework::proto::VarType_Type& ty
 
 class DecoderProto : public paddle::DecoderBase {
 public:
-    explicit DecoderProto(const std::shared_ptr<OpPlace>& op) : op_place(op) {}
+    explicit DecoderProto(const std::shared_ptr<ProtoOpPlace>& op) : op_place(op) {}
 
     ov::Any get_attribute(const std::string& name) const override;
 
@@ -45,23 +45,23 @@ public:
 
     std::string get_op_type() const override;
 
-    std::map<std::string, std::vector<ov::element::Type>> get_output_type_map() const;
+    std::map<std::string, std::vector<ov::element::Type>> get_output_type_map() const override;
     std::vector<std::pair<ov::element::Type, ov::PartialShape>> get_output_port_infos(
         const std::string& port_name) const override;
 
     std::map<std::string, OutputVector> map_for_each_input(
-        const std::function<Output<Node>(const std::string&, size_t)>& func) const;
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const override;
 
     std::map<std::string, OutputVector> map_for_each_output(
-        const std::function<Output<Node>(const std::string&, size_t)>& func) const;
+        const std::function<Output<Node>(const std::string&, size_t)>& func) const override;
 
     int64_t get_version() const override;
 
 private:
     std::vector<::paddle::framework::proto::OpDesc_Attr> decode_attribute_helper(const std::string& name) const;
-    std::weak_ptr<OpPlace> op_place;
+    std::weak_ptr<ProtoOpPlace> op_place;
 
-    const std::shared_ptr<OpPlace> get_place() const {
+    const std::shared_ptr<ProtoOpPlace> get_place() const {
         auto place = op_place.lock();
         if (!place)
             FRONT_END_THROW("This proto decoder contains empty op place.");
diff --git a/src/frontends/paddle/src/frontend.cpp b/src/frontends/paddle/src/frontend.cpp
index 9287bfe78c..d94c412f0a 100644
--- a/src/frontends/paddle/src/frontend.cpp
+++ b/src/frontends/paddle/src/frontend.cpp
@@ -3,8 +3,10 @@
 //
 
 #include "openvino/frontend/paddle/frontend.hpp"
+#include "openvino/core/graph_util.hpp"
 
 #include <google/protobuf/port_def.inc>
+#include <memory>
 #ifndef PROTOBUF_VERSION
 #    include <google/protobuf/runtime_version.h>
 #endif
@@ -30,6 +32,7 @@
 #include "input_model.hpp"
 #include "internal/pass/transform_fakequantize.hpp"
 #include "internal/pass/transform_if.hpp"
+#include "internal/pass/transform_if_else.hpp"
 #include "internal/pass/transform_tensorarray.hpp"
 #include "internal/pass/transform_while.hpp"
 #include "op_table.hpp"
@@ -42,6 +45,8 @@
 #include "paddle_utils.hpp"
 #include "place.hpp"
 #include "transformations/resolve_names_collisions.hpp"
+#include "openvino/opsets/opset7.hpp"
+#include "decoder_json.hpp"
 
 using namespace ov::frontend::paddle::op::default_opset;
 using namespace ov;
@@ -52,66 +57,201 @@ namespace frontend {
 namespace paddle {
 namespace {
 
-NamedOutputs make_ng_node(const std::map<paddle::TensorName, Output<Node>>& nodes,
-                          const std::shared_ptr<OpPlace>& op_place,
-                          const std::map<std::string, CreatorFunction>& CREATORS_MAP) {
-    const auto& op_desc = op_place->get_desc();
-
-    auto creator_it = CREATORS_MAP.find(op_desc.type());
-    FRONT_END_OP_CONVERSION_CHECK(creator_it != CREATORS_MAP.end(), "No creator found for ", op_desc.type(), " node.");
+NamedInputs create_name_inputs(const std::map<paddle::TensorName, Output<Node>>& nodes,
+                               std::shared_ptr<JsonOpPlace>& json_op_place) {
     NamedInputs named_inputs;
-    for (const auto& input_port : op_desc.inputs()) {
-        for (const auto& in_tensor_name : input_port.arguments()) {
-            auto node_it = nodes.find(in_tensor_name);
-            // general check, because in case of error partial conversion should fail
-            FRONT_END_GENERAL_CHECK(node_it != nodes.end(),
-                                    "Input ",
-                                    in_tensor_name,
-                                    " for node with type ",
-                                    op_desc.type(),
-                                    " wasn't found. It may happen if model was cut incorrectly.");
-            named_inputs[input_port.parameter()].push_back(node_it->second);
+    size_t input_name_index = 0;
+    const auto& op = json_op_place->get_op();
+    auto type = op.type;
+    for (const auto& inputId : op.inputIds) {
+        if (op.unusedInputIds.find(inputId) != op.unusedInputIds.end()) {
+            input_name_index++;
+            continue;
+        }
+        auto port_name = std::to_string(inputId);
+        auto node_it = nodes.find(port_name);
+        auto input_name = get_input_name_by_op_type(json_op_place->get_op().type, input_name_index);
+        if (node_it != nodes.end()) {
+            named_inputs[input_name].push_back(node_it->second);
+        } else {
+            // input mabe from combine node
+            size_t combine_index = 0;
+            while (true) {
+                auto combine_input_name = port_name + "_" + std::to_string(combine_index);
+                node_it = nodes.find(combine_input_name);
+                if (node_it != nodes.end()) {
+                    named_inputs[input_name].push_back(node_it->second);
+                } else {
+                    break;
+                }
+                combine_index++;
+            }
         }
+        input_name_index++;
+        FRONT_END_GENERAL_CHECK(named_inputs[input_name].size() > 0,
+                "Input ",
+                port_name,
+                " for node with type ",
+                type,
+                " wasn't found. It may happen if model was cut incorrectly.");
     }
-    NamedOutputs outputs;
-    // In case the conversion function throws exception
-    try {
-        outputs = creator_it->second(paddle::NodeContext(op_place->get_decoder(), named_inputs));
-    } catch (std::exception& ex) {
-        FRONT_END_OP_CONVERSION_CHECK(false, "Fail to convert " + op_desc.type() + " Exception " + ex.what());
+    if (op.type == "if") {
+        // only one input cond, we need to parepare if_inputs and else_inputs and sub block index
+        FRONT_END_GENERAL_CHECK(op.sub_block_idxs.size() == 2, "if op has two blocks");
+        std::vector<int32_t> sub_block_indexs;
+        auto if_block_id = op.sub_block_idxs[0];
+        sub_block_indexs.push_back(if_block_id);
+        auto& if_input_ids = op.get_sub_inputs_ids(if_block_id);
+        for (auto& inputs : if_input_ids) {
+            auto port_name = std::to_string(inputs);
+            auto it = nodes.find(port_name);
+            FRONT_END_OP_CONVERSION_CHECK(it != nodes.end(),
+                    "cant' find the input:", port_name, " for type ", type);
+            named_inputs["if_inputs"].push_back(it->second);
+        }
+        auto else_block_id = op.sub_block_idxs[1];
+        sub_block_indexs.push_back(else_block_id);
+        auto& else_input_ids = op.get_sub_inputs_ids(else_block_id);
+        for (auto& inputs : else_input_ids) {
+            auto port_name = std::to_string(inputs);
+            auto it = nodes.find(port_name);
+            FRONT_END_OP_CONVERSION_CHECK(it != nodes.end(),
+                    "cant' find the input:", port_name, " for type ", type);
+            named_inputs["else_inputs"].push_back(it->second);
+        }
+        auto sub_block_indexs_node = ov::opset7::Constant::create(ov::element::i32, {2}, sub_block_indexs);
+        named_inputs["sub_block_indexs"].push_back(sub_block_indexs_node);
     }
+    if (op.type == "while") {
+        FRONT_END_GENERAL_CHECK(op.sub_block_idxs.size() == 1, "while op has one block");
+        auto sub_block_id = op.sub_block_idxs[0];
+        // set to name to match
+        auto& sub_input_ids = op.get_sub_inputs_ids(sub_block_id);
+        // skip Condition
+        size_t index = 0;
+        for (auto& item : named_inputs["X"]) {
+            while (index < sub_input_ids.size() ) {
+                if (sub_input_ids[index] < 0 ) {
+                    item.get_tensor().add_names({std::to_string(sub_input_ids[index])});
+                    index++;
+                    break;
+                }
+                index++;
+            }
+        }
+        for (auto& item : sub_input_ids) {
+            if (item > 0) {
+                auto it = nodes.find(std::to_string(item));
+                named_inputs["X"].push_back(it->second);
+            }
+        }
+        auto sub_block_index_node = ov::opset7::Constant::create(ov::element::i32, {1}, {sub_block_id});
+        named_inputs["sub_block_index"].push_back(sub_block_index_node);
+    }
+    return named_inputs;
+};
 
-    return outputs;
-}
+NamedOutputs make_ng_node(const std::map<paddle::TensorName, Output<Node>>& nodes,
+                          const std::shared_ptr<BaseOpPlace>& op_place,
+                          const std::map<std::string, CreatorFunction>& CREATORS_MAP) {
+    if(auto proto_op_place = std::dynamic_pointer_cast<ProtoOpPlace>(op_place)) {
+        const auto& op_desc = proto_op_place->get_desc();
+
+        auto creator_it = CREATORS_MAP.find(op_desc.type());
+        FRONT_END_OP_CONVERSION_CHECK(creator_it != CREATORS_MAP.end(), "No creator found for ", op_desc.type(), " node.");
+        std::cout << "op.type()" << op_desc.type() << std::endl;
+        NamedInputs named_inputs;
+        for (const auto& input_port : op_desc.inputs()) {
+            for (const auto& in_tensor_name : input_port.arguments()) {
+                auto node_it = nodes.find(in_tensor_name);
+                // general check, because in case of error partial conversion should fail
+                FRONT_END_GENERAL_CHECK(node_it != nodes.end(),
+                        "Input ",
+                        in_tensor_name,
+                        " for node with type ",
+                        op_desc.type(),
+                        " wasn't found. It may happen if model was cut incorrectly.");
+                named_inputs[input_port.parameter()].push_back(node_it->second);
+            }
+        }
+        NamedOutputs outputs;
+        // In case the conversion function throws exception
+        try {
+            outputs = creator_it->second(paddle::NodeContext(proto_op_place->get_decoder(), named_inputs));
+        } catch (std::exception& ex) {
+            FRONT_END_OP_CONVERSION_CHECK(false, "Fail to convert " + op_desc.type() + " Exception " + ex.what());
+        }
 
-NamedOutputs make_framework_node(const std::map<paddle::TensorName, Output<Node>>& nodes,
-                                 const std::shared_ptr<OpPlace>& op_place) {
-    const auto& op_desc = op_place->get_desc();
-
-    OutputVector inputs_vector;
-    std::vector<std::string> inputs_names;
-    NamedOutputs named_outputs;
-    for (const auto& input_port : op_desc.inputs()) {
-        for (const auto& in_tensor_name : input_port.arguments()) {
-            auto it = nodes.find(in_tensor_name);
-            // general check, because in case of error partial conversion should fail
-            FRONT_END_GENERAL_CHECK(it != nodes.end(),
-                                    "Input ",
-                                    in_tensor_name,
-                                    " for node with type ",
-                                    op_desc.type(),
-                                    " wasn't found. It may happen if model was cut incorrectly.");
-            inputs_vector.push_back(it->second);
-            inputs_names.push_back(in_tensor_name);
+        return outputs;
+     } else if(auto json_op_place = std::dynamic_pointer_cast<JsonOpPlace>(op_place)){
+        auto type = json_op_place->get_op().type;
+        auto creator_it = CREATORS_MAP.find(type);
+        FRONT_END_OP_CONVERSION_CHECK(creator_it != CREATORS_MAP.end(), "No creator found for ", type, " node.");
+
+        NamedOutputs outputs;
+        // In case the conversion function throws exception
+        try {
+            outputs = creator_it->second(paddle::NodeContext(json_op_place->get_decoder(), create_name_inputs(nodes, json_op_place), true));
+        } catch (std::exception& ex) {
+            FRONT_END_OP_CONVERSION_CHECK(false, "Fail to convert " + json_op_place->get_op().type + " Exception " + ex.what());
         }
+        return outputs;
+    } else {
+        FRONT_END_OP_CONVERSION_CHECK(false, "failed to convert BaseOpPlace");
+        return {};
     }
+}
 
-    auto decoder_proto = std::dynamic_pointer_cast<DecoderProto>(op_place->get_decoder());
-    if (!decoder_proto)
-        FRONT_END_THROW("Failed to cast to DecoderProto.");
-    auto node = std::make_shared<FrameworkNode>(decoder_proto, inputs_vector, inputs_names);
+NamedOutputs make_framework_node(const std::map<paddle::TensorName, Output<Node>>& nodes,
+                                 const std::shared_ptr<BaseOpPlace>& op_place) {
+    if(auto proto_op_place = std::dynamic_pointer_cast<ProtoOpPlace>(op_place)) {
+        const auto& op_desc = proto_op_place->get_desc();
+
+        OutputVector inputs_vector;
+        std::vector<std::string> inputs_names;
+        NamedOutputs named_outputs;
+        for (const auto& input_port : op_desc.inputs()) {
+            for (const auto& in_tensor_name : input_port.arguments()) {
+                auto it = nodes.find(in_tensor_name);
+                // general check, because in case of error partial conversion should fail
+                FRONT_END_GENERAL_CHECK(it != nodes.end(),
+                        "Input ",
+                        in_tensor_name,
+                        " for node with type ",
+                        op_desc.type(),
+                        " wasn't found. It may happen if model was cut incorrectly.");
+                inputs_vector.push_back(it->second);
+                inputs_names.push_back(in_tensor_name);
+            }
+        }
+        auto decoder_proto = std::dynamic_pointer_cast<DecoderProto>(proto_op_place->get_decoder());
+        if (!decoder_proto)
+            FRONT_END_THROW("Failed to cast to DecoderProto.");
+        auto node = std::make_shared<FrameworkNode>(decoder_proto, inputs_vector, inputs_names);
+
+        return node->return_named_outputs();
+    } else if(auto json_op_place = std::dynamic_pointer_cast<JsonOpPlace>(op_place)) {
+        OutputVector inputs_vector;
+        std::vector<std::string> inputs_names;
+        NamedOutputs named_outputs;
+        auto op = json_op_place->get_op();
+        auto decoder_json = std::dynamic_pointer_cast<DecoderJson>(json_op_place->get_decoder());
+        if (!decoder_json)
+            FRONT_END_THROW("Failed to cast to DecoderJson.");
+        NamedInputs named_inputs = create_name_inputs(nodes, json_op_place);
+        for (auto& item : named_inputs) {
+            for (auto& input_node : item.second) {
+                inputs_vector.push_back(input_node);
+            }
+            inputs_names.push_back(item.first);
+        }
+        auto node = std::make_shared<FrameworkNode>(decoder_json, inputs_vector, inputs_names);
 
-    return node->return_named_outputs();
+        return node->return_named_outputs();
+    } else {
+        FRONT_END_OP_CONVERSION_CHECK(false, "failed to convert BaseOpPlace");
+        return {};
+    }
 }
 
 bool normalize_framework_node(const std::shared_ptr<FrameworkNode>& node,
@@ -120,7 +260,8 @@ bool normalize_framework_node(const std::shared_ptr<FrameworkNode>& node,
     auto creator_it = CREATORS_MAP.find(type);
     FRONT_END_OP_CONVERSION_CHECK(creator_it != CREATORS_MAP.end(), "No creator found for ", type, " node.");
 
-    auto new_node_outputs = creator_it->second(paddle::NodeContext(node->get_decoder(), node->get_named_inputs()));
+    auto new_node_outputs = creator_it->second(paddle::NodeContext(node->get_decoder(),
+                                                                   node->get_named_inputs(), node->is_json_decoder()));
     auto new_node = new_node_outputs.begin()->second[0].get_node_shared_ptr();
     new_node->set_friendly_name(node->get_friendly_name());
     auto node_outputs = node->return_named_outputs();
@@ -172,20 +313,21 @@ FrontEnd::FrontEnd() : m_op_translators(paddle::get_supported_ops()) {}
 std::vector<std::shared_ptr<ov::Model>> FrontEnd::convert_each_node(
     const std::shared_ptr<ov::frontend::InputModel>& frontend_model,
     std::function<std::map<std::string, OutputVector>(const std::map<std::string, Output<Node>>&,
-                                                      const std::shared_ptr<OpPlace>&)> func) {
+                                                      const std::shared_ptr<BaseOpPlace>&)> func) {
     auto model = std::dynamic_pointer_cast<InputModel>(frontend_model);
     FRONT_END_GENERAL_CHECK(model, "Invalid input model");
-    std::vector<std::shared_ptr<TensorPlace>> input_tensors;
-    std::vector<std::shared_ptr<TensorPlace>> output_tensors;
+    std::vector<std::shared_ptr<BaseTensorPlace>> input_tensors;
+    std::vector<std::shared_ptr<BaseTensorPlace>> output_tensors;
     for (const auto& _inp_place : model->get_inputs()) {
-        const auto& inp_place = std::dynamic_pointer_cast<TensorPlace>(_inp_place);
+        const auto& inp_place = std::dynamic_pointer_cast<BaseTensorPlace>(_inp_place);
         input_tensors.emplace_back(inp_place);
     }
     for (const auto& _outp_place : model->get_outputs()) {
-        const auto& outp_place = std::dynamic_pointer_cast<TensorPlace>(_outp_place);
+        const auto& outp_place = std::dynamic_pointer_cast<BaseTensorPlace>(_outp_place);
         output_tensors.emplace_back(outp_place);
     }
     auto funcs = convert_each_node_recursive(model, 0, input_tensors, output_tensors, func);
+    std::cout << "convert_each_node_recursive finised" << std::endl;
     std::vector<std::shared_ptr<Model>> funcs_vec;
     for (auto&& item : funcs) {
         funcs_vec.emplace_back(item.second);
@@ -199,150 +341,312 @@ std::vector<std::shared_ptr<ov::Model>> FrontEnd::convert_each_node(
 //  and 'while' ops
 using SubblockInfo = std::map<
     int32_t,
-    std::tuple<std::string, std::vector<std::shared_ptr<TensorPlace>>, std::vector<std::shared_ptr<TensorPlace>>>>;
-void try_update_sublock_info(const std::shared_ptr<OpPlace>& op_place, SubblockInfo& subblock_info) {
-    const auto& op_desc = op_place->get_desc();
-    if (op_desc.type() == "conditional_block") {
-        std::vector<std::shared_ptr<TensorPlace>> outp_tensors;
-        std::vector<std::shared_ptr<TensorPlace>> inp_tensors;
-
-        auto outp_ports = op_place->get_output_ports();
-        for (auto outp_port : outp_ports["Out"]) {
-            auto outp_tensor = outp_port->get_target_tensor_paddle();
-            outp_tensors.push_back(outp_tensor);
-        }
-        FRONT_END_GENERAL_CHECK(outp_tensors.size() > 0, "Port has no tensors connected.");
-
-        auto inp_ports = op_place->get_input_ports();
-        for (auto inp_port : inp_ports["Input"]) {
-            auto inp_tensor = inp_port->get_source_tensor_paddle();
-            inp_tensors.push_back(inp_tensor);
-        }
-
-        auto tmp_node = paddle::NodeContext(op_place->get_decoder(), paddle::NamedInputs());
-        auto block_idx = tmp_node.get_attribute<int32_t>("sub_block");
-
-        subblock_info[block_idx] = std::make_tuple(op_desc.type(), inp_tensors, outp_tensors);
-    } else if (op_desc.type() == "while") {
-        std::vector<std::shared_ptr<TensorPlace>> outp_tensors;
-        std::vector<std::shared_ptr<TensorPlace>> inp_tensors;
-
-        auto outp_ports = op_place->get_output_ports();
-        for (auto outp_port : outp_ports["Out"]) {
-            auto outp_tensor = outp_port->get_target_tensor_paddle();
-            outp_tensors.push_back(outp_tensor);
+    std::tuple<std::string, std::vector<std::shared_ptr<BaseTensorPlace>>, std::vector<std::shared_ptr<BaseTensorPlace>>>>;
+void try_update_sublock_info(const std::shared_ptr<BaseOpPlace>& base_op_place, SubblockInfo& subblock_info,
+        std::shared_ptr<ov::frontend::InputModel>  model = nullptr) {
+    if (auto op_place = std::dynamic_pointer_cast<ProtoOpPlace>(base_op_place)) {
+        const auto& op_desc = op_place->get_desc();
+        if (op_desc.type() == "conditional_block") {
+            std::vector<std::shared_ptr<BaseTensorPlace>> outp_tensors;
+            std::vector<std::shared_ptr<BaseTensorPlace>> inp_tensors;
+            auto outp_ports = op_place->get_output_ports();
+            for (auto outp_port : outp_ports["Out"]) {
+                auto outp_tensor = std::dynamic_pointer_cast<BaseTensorPlace>(outp_port->get_target_tensor_paddle());
+                outp_tensors.push_back(outp_tensor);
+            }
+            FRONT_END_GENERAL_CHECK(outp_tensors.size() > 0, "Port has no tensors connected.");
+            auto inp_ports = op_place->get_input_ports();
+            for (auto inp_port : inp_ports["Input"]) {
+                auto inp_tensor = std::dynamic_pointer_cast<BaseTensorPlace>(inp_port->get_source_tensor_paddle());
+                inp_tensors.push_back(inp_tensor);
+            }
+            auto tmp_node = paddle::NodeContext(op_place->get_decoder(), paddle::NamedInputs(), true);
+            auto block_idx = tmp_node.get_attribute<int32_t>("sub_block");
+            subblock_info[block_idx] = std::make_tuple(op_desc.type(), inp_tensors, outp_tensors);
+        } else if (op_desc.type() == "while") {
+            std::vector<std::shared_ptr<BaseTensorPlace>> outp_tensors;
+            std::vector<std::shared_ptr<BaseTensorPlace>> inp_tensors;
+            auto outp_ports = op_place->get_output_ports();
+            for (auto outp_port : outp_ports["Out"]) {
+                auto outp_tensor = std::dynamic_pointer_cast<BaseTensorPlace>(outp_port->get_target_tensor_paddle());
+                outp_tensors.push_back(outp_tensor);
+            }
+            FRONT_END_GENERAL_CHECK(outp_tensors.size() > 0, "Port has no tensors connected.");
+            auto inp_ports = op_place->get_input_ports();
+            for (auto inp_port : inp_ports["X"]) {
+                auto inp_tensor = std::dynamic_pointer_cast<BaseTensorPlace>(inp_port->get_source_tensor_paddle());
+                inp_tensors.push_back(inp_tensor);
+            }
+            FRONT_END_GENERAL_CHECK(inp_tensors.size() > 0, "Port has no tensors connected.");
+            auto tmp_node = paddle::NodeContext(op_place->get_decoder(), paddle::NamedInputs(), true);
+            auto block_idx = tmp_node.get_attribute<int32_t>("sub_block");
+            subblock_info[block_idx] = std::make_tuple(op_desc.type(), inp_tensors, outp_tensors);
         }
-        FRONT_END_GENERAL_CHECK(outp_tensors.size() > 0, "Port has no tensors connected.");
-
-        auto inp_ports = op_place->get_input_ports();
-        for (auto inp_port : inp_ports["X"]) {
-            auto inp_tensor = inp_port->get_source_tensor_paddle();
-            inp_tensors.push_back(inp_tensor);
+    } else if (auto op_place = std::dynamic_pointer_cast<JsonOpPlace>(base_op_place)) {
+        auto op = op_place->get_op();
+        if (op.type == "if" || op.type == "while") {
+            // get inputs and output block
+            for (auto& block_index : op.sub_block_idxs) {
+                auto& input_ids = op.get_sub_inputs_ids(block_index);
+                auto& output_ids = op.get_sub_outputs_ids(block_index);
+                std::vector<std::shared_ptr<BaseTensorPlace>> outp_tensors;
+                std::vector<std::shared_ptr<BaseTensorPlace>> inp_tensors;
+                for (auto id : input_ids)  {
+                    auto port_name = std::to_string(id);
+                    auto place = model->get_place_by_tensor_name(port_name);
+                    FRONT_END_OP_CONVERSION_CHECK(place != nullptr, "find the input:", port_name);
+                    auto tensor_place = std::dynamic_pointer_cast<BaseTensorPlace>(place);
+                    inp_tensors.push_back(tensor_place);
+                }
+                for (auto id : output_ids)  {
+                    auto port_name = std::to_string(id);
+                    auto place = model->get_place_by_tensor_name(port_name);
+                    FRONT_END_OP_CONVERSION_CHECK(place != nullptr, "find the input:", port_name);
+                    auto tensor_place = std::dynamic_pointer_cast<BaseTensorPlace>(place);
+                    outp_tensors.push_back(tensor_place);
+                }
+                subblock_info[block_index] = std::make_tuple(op.type, inp_tensors, outp_tensors);
+            }
         }
-        FRONT_END_GENERAL_CHECK(inp_tensors.size() > 0, "Port has no tensors connected.");
-
-        auto tmp_node = paddle::NodeContext(op_place->get_decoder(), paddle::NamedInputs());
-        auto block_idx = tmp_node.get_attribute<int32_t>("sub_block");
-
-        subblock_info[block_idx] = std::make_tuple(op_desc.type(), inp_tensors, outp_tensors);
+    } else {
+        FRONT_END_GENERAL_CHECK(false, "haven't implement for json format model");
     }
 }
 
 std::map<int32_t, std::shared_ptr<ov::Model>> FrontEnd::convert_each_node_recursive(
-    const std::shared_ptr<ov::frontend::InputModel>& frontend_model,
-    const int32_t block_idx,
-    const std::vector<std::shared_ptr<TensorPlace>>& input_tensors,
-    const std::vector<std::shared_ptr<TensorPlace>>& output_tensors,
-    std::function<std::map<std::string, OutputVector>(const std::map<std::string, Output<Node>>&,
-                                                      const std::shared_ptr<OpPlace>&)> func) {
+         const std::shared_ptr<ov::frontend::InputModel>& frontend_model,
+         const int32_t block_idx,
+         const std::vector<std::shared_ptr<BaseTensorPlace>>& input_tensors,
+         const std::vector<std::shared_ptr<BaseTensorPlace>>& output_tensors,
+         std::function<std::map<std::string, OutputVector>(const std::map<std::string, Output<Node>>&,
+         const std::shared_ptr<BaseOpPlace>&) > func) {
     auto model = std::dynamic_pointer_cast<InputModel>(frontend_model);
     FRONT_END_GENERAL_CHECK(model, "Invalid input model");
     auto nodes_dict(model->get_tensor_values());
     ParameterVector parameter_nodes;
     ResultVector result_nodes;
     OutputVector output_nodes;
-
     SubblockInfo subblock_inputs_outputs;  // keep info of controlflow ops
-
     for (const auto& _inp_place : input_tensors) {
-        const auto& inp_place = std::dynamic_pointer_cast<TensorPlace>(_inp_place);
-        const auto& var = inp_place->get_desc();
-        const auto& shape = inp_place->get_partial_shape();
-        const auto& type = inp_place->get_element_type();
-        auto param = std::make_shared<Parameter>(type, shape);
-        param->set_friendly_name(var.name());
-        param->output(0).get_tensor().add_names({var.name()});
-        nodes_dict[var.name()] = param;
-        parameter_nodes.push_back(param);
+        if (const auto& inp_place = std::dynamic_pointer_cast<ProtoTensorPlace>(_inp_place)) {
+            const auto& var = inp_place->get_desc();
+            const auto& shape = inp_place->get_partial_shape();
+            const auto& type = inp_place->get_element_type();
+            auto param = std::make_shared<Parameter>(type, shape);
+            param->set_friendly_name(var.name());
+            param->output(0).get_tensor().add_names({var.name()});
+            nodes_dict[var.name()] = param;
+            parameter_nodes.push_back(param);
+        } else if (const auto& json_place = std::dynamic_pointer_cast<JsonTensorPlace>(_inp_place)) {
+            const auto& port = json_place->get_port();
+            const auto& port_name = std::to_string(port.id);
+            const auto shape =  ov::PartialShape(port.get_shapes());
+            const auto type = json::convert_to_ov_type(port.get_precision());
+            auto param = std::make_shared<Parameter>(type, shape);
+            param->set_friendly_name(port_name);
+            param->output(0).get_tensor().add_names({port_name});
+            nodes_dict[port_name] = param;
+            parameter_nodes.push_back(param);
+        } else {
+            FRONT_END_OP_CONVERSION_CHECK(false, "convert BaseTensorPlace failed ");
+        }
     }
-
     const auto& op_places = model->get_op_places(block_idx);
-    for (const auto& op_place : op_places) {
-        const auto& op_desc = op_place->get_desc();
-        if (op_desc.type() == "feed" || op_desc.type() == "fetch") {
-            // inputs and outputs are stored in the model already
-            continue;
-        } else {
-            try_update_sublock_info(op_place, subblock_inputs_outputs);
-
-            paddle::NamedOutputs named_outputs = func(nodes_dict, op_place);
+    for (const auto& base_op_place : op_places) {
+        if (const auto& op_place = std::dynamic_pointer_cast<ProtoOpPlace>(base_op_place)) {
+            const auto& op_desc = op_place->get_desc();
+            if (op_desc.type() == "feed" || op_desc.type() == "fetch") {
+                // inputs and outputs are stored in the model already
+                continue;
+            } else {
+                try_update_sublock_info(op_place, subblock_inputs_outputs);
+                paddle::NamedOutputs named_outputs = func(nodes_dict, op_place);
+                if (!named_outputs.empty()) {
+                    if (!op_desc.outputs().begin()->arguments().empty()) {
+                        const auto& tensor_name = op_desc.outputs().begin()->arguments()[0];
+                        auto node = named_outputs.begin()->second[0].get_node_shared_ptr();
+                        node->set_friendly_name(tensor_name);
+                        std::cout << "block index:" << block_idx <<  " tensor_name:" << tensor_name << std::endl;
+                    }
 
-            if (!named_outputs.empty()) {
-                if (!op_desc.outputs().begin()->arguments().empty()) {
-                    const auto& tensor_name = op_desc.outputs().begin()->arguments()[0];
+                    const auto& out_ports = op_desc.outputs();
+                    for (const auto& port : out_ports) {
+                        // TODO: figure a way to safely handle unused outputs
+                        if (named_outputs.count(port.parameter())) {
+                            const auto& ng_outputs = named_outputs.at(port.parameter());
+                            FRONT_END_OP_CONVERSION_CHECK(ng_outputs.size() == (size_t)port.arguments_size(),
+                                                          "The number of output tensors must be equal to "
+                                                          "the number of outputs of the OV node.");
+                            for (size_t idx = 0; idx < ng_outputs.size(); ++idx) {
+                                const auto& var_name = port.arguments()[static_cast<int>(idx)];
+                                ng_outputs[idx].get_tensor().set_names({var_name});
+                                // if nodes_dict already has node mapped to this tensor name it
+                                // usually means that it was overwritten using set_tensor_value
+                                nodes_dict[var_name] = ng_outputs[idx];
+                                std::cout << "block index:" << block_idx << " var_name:" << var_name << std::endl;
+                            }
+                        }
+                    }
+                }
+            }
+        } else if (const auto& op_place = std::dynamic_pointer_cast<JsonOpPlace>(base_op_place)) {
+            const auto& op = op_place->get_op();
+            if (op.type == "data" || op.type == "fetch" || op.type == "yield") {
+                // inputs and outputs are stored in the model already
+                continue;
+            } else if (op.is_parameter) {
+                // const are stored in the model already
+                continue;
+            } else if (op.type == "combine") {
+                // save output of combine as node name_0 name_1 name_2
+                FRONT_END_OP_CONVERSION_CHECK(op.outputPorts.size() == 1, "combine only has one output");
+                const auto& port = op.outputPorts[0];
+                if (!port.used)  {
+                    continue;
+                }
+                auto output_port_name_base = std::to_string(port.id);
+                size_t combine_index = 0;
+                for (const auto& inputId : op.inputIds) {
+                    if (op.unusedInputIds.find(inputId) != op.unusedInputIds.end()) {
+                        continue;
+                    }
+                    auto input_name = std::to_string(inputId);
+                    auto node_it = nodes_dict.find(input_name);
+                    auto output_port_name = output_port_name_base + "_" + std::to_string(combine_index);
+                    nodes_dict[output_port_name] = node_it->second;
+                    combine_index++;
+                }
+            } else if (op.type == "split") {
+                auto& inputIds = op.inputIds;
+                FRONT_END_OP_CONVERSION_CHECK(inputIds.size() == 1, "split only has one input, but size:", inputIds.size());
+                size_t split_index = 0;
+                for (const auto& port : op.outputPorts) {
+                    auto input_name_base = std::to_string(inputIds[0]);
+                    auto input_port_name = input_name_base + "_" + std::to_string(split_index);
+                    auto it = nodes_dict.find(input_port_name);
+                    FRONT_END_OP_CONVERSION_CHECK(it != nodes_dict.end(),
+                        "split can't find the input:", input_name_base, " index:", split_index);
+                    auto output_port_name = std::to_string(port.id);
+                    nodes_dict[output_port_name] = it->second;
+                    split_index++;
+                }
+            } else {
+                try_update_sublock_info(op_place, subblock_inputs_outputs, frontend_model);
+                paddle::NamedOutputs named_outputs = func(nodes_dict, op_place);
+                if (!named_outputs.empty()) {
+                    const auto& tensor_name = op.name;
                     auto node = named_outputs.begin()->second[0].get_node_shared_ptr();
-                    node->set_friendly_name(tensor_name);
+                    if(!tensor_name.empty()) {
+                        node->set_friendly_name(tensor_name);
+                    } else {
+                        std::string node_name = op.type;
+                        for (const auto& port : op.outputPorts) {
+                            node_name += "/" + std::to_string(port.id);
+                        }
+                        node->set_friendly_name(node_name);
+                    }
+                    std::cout << "node_name:" << node->get_friendly_name() << std::endl;
+                    auto output_name = get_output_name_by_op_type(op.type);
+                    size_t name_idx = 0;
+                    size_t output_idx = 0;
+                    for (const auto& port : op.outputPorts) {
+                        if (!port.used)  {
+                            if (op.type == "unique" || op.type == "matrix_nms" || op.type == "multiclass_nms3") name_idx++;
+                            continue;
+                        }
+                        // cal
+                        std::string port_name = std::to_string(port.id);
+                        FRONT_END_OP_CONVERSION_CHECK(name_idx < output_name.size(), "idx is greater than output name size, idx:", name_idx);
+                        auto it = named_outputs.find(output_name[name_idx]);
+                        std::cout << "port_name:" << port_name << "output_name[" << name_idx << "]:" << output_name[name_idx] << std::endl;
+                        FRONT_END_OP_CONVERSION_CHECK(it != named_outputs.end(), "can't find output name", output_name[name_idx]);
+                        const auto& ng_outputs = it->second;
+                        FRONT_END_OP_CONVERSION_CHECK(ng_outputs.size() > 0, "at least one output for output name ", output_name[name_idx]);
+                        if (ng_outputs.size() == 1) {
+                            nodes_dict[port_name] = ng_outputs[0];
+                            name_idx++;
+                        } else if (op.type == "if") {
+                           // subblock may has multi outputs in one name out
+                           nodes_dict[port_name] = ng_outputs[output_idx];
+                           output_idx++;
+                           if (output_idx == ng_outputs.size()) {
+                               output_idx = 0;
+                           }
+                        } else if (port.type == "t_vec") {
+                            // split has multi output, use name_0, name_1, name_2 to save output
+                           size_t split_index = 0;
+                           for (const auto& ng_output : ng_outputs) {
+                               auto output_port_name = port_name + "_" + std::to_string(split_index);
+                               nodes_dict[output_port_name] = ng_output;
+                               split_index++;
+                           }
+                           name_idx++;
+                        } else if (op.type == "while") {
+                            nodes_dict[port_name] = ng_outputs[output_idx];
+                            output_idx++;
+                        } else {
+                            FRONT_END_OP_CONVERSION_CHECK(false,
+                                "port name:", port.id, "  can't much output name:", output_name[name_idx]);
+                        }
+                    }
                 }
-
-                const auto& out_ports = op_desc.outputs();
-                for (const auto& port : out_ports) {
-                    // TODO: figure a way to safely handle unused outputs
-                    if (named_outputs.count(port.parameter())) {
-                        const auto& ng_outputs = named_outputs.at(port.parameter());
-                        FRONT_END_OP_CONVERSION_CHECK(ng_outputs.size() == (size_t)port.arguments_size(),
-                                                      "The number of output tensors must be equal to "
-                                                      "the number of outputs of the OV node.");
-                        for (size_t idx = 0; idx < ng_outputs.size(); ++idx) {
-                            const auto& var_name = port.arguments()[static_cast<int>(idx)];
-                            ng_outputs[idx].get_tensor().set_names({var_name});
-                            // if nodes_dict already has node mapped to this tensor name it
-                            // usually means that it was overwritten using set_tensor_value
-                            nodes_dict[var_name] = ng_outputs[idx];
+                // match args to while input
+                if (op.type == "while") {
+                    auto sub_block_id = op.sub_block_idxs[0];
+                    auto& sub_input_ids = op.get_sub_inputs_ids(sub_block_id);
+                    auto& while_inputs = op.inputIds;
+                    // skip Condition
+                    size_t index = 1;
+                    for (auto& input_id : sub_input_ids) {
+                        if (input_id < 0 ) {
+                            nodes_dict[std::to_string(input_id)] = nodes_dict[std::to_string(while_inputs[index])];
+                            index++;
                         }
                     }
                 }
             }
+        } else {
+            FRONT_END_OP_CONVERSION_CHECK(false, "convert BaseTensorPlace failed ");
         }
     }
-
     for (const auto& _outp_place : output_tensors) {
-        const auto& outp_place = std::dynamic_pointer_cast<TensorPlace>(_outp_place);
-        auto var = outp_place->get_desc();
-        auto input_var_name = var.name();
-        auto result = std::make_shared<Result>(nodes_dict.at(input_var_name));
-        result->set_friendly_name(input_var_name + "/Result");
-        result_nodes.push_back(result);
-        output_nodes.push_back(nodes_dict.at(input_var_name));
+        if (const auto& outp_place = std::dynamic_pointer_cast<ProtoTensorPlace>(_outp_place)) {
+            auto var = outp_place->get_desc();
+            auto input_var_name = var.name();
+            auto result = std::make_shared<Result>(nodes_dict.at(input_var_name));
+            std::cout << "input_var_name:" << input_var_name << std::endl;
+            result->set_friendly_name(input_var_name + "/Result");
+            result_nodes.push_back(result);
+            output_nodes.push_back(nodes_dict.at(input_var_name));
+        } else if (const auto& outp_place = std::dynamic_pointer_cast<JsonTensorPlace>(_outp_place)) {
+            auto port = outp_place->get_port();
+            auto input_var_name = std::to_string(port.id);
+            std::cout << "input_var_name:" << input_var_name << std::endl;
+            auto result = std::make_shared<Result>(nodes_dict.at(input_var_name));
+            result->set_friendly_name(input_var_name + "/Result");
+            result_nodes.push_back(result);
+            output_nodes.push_back(nodes_dict.at(input_var_name));
+        } else {
+            FRONT_END_OP_CONVERSION_CHECK(false, "convert BaseTensorPlace failed ");
+        }
     }
-
     std::shared_ptr<ov::Model> main_block_func;
+    std::cout << "parameter_nodes.size():" << parameter_nodes.size() << std::endl;
     if (parameter_nodes.size() > 0) {
         main_block_func = std::make_shared<ov::Model>(result_nodes, parameter_nodes);
     } else {
         main_block_func = std::make_shared<ov::Model>(output_nodes);
     }
-
     // convert each sub block
+    // ov::save_model(main_block_func, "resnet.xml");
     std::map<int32_t, std::shared_ptr<ov::Model>> block_funcs;
     block_funcs.insert({block_idx, main_block_func});
-
     for (auto& item : subblock_inputs_outputs) {
         auto ctl_op_info = item.second;
         auto sub_block_func =
             convert_each_node_recursive(model, item.first, std::get<1>(ctl_op_info), std::get<2>(ctl_op_info), func);
         block_funcs.insert(sub_block_func.begin(), sub_block_func.end());
     }
-
     return block_funcs;
 }
 
@@ -352,6 +656,7 @@ void FrontEnd::try_remove_internal_ops(const std::vector<std::shared_ptr<Model>>
         manager.register_pass<ov::frontend::paddle::pass::TransformTensorArray>(models);
         manager.register_pass<ov::frontend::paddle::pass::TransformIf>(models);
         manager.register_pass<ov::frontend::paddle::pass::TransformWhile>(models);
+        manager.register_pass<ov::frontend::paddle::pass::TransformIfElse>(models);
         manager.run_passes(model);
     }
     if (models.size() > 0) {
@@ -382,27 +687,30 @@ bool FrontEnd::supported_impl(const std::vector<ov::Any>& variants) const {
     // Validating first path, it must contain a model
     if (variants[0].is<std::string>()) {
         std::string suffix = ".pdmodel";
+        std::string json_suffix = ".json";
         std::string model_path = variants[0].as<std::string>();
         FRONT_END_GENERAL_CHECK(util::file_exists(model_path), "Could not open the file: \"", model_path, '"');
-        if (!ov::util::ends_with(model_path, suffix)) {
+        if (!ov::util::ends_with(model_path, suffix) && !ov::util::ends_with(model_path, json_suffix)) {
             model_path += paddle::get_path_sep<char>() + "__model__";
         }
         std::ifstream model_str(model_path, std::ios::in | std::ifstream::binary);
         // It is possible to validate here that protobuf can read model from the stream,
         // but it will complicate the check, while it should be as quick as possible
-        return model_str && model_str.is_open();
+        return  model_str && model_str.is_open();
     }
 #if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
     else if (variants[0].is<std::wstring>()) {
         std::wstring suffix = L".pdmodel";
+        std::wstring json_suffix = L".json";
         std::wstring model_path = variants[0].as<std::wstring>();
         FRONT_END_GENERAL_CHECK(util::file_exists(model_path),
                                 "Could not open the file: \"",
                                 util::path_to_string(model_path),
                                 '"');
-        if (!ov::util::ends_with(model_path, suffix)) {
+        if (!ov::util::ends_with(model_path, suffix) && !ov::util::ends_with(model_path, json_suffix)) {
             model_path += paddle::get_path_sep<wchar_t>() + L"__model__";
         }
+
         std::ifstream model_str(model_path.c_str(), std::ios::in | std::ifstream::binary);
         // It is possible to validate here that protobuf can read model from the stream,
         // but it will complicate the check, while it should be as quick as possible
@@ -421,6 +729,18 @@ bool FrontEnd::supported_impl(const std::vector<ov::Any>& variants) const {
         // step 2:
         // reset the stream position to the beginning.
         p_model_stream->seekg(0, p_model_stream->beg);
+        if (!ret) {
+            try {
+                std::stringstream buffer;
+                buffer << p_model_stream->rdbuf();
+                nlohmann::json data = nlohmann::json::parse(buffer);
+                p_model_stream->seekg(0, p_model_stream->beg);
+                ret = true;
+            } catch (nlohmann::json::parse_error& e) {
+                p_model_stream->seekg(0, p_model_stream->beg);
+                ret = false;
+            }
+        }
         return ret;
     }
     return false;
@@ -479,13 +799,14 @@ std::shared_ptr<ov::Model> FrontEnd::convert(const InputModel::Ptr& model) const
 
     auto f = convert_each_node(
         paddle_model,
-        [&](const std::map<std::string, Output<Node>>& nodes_dict, const std::shared_ptr<OpPlace>& op_place) {
+        [&](const std::map<std::string, Output<Node>>& nodes_dict, const std::shared_ptr<BaseOpPlace>& op_place) {
             return paddle::make_ng_node(nodes_dict, op_place, m_op_translators);
         });
 
     fuse_fakequantize_ops(f);
     try_remove_internal_ops(f);
     normalize(f[0]);
+    ov::save_model(f[0], "test.xml", false);
     return f[0];
 }
 
@@ -522,7 +843,7 @@ std::shared_ptr<ov::Model> FrontEnd::convert_partially(const InputModel::Ptr& mo
 
     auto f = convert_each_node(
         paddle_model,
-        [&](const std::map<std::string, Output<Node>>& nodes_dict, const std::shared_ptr<OpPlace>& op_place) {
+        [&](const std::map<std::string, Output<Node>>& nodes_dict, const std::shared_ptr<BaseOpPlace>& op_place) {
             paddle::NamedOutputs named_outputs;
             try {
                 named_outputs = paddle::make_ng_node(nodes_dict, op_place, m_op_translators);
@@ -542,6 +863,7 @@ std::shared_ptr<ov::Model> FrontEnd::decode(const InputModel::Ptr& model) const
     auto paddle_model = std::dynamic_pointer_cast<InputModel>(model);
     FRONT_END_GENERAL_CHECK(paddle_model != nullptr, "Invalid input model");
 
+    // FRONT_END_GENERAL_CHECK(false, "haven't implement");
     auto f = convert_each_node(paddle_model, paddle::make_framework_node);
     FRONT_END_GENERAL_CHECK(f.size() == 1, "Input model has subblocks, currently 'decode' could not support it");
     return f[0];
diff --git a/src/frontends/paddle/src/input_model.cpp b/src/frontends/paddle/src/input_model.cpp
index 33faec4e4f..5f7caf236d 100644
--- a/src/frontends/paddle/src/input_model.cpp
+++ b/src/frontends/paddle/src/input_model.cpp
@@ -20,579 +20,54 @@
 #include "openvino/util/file_util.hpp"
 #include "paddle_utils.hpp"
 #include "place.hpp"
+#include "json_input_model_imp.hpp"
+#include "proto_input_model_imp.hpp"
 
 namespace ov {
 namespace frontend {
 namespace paddle {
 
 using namespace ::paddle::framework::proto;
-
-class InputModel::InputModelImpl {
-public:
-    template <typename T>
-    InputModelImpl(const std::basic_string<T>& path,
-                   const InputModel& input_model,
-                   const std::shared_ptr<TelemetryExtension>& telemetry);
-    InputModelImpl(const std::vector<std::istream*>& streams,
-                   const InputModel& input_model,
-                   const std::shared_ptr<TelemetryExtension>& telemetry);
-    std::vector<Place::Ptr> get_inputs() const;
-    std::vector<Place::Ptr> get_outputs() const;
-    int64_t get_version() const {
-        return m_fw_ptr->version().version();
-    }
-    Place::Ptr get_place_by_tensor_name(const std::string& tensorName) const;
-    void override_all_outputs(const std::vector<Place::Ptr>& outputs);
-    void override_all_inputs(const std::vector<Place::Ptr>& inputs);
-    void extract_subgraph(const std::vector<Place::Ptr>& inputs, const std::vector<Place::Ptr>& outputs);
-    void set_default_shape(Place::Ptr place, const ov::Shape&);
-    void set_partial_shape(Place::Ptr place, const ov::PartialShape&);
-    ov::PartialShape get_partial_shape(Place::Ptr place) const;
-    void set_element_type(Place::Ptr place, const ov::element::Type&);
-    ov::element::Type get_element_type(const Place::Ptr& place) const;
-    void set_tensor_value(Place::Ptr place, const void* value);
-    std::vector<std::shared_ptr<OpPlace>> get_op_places(const int32_t blck_idx) const;
-    std::map<std::string, std::shared_ptr<TensorPlace>> get_var_places() const {
-        return m_var_places;
-    }
-    std::map<paddle::TensorName, Output<Node>> get_tensor_values() const {
-        return m_tensor_values;
-    };
-
-private:
-    void load_places();
-    template <typename T>
-    void load_consts(const std::basic_string<T>& folder_with_weights);
-    void load_consts(std::istream* weight_stream);
-    void create_temp_consts();
-    std::vector<std::shared_ptr<OpPlace>> determine_cut_nodes() const;
-
-    std::vector<std::vector<std::shared_ptr<OpPlace>>> m_op_places;
-    std::map<std::string, std::shared_ptr<TensorPlace>> m_var_places;
-    std::shared_ptr<ProgramDesc> m_fw_ptr;
-    const InputModel& m_input_model;
-    std::vector<Place::Ptr> m_inputs;
-    std::vector<Place::Ptr> m_outputs;
-    std::map<paddle::TensorName, Output<Node>> m_tensor_values;
-
-    std::shared_ptr<TelemetryExtension> m_telemetry;
-
-    // shows if some nodes might be deleted from graph
-    bool m_graph_changed = false;
-};
-
-void InputModel::InputModelImpl::load_places() {
-    const int cnt_of_blocks = m_fw_ptr->blocks_size();
-    const auto& blocks = m_fw_ptr->blocks();
-    std::map<std::string, uint64_t> op_statistics;
-
-    m_op_places.resize(cnt_of_blocks);
-
-    for (int block_idx = 0; block_idx < cnt_of_blocks; block_idx++) {
-        const auto& block = blocks[block_idx];
-
-        for (const auto& var : block.vars()) {
-            m_var_places[var.name()] = std::make_shared<TensorPlace>(m_input_model, var);
-        }
-
-        for (const auto& op : block.ops()) {
-            auto op_place = std::make_shared<OpPlace>(m_input_model, op);
-            op_place->set_decoder(std::make_shared<DecoderProto>(op_place));
-
-            if (m_telemetry) {
-                op_statistics[op.type()]++;
-            }
-
-            m_op_places[block_idx].push_back(op_place);
-
-            for (const auto& output : op.outputs()) {
-                for (const auto& var_name : output.arguments()) {
-                    auto out_port = std::make_shared<OutPortPlace>(m_input_model);
-
-                    // connect out_port and tensor
-                    const auto& tensor = m_var_places.at(var_name);
-                    tensor->add_producing_port(out_port);
-                    out_port->set_target_tensor(tensor);
-
-                    // connect out_port and op
-                    op_place->add_out_port(out_port, output.parameter());
-                    out_port->set_op(op_place);
-                }
-            }
-
-            for (const auto& input : op.inputs()) {
-                for (const auto& var_name : input.arguments()) {
-                    auto in_port = std::make_shared<InPortPlace>(m_input_model);
-
-                    // connect in_port and tensor
-                    const auto& tensor = m_var_places.at(var_name);
-                    tensor->add_consuming_port(in_port);
-                    in_port->set_source_tensor(tensor);
-
-                    // connect in_port and op
-                    op_place->add_in_port(in_port, input.parameter());
-                    in_port->set_op(op_place);
-                }
-            }
-
-            // Determine outputs and inputs
-            if (op.type() == "feed") {
-                const auto& place = op_place->get_output_port_paddle("Out", 0);
-                const auto& var_place = std::dynamic_pointer_cast<TensorPlace>(place->get_target_tensor_paddle());
-                const auto& tensor_desc = var_place->get_desc().type().lod_tensor().tensor();
-                const auto& dims = tensor_desc.dims();
-
-                var_place->set_element_type(get_ov_type(tensor_desc.data_type()));
-                var_place->set_partial_shape(PartialShape(std::vector<Dimension>(dims.begin(), dims.end())));
-                m_inputs.push_back(var_place);
-            } else if (op.type() == "fetch") {
-                auto place = op_place->get_input_port_paddle("X", 0);
-                m_outputs.push_back(place->get_source_tensor_paddle());
-            }
-        }
-    }
-    if (m_telemetry) {
-        for (const auto& op : op_statistics) {
-            m_telemetry->send_event("op_count", "paddle_" + op.first, static_cast<int>(op.second));
-        }
-    }
-}
-
-namespace {
-bool read_tensor(std::istream& is, char* data, size_t len) {
-    is.read(data, len);
-    return (size_t)is.gcount() == len;
-}
-
-template <typename T>
-std::basic_string<T> get_const_path(const std::basic_string<T>& folder_with_weights, const std::string& name) {
-    return folder_with_weights + paddle::get_path_sep<T>() + name;
-}
-
-#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
-template <>
-std::basic_string<wchar_t> get_const_path(const std::basic_string<wchar_t>& folder, const std::string& name) {
-    return folder + paddle::get_path_sep<wchar_t>() + ov::util::string_to_wstring(name);
-}
-#endif
-
-template <typename T>
-bool is_pdmodel(const std::basic_string<T>& path) {
-    std::string ext = ".pdmodel";
-    return ov::util::ends_with(path, ext);
-}
-
-#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
-template <>
-bool is_pdmodel(const std::basic_string<wchar_t>& path) {
-    std::wstring ext = L".pdmodel";
-    return ov::util::ends_with(path, ext);
-}
-#endif
-
-template <typename T>
-std::basic_string<T> get_model_path(const std::basic_string<T>& path, std::ifstream* weights_stream) {
+InputModel::InputModel(const std::string& path, const std::shared_ptr<TelemetryExtension>& telemetry) {
     std::string model_file{path};
-    std::string ext = ".pdmodel";
+    std::string ext = ".json";
     if (ov::util::ends_with(model_file, ext)) {
-        std::string params_ext = ".pdiparams";
-        std::string weights_file{path};
-        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
-        weights_stream->open(weights_file, std::ios::binary);
-        // Don't throw error if file isn't opened
-        // It may mean that model don't have constants
+        _impl = std::make_shared<json::JsonInputModelImpl>(path, *this, telemetry);
     } else {
-        model_file += paddle::get_path_sep<T>() + "__model__";
+        _impl = std::make_shared<proto::ProtoInputModelImpl>(path, *this, telemetry);
     }
-    return model_file;
 }
 
 #if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
-template <>
-std::basic_string<wchar_t> get_model_path(const std::basic_string<wchar_t>& path, std::ifstream* weights_stream) {
-    std::wstring model_file{path};
-    std::wstring ext = L".pdmodel";
+InputModel::InputModel(const std::wstring& path, const std::shared_ptr<TelemetryExtension>& telemetry) {
+    std::string model_file{path};
+    std::string ext = ".json";
     if (ov::util::ends_with(model_file, ext)) {
-        std::wstring params_ext = L".pdiparams";
-        std::wstring weights_file{path};
-        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
-        weights_stream->open(weights_file.c_str(), std::ios::binary);
-        // Don't throw error if file isn't opened
-        // It may mean that model don't have constants
+        _impl = std::make_shared<json::JsonInputModelImpl>(path, *this, telemetry);
     } else {
-        model_file += paddle::get_path_sep<wchar_t>() + L"__model__";
+        _impl = std::make_shared<proto::ProtoInputModelImpl>(path, *this, telemetry);
     }
-    return model_file;
 }
 #endif
-}  // namespace
 
-std::vector<std::shared_ptr<OpPlace>> InputModel::InputModelImpl::get_op_places(const int32_t blck_idx) const {
-    if (m_graph_changed) {
-        return determine_cut_nodes();
+InputModel::InputModel(const std::vector<std::istream*>& streams, const std::shared_ptr<TelemetryExtension>& telemetry) {
+    try {
+        std::stringstream buffer;
+        buffer << streams[0]->rdbuf();
+        nlohmann::json data = nlohmann::json::parse(buffer);
+        streams[0]->seekg(0, streams[0]->beg);
+        _impl = std::make_shared<json::JsonInputModelImpl>(streams, *this, telemetry);
+    } catch (nlohmann::json::parse_error& e) {
+        streams[0]->seekg(0, streams[0]->beg);
+        _impl = std::make_shared<proto::ProtoInputModelImpl>(streams, *this, telemetry);
     }
-    if (static_cast<size_t>(blck_idx) < m_op_places.size())
-        return m_op_places[blck_idx];
-    return {};
 }
 
-std::vector<std::shared_ptr<OpPlace>> InputModel::InputModelImpl::determine_cut_nodes() const {
-    std::queue<OpPlace*> q;
-    std::unordered_set<OpPlace*> visited;
-    std::vector<std::shared_ptr<OpPlace>> new_op_places;
-    new_op_places.reserve(m_op_places[0].size());
-    // Marking nodes from outputs to inputs/constants
-    for (const auto& output : get_outputs()) {
-        if (!output->is_input()) {
-            auto paddle_output_op = std::dynamic_pointer_cast<OpPlace>(output->get_producing_operation());
-            FRONT_END_GENERAL_CHECK(paddle_output_op != nullptr, "Output doesn't have producing operation");
-            if (!visited.count(paddle_output_op.get())) {
-                visited.insert(paddle_output_op.get());
-                q.push(paddle_output_op.get());
-                new_op_places.push_back(paddle_output_op);
-            }
-        }
-    }
-    while (!q.empty()) {
-        auto p_op = q.front();
-        q.pop();
-        for (const auto& map_pair : p_op->get_input_ports()) {
-            for (const auto& port : map_pair.second) {
-                auto tensor = port->get_source_tensor();
-                if (tensor && !tensor->is_input() && !m_tensor_values.count(tensor->get_names()[0])) {
-                    std::shared_ptr<OpPlace> paddle_op =
-                        std::dynamic_pointer_cast<OpPlace>(tensor->get_producing_operation());
-                    if (paddle_op && !visited.count(paddle_op.get())) {
-                        visited.insert(paddle_op.get());
-                        q.push(paddle_op.get());
-                        new_op_places.push_back(paddle_op);
-                    }
-                }
-            }
-        }
-    }
-    std::reverse(new_op_places.begin(), new_op_places.end());
-    return new_op_places;
-}
-
-// load_consts with folder is compatible with old PaddlePaddle API.
-template <typename T>
-void InputModel::InputModelImpl::load_consts(const std::basic_string<T>& folder_with_weights) {
-    for (const auto& item : m_var_places) {
-        const auto& var_desc = item.second->get_desc();
-        const auto& name = item.first;
-        if (ov::util::ends_with(name, std::string{"feed"}) || ov::util::ends_with(name, std::string{"fetch"}))
-            continue;
-        if (!var_desc.persistable())
-            continue;
-
-        FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
-        const auto& tensor = var_desc.type().lod_tensor().tensor();
-        Shape shape(tensor.dims().cbegin(), tensor.dims().cend());
-        const auto& type = get_ov_type(tensor.data_type());
-        const auto& data_length = shape_size(shape) * type.size();
-        std::vector<uint8_t> tensor_data(data_length);
-
-        bool read_succeed = false;
-        if (!folder_with_weights.empty()) {
-#if defined(__MINGW32__) || defined(__MINGW64__)
-            std::ifstream is(std::filesystem::path(get_const_path(folder_with_weights, name)),
-                             std::ios::in | std::ifstream::binary);
-#else
-            std::ifstream is(get_const_path(folder_with_weights, name), std::ios::in | std::ifstream::binary);
-#endif
-            FRONT_END_GENERAL_CHECK(is && is.is_open(), "Cannot open file for constant value.");
-            const size_t header_size = 16;
-            std::vector<char> header(header_size);
-            is.read(&header[0], header_size);
-
-            uint32_t dims_len = 0;
-            is.read(reinterpret_cast<char*>(&dims_len), 4);
-            std::vector<char> dims_struct(dims_len);
-            is.read(&dims_struct[0], dims_len);
-            read_succeed = read_tensor(is, reinterpret_cast<char*>(&tensor_data[0]), data_length);
-        } else {
-            FRONT_END_GENERAL_CHECK(false, "Folder with weights must be provided.");
-        }
-        FRONT_END_GENERAL_CHECK(read_succeed,
-                                "File containing constant with name ",
-                                name,
-                                " wasn't successfully read.");
-        auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
-        const_node->set_friendly_name(name);
-        m_tensor_values[name] = const_node;
-    }
-}
-
-// load_consts with stream is compatible with new PaddlePaddle API.
-void InputModel::InputModelImpl::load_consts(std::istream* weight_stream) {
-    for (const auto& item : m_var_places) {
-        const auto& var_desc = item.second->get_desc();
-        const auto& name = item.first;
-        if (ov::util::ends_with(name, std::string{"feed"}) || ov::util::ends_with(name, std::string{"fetch"}))
-            continue;
-
-        // var_desc.persistable() is used to mark node const value or not.
-        if (!var_desc.persistable())
-            continue;
-
-        FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
-        FRONT_END_GENERAL_CHECK(weight_stream != nullptr && weight_stream->peek() != EOF,
-                                "PaddlePaddle *.pdiparams format weight file doesn't exist!");
-        /*
-            reference:
-            https://github.com/PaddlePaddle/Paddle2ONNX/blob/c14446437041a0aa3572994d085b7a35c5b0985c/paddle2onnx/parser/parser.cc#L261
-            When deserialize the proto, the header of each weight
-            [ 4 byte ]      -- version(not need)
-            [   8 byte   ]  -- lod_level(not need)
-            [ 4 byte ]      -- version(not need)
-            [ 4 byte ]      -- TensorDesc size
-            [ x byte ... ]  -- TensorDesc
-            [ y byte ... ]  -- weight
-        */
-        {
-            const size_t header_size = 16;
-            std::vector<char> header(header_size);
-            weight_stream->read(&header[0], header_size);
-        }
-
-        int32_t size;
-        weight_stream->read(reinterpret_cast<char*>(&size), sizeof(size));
-
-        std::unique_ptr<char[]> buf(new char[size]);
-        weight_stream->read(reinterpret_cast<char*>(buf.get()), size);
-
-        std::unique_ptr<::paddle::framework::proto::VarType_TensorDesc> tensor_desc(
-            new ::paddle::framework::proto::VarType_TensorDesc());
-        tensor_desc->ParseFromArray(buf.get(), size);
-        Shape shape(tensor_desc->dims().cbegin(), tensor_desc->dims().cend());
-        const auto& type = get_ov_type(tensor_desc->data_type());
-        const auto& data_length = shape_size(shape) * type.size();
-        std::vector<uint8_t> tensor_data(data_length);
-
-        bool read_succeed = read_tensor(*weight_stream, reinterpret_cast<char*>(&tensor_data[0]), data_length);
-        FRONT_END_GENERAL_CHECK(read_succeed,
-                                "File containing constant with name ",
-                                name,
-                                " wasn't successfully read.");
-
-        auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
-        const_node->set_friendly_name(name);
-        m_tensor_values[name] = const_node;
-    }
-}
-
-/*
-    1. path: is a directory, compatible with old PaddlePaddle API.
-             read __model__ as model stream.
-             read the separate weights in the directory.
-    2. path: is a pdmodel file, compatible with new PaddlePaddle API.
-             read *.pdmodel as model stream.
-             read *.pdiparam as weight stream.
-*/
-template <typename T>
-InputModel::InputModelImpl::InputModelImpl(const std::basic_string<T>& path,
-                                           const InputModel& input_model,
-                                           const std::shared_ptr<TelemetryExtension>& telemetry)
-    : m_fw_ptr{std::make_shared<ProgramDesc>()},
-      m_input_model(input_model),
-      m_telemetry(telemetry) {
-    std::ifstream weights_stream;
-    std::ifstream pb_stream(get_model_path<T>(path, &weights_stream).c_str(), std::ios::in | std::ifstream::binary);
-
-    FRONT_END_GENERAL_CHECK(pb_stream && pb_stream.is_open(),
-                            "Could not open the file: \"",
-                            util::path_to_string(path),
-                            '"');
-    FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(&pb_stream), "Model can't be parsed");
-    // According to Paddle, the saved model has the framework version
-    // For example Paddle 2.1.0 is encoded as 2001000. 0 means the latest framework.
-    // https://github.com/paddle/Paddle/blob/develop/cmake/version.cmake
-    // https://github.com/paddle/Paddle/blob/2100816c5190693cc7dee181e96af72e9f0fbd1d/paddle/fluid/framework/program_desc.cc#L52
-    int64_t version = m_fw_ptr->version().version();
-    FRONT_END_GENERAL_CHECK(
-        version >= 2000000 || version == 0,
-        "[Frontend]Only Support Paddle greater than 2.0.0, current version " + std::to_string(version));
-    load_places();
-    if (is_pdmodel(path)) {
-        load_consts(&weights_stream);
-    } else {
-        load_consts(path);
-    }
-    create_temp_consts();
-}
-
-void InputModel::InputModelImpl::create_temp_consts() {
-    for (const auto& item : m_var_places) {
-        const auto& var_place = item.second;
-        const auto& var_desc = var_place->get_desc();
-        const auto& name = item.first;
-        if (var_desc.persistable())
-            continue;
-
-        // The node with tensorarray as its input may be created before the node with this tensorarray
-        // as its output. e.g. the tensorarray is both the input and output of the same node.
-        // So we have to create a fake empty node here.
-        // Problem is, we have no idea which axis should be 0.
-        // Since the models (faster/mask rcnn) are either concating tensors in tensorarray along the dynamic
-        // dimension, or concating static shape tensors. So we make the dynamic dimension to be 0. In case of static
-        // shape, we simply the the first dimension be 0.
-        if (var_desc.type().has_tensor_array()) {
-            const auto& tensor = var_desc.type().tensor_array().tensor();
-            const auto& type = get_ov_type(tensor.data_type());
-
-            util::log_message("WARNING: The PaddlePaddle model has \"TENSOR_ARRAY\" variables, which is supported "
-                              "under limited situations.");
-
-            PartialShape tensor_ps(std::vector<Dimension>(tensor.dims().cbegin(), tensor.dims().cend()));
-            tensor_ps.insert(tensor_ps.begin(), 1);  // unsqueeze
-            // also update the place for following initialize the graph connection
-            var_place->set_element_type(type);
-            var_place->set_partial_shape(tensor_ps);
-
-            Shape shape(tensor_ps.size(), 0);
-            for (size_t i = 0; i < tensor_ps.size(); i++) {
-                const auto& dim = tensor_ps[i];
-                if (dim.is_static()) {
-                    shape[i] = dim.get_length();
-                }
-            }
-
-            if (tensor_ps.is_static()) {
-                // this tensorarray tensor originally could be scalar, then
-                // tensor_ps size would be 1 after unsqueeze.
-                auto idx = tensor_ps.size() > 1 ? 1 : 0;
-                shape[idx] = 0;
-            }
-
-            auto node = opset7::Constant::create(type, shape, {0});
-            node->set_friendly_name(name);
-            node->output(0).get_tensor().add_names({name});
-
-            m_tensor_values[name] = node;
-        }
-    }
-}
-
-InputModel::InputModelImpl::InputModelImpl(const std::vector<std::istream*>& streams,
-                                           const InputModel& input_model,
-                                           const std::shared_ptr<TelemetryExtension>& telemetry)
-    : m_fw_ptr{std::make_shared<ProgramDesc>()},
-      m_input_model(input_model),
-      m_telemetry(telemetry) {
-    if (streams.size() != 1) {
-        FRONT_END_GENERAL_CHECK(streams.size() == 2,
-                                "Two streams are needed to load a model: model and weights streams");
-    }
-    FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(streams[0]), "Model can't be parsed");
-    int64_t version = m_fw_ptr->version().version();
-    FRONT_END_GENERAL_CHECK(
-        version >= 2000000 || version == 0,
-        "[Frontend]Only Support Paddle greater than 2.0.0, current version " + std::to_string(version));
-    load_places();
-    if (streams.size() > 1)
-        load_consts(streams[1]);
-    create_temp_consts();
-}
-
-std::vector<Place::Ptr> InputModel::InputModelImpl::get_inputs() const {
-    return m_inputs;
-}
-
-std::vector<Place::Ptr> InputModel::InputModelImpl::get_outputs() const {
-    return m_outputs;
-}
-
-Place::Ptr InputModel::InputModelImpl::get_place_by_tensor_name(const std::string& tensorName) const {
-    if (m_var_places.count(tensorName))
-        return m_var_places.at(tensorName);
-    return nullptr;
-}
-
-namespace {
-std::shared_ptr<TensorPlace> castToTensorPlace(const Place::Ptr& place) {
-    if (auto var_place = std::dynamic_pointer_cast<TensorPlace>(place)) {
-        return var_place;
-    } else if (auto in_port_place = std::dynamic_pointer_cast<InPortPlace>(place)) {
-        return in_port_place->get_source_tensor_paddle();
-    } else if (auto out_port_place = std::dynamic_pointer_cast<OutPortPlace>(place)) {
-        return out_port_place->get_target_tensor_paddle();
-    }
-    FRONT_END_GENERAL_CHECK(false, "Cannot cast this Place to TensorPlacepaddle.");
-}
-
-}  // namespace
-
-void InputModel::InputModelImpl::override_all_inputs(const std::vector<Place::Ptr>& inputs) {
-    m_graph_changed = true;
-    m_inputs.clear();
-    for (const auto& inp : inputs) {
-        m_inputs.push_back(castToTensorPlace(inp));
-    }
-}
-
-void InputModel::InputModelImpl::override_all_outputs(const std::vector<Place::Ptr>& outputs) {
-    m_graph_changed = true;
-    m_outputs.clear();
-    for (const auto& outp : outputs) {
-        m_outputs.push_back(castToTensorPlace(outp));
-    }
-}
-
-void InputModel::InputModelImpl::extract_subgraph(const std::vector<Place::Ptr>& inputs,
-                                                  const std::vector<Place::Ptr>& outputs) {
-    m_graph_changed = true;
-    override_all_inputs(inputs);
-    override_all_outputs(outputs);
-}
-
-void InputModel::InputModelImpl::set_default_shape(Place::Ptr place, const ov::Shape& shape) {
-    FRONT_END_NOT_IMPLEMENTED("set_default_shape");
-}
-
-void InputModel::InputModelImpl::set_partial_shape(Place::Ptr place, const ov::PartialShape& p_shape) {
-    castToTensorPlace(place)->set_partial_shape(p_shape);
-}
-
-ov::PartialShape InputModel::InputModelImpl::get_partial_shape(Place::Ptr place) const {
-    return castToTensorPlace(place)->get_partial_shape();
-}
-
-void InputModel::InputModelImpl::set_element_type(Place::Ptr place, const ov::element::Type& type) {
-    castToTensorPlace(place)->set_element_type(type);
-}
-
-ov::element::Type InputModel::InputModelImpl::get_element_type(const Place::Ptr& place) const {
-    return castToTensorPlace(place)->get_element_type();
-}
-
-void InputModel::InputModelImpl::set_tensor_value(Place::Ptr place, const void* value) {
-    m_graph_changed = true;
-    auto tensor_place = castToTensorPlace(place);
-    auto p_shape = tensor_place->get_partial_shape();
-    auto type = tensor_place->get_element_type();
-    auto constant = opset7::Constant::create(type, p_shape.to_shape(), value);
-    auto name = tensor_place->get_names()[0];
-    constant->set_friendly_name(name);
-    m_tensor_values[name] = constant;
-}
-
-InputModel::InputModel(const std::string& path, const std::shared_ptr<TelemetryExtension>& telemetry)
-    : _impl{std::make_shared<InputModelImpl>(path, *this, telemetry)} {}
-
-#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
-InputModel::InputModel(const std::wstring& path, const std::shared_ptr<TelemetryExtension>& telemetry)
-    : _impl{std::make_shared<InputModelImpl>(path, *this, telemetry)} {}
-#endif
-
-InputModel::InputModel(const std::vector<std::istream*>& streams, const std::shared_ptr<TelemetryExtension>& telemetry)
-    : _impl{std::make_shared<InputModelImpl>(streams, *this, telemetry)} {}
-
-std::vector<std::shared_ptr<OpPlace>> InputModel::get_op_places(const int32_t blck_idx) const {
+std::vector<std::shared_ptr<BaseOpPlace>> InputModel::get_op_places(const int32_t blck_idx) const {
     return _impl->get_op_places(blck_idx);
 }
 
-std::map<std::string, std::shared_ptr<TensorPlace>> InputModel::get_var_places() const {
+std::map<std::string, std::shared_ptr<BaseTensorPlace>> InputModel::get_var_places() const {
     return _impl->get_var_places();
 }
 
@@ -644,6 +119,7 @@ void InputModel::set_element_type(const Place::Ptr& place, const ov::element::Ty
     _impl->set_element_type(place, type);
 }
 
+
 ov::element::Type InputModel::get_element_type(const Place::Ptr& place) const {
     return castToTensorPlace(place)->get_element_type();
 }
@@ -651,7 +127,21 @@ ov::element::Type InputModel::get_element_type(const Place::Ptr& place) const {
 void InputModel::set_tensor_value(const Place::Ptr& place, const void* value) {
     _impl->set_tensor_value(place, value);
 }
+std::shared_ptr<BaseTensorPlace> castToTensorPlace(const Place::Ptr& place) {
+    if (auto var_place = std::dynamic_pointer_cast<BaseTensorPlace>(place)) {
+        return var_place;
+    } else if (auto in_port_place = std::dynamic_pointer_cast<InPortPlace>(place)) {
+        return in_port_place->get_source_tensor_paddle();
+    } else if (auto out_port_place = std::dynamic_pointer_cast<OutPortPlace>(place)) {
+        return out_port_place->get_target_tensor_paddle();
+    }
+    FRONT_END_GENERAL_CHECK(false, "Cannot cast this Place to TensorPlacepaddle.");
+}
 
+bool read_tensor(std::istream& is, char* data, size_t len) {
+    is.read(data, len);
+    return (size_t)is.gcount() == len;
+}
 }  // namespace paddle
 }  // namespace frontend
 }  // namespace ov
diff --git a/src/frontends/paddle/src/input_model.hpp b/src/frontends/paddle/src/input_model.hpp
index 605e7b4abd..5c39adbff1 100644
--- a/src/frontends/paddle/src/input_model.hpp
+++ b/src/frontends/paddle/src/input_model.hpp
@@ -6,13 +6,148 @@
 
 #include "openvino/frontend/extension/telemetry.hpp"
 #include "openvino/frontend/paddle/frontend.hpp"
+#include "paddle_utils.hpp"
+#include "openvino/util/common_util.hpp"
+#include "openvino/util/file_util.hpp"
 
 namespace ov {
 namespace frontend {
 namespace paddle {
 
-class OpPlace;
-class TensorPlace;
+class BaseOpPlace;
+class BaseTensorPlace;
+class BaseInputModelImpl {
+public:
+    virtual std::vector<Place::Ptr> get_inputs() const = 0;
+    virtual std::vector<Place::Ptr> get_outputs() const = 0;
+    virtual int64_t get_version() const = 0;
+    virtual Place::Ptr get_place_by_tensor_name(const std::string& tensorName) const = 0;
+    virtual void override_all_outputs(const std::vector<Place::Ptr>& outputs) = 0;
+    virtual void override_all_inputs(const std::vector<Place::Ptr>& inputs) = 0;
+    virtual void extract_subgraph(const std::vector<Place::Ptr>& inputs, const std::vector<Place::Ptr>& outputs) = 0;
+    virtual void set_default_shape(Place::Ptr place, const ov::Shape&) = 0;
+    virtual void set_partial_shape(Place::Ptr place, const ov::PartialShape&) = 0;
+    virtual ov::PartialShape get_partial_shape(Place::Ptr place) const = 0;
+    virtual void set_element_type(Place::Ptr place, const ov::element::Type&) = 0;
+    virtual ov::element::Type get_element_type(const Place::Ptr& place) const = 0;
+    virtual void set_tensor_value(Place::Ptr place, const void* value) = 0;
+    virtual std::vector<std::shared_ptr<BaseOpPlace>> get_op_places(const int32_t blck_idx) const = 0;
+    virtual std::map<std::string, std::shared_ptr<BaseTensorPlace>> get_var_places() const = 0;
+    virtual std::map<paddle::TensorName, Output<Node>> get_tensor_values() const = 0;
+};
+
+bool read_tensor(std::istream& is, char* data, size_t len);
+template <typename T>
+std::basic_string<T> get_const_path(const std::basic_string<T>& folder_with_weights, const std::string& name) {
+    return folder_with_weights + paddle::get_path_sep<T>() + name;
+}
+
+#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
+template <>
+std::basic_string<wchar_t> get_const_path(const std::basic_string<wchar_t>& folder, const std::string& name) {
+    return folder + paddle::get_path_sep<wchar_t>() + ov::util::string_to_wstring(name);
+}
+#endif
+
+template <typename T>
+bool is_pdmodel(const std::basic_string<T>& path) {
+    std::string ext = ".pdmodel";
+    return ov::util::ends_with(path, ext);
+}
+
+#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
+template <>
+bool is_pdmodel(const std::basic_string<wchar_t>& path) {
+    std::wstring ext = L".pdmodel";
+    return ov::util::ends_with(path, ext);
+}
+#endif
+
+template <typename T>
+bool is_json_model(const std::basic_string<T>& path) {
+    std::string ext = ".json";
+    return ov::util::ends_with(path, ext);
+}
+
+#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
+template <>
+bool is_json_model(const std::basic_string<wchar_t>& path) {
+    std::wstring ext = L".json";
+    return ov::util::ends_with(path, ext);
+}
+#endif
+
+template <typename T>
+std::basic_string<T> get_json_model_path(const std::basic_string<T>& path, std::ifstream* weights_stream) {
+    std::string model_file{path};
+    std::string ext = ".json";
+    if (ov::util::ends_with(model_file, ext)) {
+        std::string params_ext = ".pdiparams";
+        std::string weights_file{path};
+        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
+        weights_stream->open(weights_file, std::ios::binary);
+        // Don't throw error if file isn't opened
+        // It may mean that model don't have constants
+    } else {
+        model_file += paddle::get_path_sep<T>() + "__model__";
+    }
+    return model_file;
+}
+
+#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
+template <>
+std::basic_string<wchar_t> get_json_model_path(const std::basic_string<wchar_t>& path, std::ifstream* weights_stream) {
+    std::wstring model_file{path};
+    std::wstring ext = L".json";
+    if (ov::util::ends_with(model_file, ext)) {
+        std::wstring params_ext = L".pdiparams";
+        std::wstring weights_file{path};
+        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
+        weights_stream->open(weights_file.c_str(), std::ios::binary);
+        // Don't throw error if file isn't opened
+        // It may mean that model don't have constants
+    } else {
+        model_file += paddle::get_path_sep<wchar_t>() + L"__model__";
+    }
+    return model_file;
+}
+#endif
+
+template <typename T>
+std::basic_string<T> get_model_path(const std::basic_string<T>& path, std::ifstream* weights_stream) {
+    std::string model_file{path};
+    std::string ext = ".pdmodel";
+    if (ov::util::ends_with(model_file, ext)) {
+        std::string params_ext = ".pdiparams";
+        std::string weights_file{path};
+        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
+        weights_stream->open(weights_file, std::ios::binary);
+        // Don't throw error if file isn't opened
+        // It may mean that model don't have constants
+    } else {
+        model_file += paddle::get_path_sep<T>() + "__model__";
+    }
+    return model_file;
+}
+
+#if defined(OPENVINO_ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
+template <>
+std::basic_string<wchar_t> get_model_path(const std::basic_string<wchar_t>& path, std::ifstream* weights_stream) {
+    std::wstring model_file{path};
+    std::wstring ext = L".pdmodel";
+    if (ov::util::ends_with(model_file, ext)) {
+        std::wstring params_ext = L".pdiparams";
+        std::wstring weights_file{path};
+        weights_file.replace(weights_file.size() - ext.size(), ext.size(), params_ext);
+        weights_stream->open(weights_file.c_str(), std::ios::binary);
+        // Don't throw error if file isn't opened
+        // It may mean that model don't have constants
+    } else {
+        model_file += paddle::get_path_sep<wchar_t>() + L"__model__";
+    }
+    return model_file;
+}
+#endif
 
 class InputModel : public ov::frontend::InputModel {
 public:
@@ -38,14 +173,13 @@ public:
 
 private:
     friend class ov::frontend::paddle::FrontEnd;
-    class InputModelImpl;
-    std::shared_ptr<InputModelImpl> _impl;
+    std::shared_ptr<BaseInputModelImpl> _impl;
 
-    std::vector<std::shared_ptr<OpPlace>> get_op_places(const int32_t block_idx) const;
-    std::map<std::string, std::shared_ptr<TensorPlace>> get_var_places() const;
+    std::vector<std::shared_ptr<BaseOpPlace>> get_op_places(const int32_t block_idx) const;
+    std::map<std::string, std::shared_ptr<BaseTensorPlace>> get_var_places() const;
     std::map<std::string, Output<Node>> get_tensor_values() const;
 };
-
+std::shared_ptr<BaseTensorPlace> castToTensorPlace(const Place::Ptr& place);
 }  // namespace paddle
 }  // namespace frontend
 }  // namespace ov
diff --git a/src/frontends/paddle/src/internal/op/if_else_block.cpp b/src/frontends/paddle/src/internal/op/if_else_block.cpp
new file mode 100644
index 0000000000..2a935a150c
--- /dev/null
+++ b/src/frontends/paddle/src/internal/op/if_else_block.cpp
@@ -0,0 +1,65 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "internal/op/if_else_block.hpp"
+
+#include <algorithm>
+#include <cassert>
+
+#include "openvino/op/constant.hpp"
+#include "openvino/op/util/precision_sensitive_attribute.hpp"
+
+using namespace std;
+using namespace ov;
+
+op::internal::IfElseBlock::IfElseBlock(
+    const Output<Node>& cond,
+    const OutputVector& if_inputs,
+    const OutputVector& else_inputs,
+    const std::vector<int32_t>& sub_block_indexs,
+    const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos)
+    : Op({cond}),
+      m_sub_block_indexs(sub_block_indexs),
+      m_output_infos(output_infos) {
+      m_inputs_from_parent.push_back(if_inputs);
+      m_inputs_from_parent.push_back(else_inputs);
+    constructor_validate_and_infer_types();
+}
+
+// op::internal::IfElseBlock::IfElseBlock(
+//     const OutputVector& inputs,
+//     const Output<Node>& cond,
+//     bool is_scalar_condition,
+//     int32_t sub_block_index,
+//     const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos)
+//     : m_is_scalar_condition(is_scalar_condition),
+//       m_sub_block_index(sub_block_index),
+//       m_output_infos(output_infos) {
+//     OutputVector new_args;
+//     std::move(inputs.begin(), inputs.end(), std::back_inserter(new_args));
+//     new_args.emplace_back(cond);
+//     set_arguments(new_args);
+//     constructor_validate_and_infer_types();
+// }
+
+std::shared_ptr<Node> op::internal::IfElseBlock::clone_with_new_inputs(const OutputVector& new_args) const {
+    check_new_args_count(this, new_args);
+    return make_shared<IfElseBlock>(new_args.at(0), m_inputs_from_parent[0], m_inputs_from_parent[1], m_sub_block_indexs, m_output_infos);
+}
+
+bool op::internal::IfElseBlock::visit_attributes(AttributeVisitor& visitor) {
+    visitor.on_attribute("sub_block_indexs", m_sub_block_indexs);
+    return true;
+}
+
+void op::internal::IfElseBlock::validate_and_infer_types() {
+    for (size_t i = 0; i < m_output_infos.size(); i++) {
+        set_output_type(i, m_output_infos[i].first, m_output_infos[i].second);
+    }
+}
+
+const OutputVector& op::internal::IfElseBlock::get_inputs_from_parent(size_t index) const {
+    assert(index < m_inputs_from_parent.size());
+    return m_inputs_from_parent[index];
+}
diff --git a/src/frontends/paddle/src/internal/op/if_else_block.hpp b/src/frontends/paddle/src/internal/op/if_else_block.hpp
new file mode 100644
index 0000000000..664adba5c5
--- /dev/null
+++ b/src/frontends/paddle/src/internal/op/if_else_block.hpp
@@ -0,0 +1,49 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+
+#include "openvino/op/op.hpp"
+
+namespace ov {
+namespace op {
+namespace internal {
+class IfElseBlock : public Op {
+public:
+    OPENVINO_OP("IfElseBlock", "internal");
+
+    IfElseBlock() = default;
+
+    // IfElseBlock(const OutputVector& inputs,
+    //                  const Output<Node>& cond,
+    //                  std::vector<int32_t>& sub_block_indexs,
+    //                  const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos);
+    IfElseBlock(const Output<Node>& cond,
+                const OutputVector& if_inputs,
+                const OutputVector& else_inputs,
+                const std::vector<int32_t>& sub_block_indexs,
+                const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos);
+
+    void validate_and_infer_types() override;
+
+    bool visit_attributes(AttributeVisitor& visitor) override;
+
+    std::shared_ptr<Node> clone_with_new_inputs(const OutputVector& new_args) const override;
+
+    /// \return A vector containing the values for each input except "cond".
+    const OutputVector& get_inputs_from_parent(size_t index) const;
+
+    const std::vector<int32_t>& get_subblock_indexs() const {
+        return m_sub_block_indexs;
+    }
+
+private:
+    std::vector<int32_t> m_sub_block_indexs;
+    std::vector<OutputVector> m_inputs_from_parent;
+    std::vector<std::pair<ov::element::Type, ov::PartialShape>> m_output_infos;
+};
+
+}  // namespace internal
+}  // namespace op
+}  // namespace ov
diff --git a/src/frontends/paddle/src/internal/op/while.cpp b/src/frontends/paddle/src/internal/op/while.cpp
index ba978a35a5..2b252d4fef 100644
--- a/src/frontends/paddle/src/internal/op/while.cpp
+++ b/src/frontends/paddle/src/internal/op/while.cpp
@@ -11,15 +11,17 @@ using namespace ov;
 
 op::internal::While::While(const OutputVector& inputs,
                            int32_t sub_block,
-                           const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos)
+                           const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos,
+                           bool is_json_format)
     : Op(inputs),
       m_sub_block(sub_block),
-      m_output_infos(output_infos) {
+      m_output_infos(output_infos),
+      m_is_json_format(is_json_format) {
     constructor_validate_and_infer_types();
 }
 
 std::shared_ptr<Node> op::internal::While::clone_with_new_inputs(const OutputVector& new_args) const {
-    return make_shared<While>(new_args, m_sub_block, m_output_infos);
+    return make_shared<While>(new_args, m_sub_block, m_output_infos, m_is_json_format);
 }
 
 bool op::internal::While::visit_attributes(AttributeVisitor& visitor) {
diff --git a/src/frontends/paddle/src/internal/op/while.hpp b/src/frontends/paddle/src/internal/op/while.hpp
index e792e3c14c..2e938a188e 100644
--- a/src/frontends/paddle/src/internal/op/while.hpp
+++ b/src/frontends/paddle/src/internal/op/while.hpp
@@ -17,7 +17,8 @@ public:
 
     While(const OutputVector& inputs,
           int32_t sub_block,
-          const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos);
+          const std::vector<std::pair<ov::element::Type, ov::PartialShape>>& output_infos,
+          bool is_json_format);
 
     void validate_and_infer_types() override;
 
@@ -28,11 +29,15 @@ public:
     const int32_t get_subblock_index() const {
         return m_sub_block;
     }
+    bool is_json_format() {
+        return m_is_json_format;
+    }
 
 private:
     int32_t m_sub_block = 0;
 
     std::vector<std::pair<ov::element::Type, ov::PartialShape>> m_output_infos;
+    bool m_is_json_format = false;
 };
 
 }  // namespace internal
diff --git a/src/frontends/paddle/src/internal/pass/transform_if_else.cpp b/src/frontends/paddle/src/internal/pass/transform_if_else.cpp
new file mode 100644
index 0000000000..ba70a21c52
--- /dev/null
+++ b/src/frontends/paddle/src/internal/pass/transform_if_else.cpp
@@ -0,0 +1,78 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "internal/pass/transform_if_else.hpp"
+
+#include "default_opset.hpp"
+#include "internal/op/if_else_block.hpp"
+#include "internal/op/tensorarray_write.hpp"
+#include "openvino/core/graph_util.hpp"
+#include "openvino/pass/pattern/matcher.hpp"
+#include "openvino/pass/pattern/op/wrap_type.hpp"
+#include "transformations/common_optimizations/fold_subgraph_empty_inputs.hpp"
+
+using namespace std;
+using namespace ov;
+using namespace ov::pass;
+using namespace ov::frontend::paddle::op::default_opset;
+
+// Transform Paddle "if" to OpenVINO If op.
+ov::frontend::paddle::pass::TransformIfElse::TransformIfElse(std::vector<std::shared_ptr<Model>> funcs) {
+    const auto cond_label = pattern::wrap_type<ov::op::internal::IfElseBlock>();
+
+    matcher_pass_callback callback = [funcs](pattern::Matcher& m) -> bool {
+        const auto if_else_block = ov::as_type_ptr<ov::op::internal::IfElseBlock>(m.get_match_root());
+        if (!if_else_block) {
+            return false;
+        }
+        const auto mask_idx = if_else_block->get_input_size() - 1;
+        const auto cond = if_else_block->get_input_node_shared_ptr(mask_idx);
+
+        if (!cond) {
+            return false;
+        }
+
+        // build_if_node
+        const auto if_else_block_ids = if_else_block->get_subblock_indexs();
+        OPENVINO_ASSERT(if_else_block_ids.size() == 2, "there should be two branch here");
+        const auto then_idx = if_else_block_ids[0];
+        const auto& then_branch = funcs[then_idx];
+        const auto& then_params = then_branch->get_parameters();
+        const auto else_idx = if_else_block_ids[1];
+        const auto& else_branch = funcs[else_idx];
+        const auto& else_params = else_branch->get_parameters();
+
+        auto if_node = std::make_shared<If>(cond);
+        ov::pass::disable_fold_subgraph_empty_inputs(if_node);
+        if_node->set_then_body(then_branch);
+        if_node->set_else_body(else_branch);
+
+        // get inputs
+        const auto then_branch_inputs_from_parent = if_else_block->get_inputs_from_parent(0);
+        auto then_param = then_params.cbegin();
+        for (const auto& from_parent : then_branch_inputs_from_parent) {
+            if_node->set_input(from_parent, *then_param, nullptr);
+            then_param++;
+        }
+        const auto else_branch_inputs_from_parent = if_else_block->get_inputs_from_parent(1);
+        auto else_param = else_params.cbegin();
+        for (const auto& from_parent : else_branch_inputs_from_parent) {
+            if_node->set_input(from_parent,  nullptr, *else_param);
+            else_param++;
+        }
+
+        auto then_results = then_branch->get_results();
+        auto else_results = else_branch->get_results();
+        for (size_t i = 0; i < else_results.size(); i++) {
+            if_node->set_output(then_results[i], else_results[i]);
+        }
+        replace_node(if_else_block, if_node);
+        if_node->set_friendly_name(if_else_block->get_friendly_name());
+
+        return true;
+    };
+
+    auto m = std::make_shared<pattern::Matcher>(cond_label, "if_else_block");
+    this->register_matcher(m, callback);
+}
diff --git a/src/frontends/paddle/src/internal/pass/transform_if_else.hpp b/src/frontends/paddle/src/internal/pass/transform_if_else.hpp
new file mode 100644
index 0000000000..ee269d2580
--- /dev/null
+++ b/src/frontends/paddle/src/internal/pass/transform_if_else.hpp
@@ -0,0 +1,27 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+
+#include "openvino/pass/graph_rewrite.hpp"
+#include "openvino/pass/pass.hpp"
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace pass {
+
+class TransformIfElse : public ov::pass::MatcherPass {
+public:
+    OPENVINO_MATCHER_PASS_RTTI("ov::frontend::paddle::pass::TransformIfElse");
+    TransformIfElse(std::vector<std::shared_ptr<Model>> functions);
+
+private:
+    std::vector<std::shared_ptr<Model>> m_functions;
+};
+
+}  // namespace pass
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/internal/pass/transform_while.cpp b/src/frontends/paddle/src/internal/pass/transform_while.cpp
index 702d9fd5c8..d0bc76e014 100644
--- a/src/frontends/paddle/src/internal/pass/transform_while.cpp
+++ b/src/frontends/paddle/src/internal/pass/transform_while.cpp
@@ -41,45 +41,64 @@ ov::frontend::paddle::pass::TransformWhile::TransformWhile(std::vector<std::shar
         const auto block_idx = while_node->get_subblock_index();
         const auto sub_model = functions[block_idx];
         loop->set_function(sub_model);
-
         const auto& parameters = sub_model->get_parameters();
         const auto submodel_outputs = sub_model->outputs();
-        const auto is_exist = [&submodel_outputs](const std::string& name) {
-            for (const auto& out : submodel_outputs) {
-                if (out.get_any_name() == name)
-                    return true;
+        if (while_node->is_json_format()) {
+            size_t param_size = parameters.size();
+            size_t ouput_index = 0;
+            for (size_t i = 0; i < parameters.size(); i++) {
+                const auto names = inputs[i].get_names();
+                if ((!names.empty()) && std::stoi(*names.begin()) < 0) {
+                    loop->set_merged_input(parameters[param_size - 1 - i], inputs[i], submodel_outputs[ouput_index++]);
+                } else {
+                    loop->set_invariant_input(parameters[param_size - 1 - i], inputs[i]);
+                }
             }
-            return false;
-        };
-        for (size_t i = 0; i < parameters.size(); i++) {
-            const auto names = inputs[i].get_names();
-            std::string param_name;
-            if (!names.empty()) {
-                param_name = *names.begin();
+            loop->set_special_body_ports(Loop::SpecialBodyPorts{-1, submodel_outputs.size() - 1});
+            // replace output
+            const auto& results = sub_model->get_results();
+            for (size_t i = 0; i < results.size(); i++) {
+                auto out = loop->get_iter_value(results[i], -1);
+                while_node->output(i).replace(out);
             }
-            if (!param_name.empty() && is_exist(param_name)) {
-                auto out_node = sub_model->output(param_name);
-                loop->set_merged_input(parameters[i], inputs[i], out_node);
-            } else {
-                loop->set_invariant_input(parameters[i], inputs[i]);
+        } else {
+            const auto is_exist = [&submodel_outputs](const std::string& name) {
+                for (const auto& out : submodel_outputs) {
+                    if (out.get_any_name() == name)
+                        return true;
+                }
+                return false;
+            };
+            for (size_t i = 0; i < parameters.size(); i++) {
+                const auto names = inputs[i].get_names();
+                std::string param_name;
+                if (!names.empty()) {
+                    param_name = *names.begin();
+                }
+                if (!param_name.empty() && is_exist(param_name)) {
+                    auto out_node = sub_model->output(param_name);
+                    loop->set_merged_input(parameters[i], inputs[i], out_node);
+                } else {
+                    loop->set_invariant_input(parameters[i], inputs[i]);
+                }
             }
-        }
-        int64_t idx = -1;
-        for (size_t i = 0; i < sub_model->get_results().size(); i++) {
-            if (sub_model->output(i).get_tensor().get_any_name() == cond_name)
-                idx = static_cast<int64_t>(i);
-        }
-        FRONT_END_GENERAL_CHECK(idx != -1, "could not find condition node in outputs.");
-
-        loop->set_special_body_ports(Loop::SpecialBodyPorts{-1, idx});
+            int64_t idx = -1;
+            for (size_t i = 0; i < sub_model->get_results().size(); i++) {
+                if (sub_model->output(i).get_tensor().get_any_name() == cond_name)
+                    idx = static_cast<int64_t>(i);
+            }
+            FRONT_END_GENERAL_CHECK(idx != -1, "could not find condition node in outputs.");
 
-        // replace output
-        const auto& results = sub_model->get_results();
-        for (size_t i = 0; i < results.size(); i++) {
-            auto out = loop->get_iter_value(results[i], -1);
-            while_node->output(i).replace(out);
+            loop->set_special_body_ports(Loop::SpecialBodyPorts{-1, idx});
+            // replace output
+            const auto& results = sub_model->get_results();
+            for (size_t i = 0; i < results.size(); i++) {
+                auto out = loop->get_iter_value(results[i], -1);
+                while_node->output(i).replace(out);
+            }
         }
 
+
         loop->add_node_control_dependents(while_node);
         loop->add_node_control_dependencies(while_node);
         while_node->clear_control_dependents();
diff --git a/src/frontends/paddle/src/json_data.cpp b/src/frontends/paddle/src/json_data.cpp
new file mode 100644
index 0000000000..1e75732b45
--- /dev/null
+++ b/src/frontends/paddle/src/json_data.cpp
@@ -0,0 +1,318 @@
+#include "json_data.hpp"
+#include "openvino/frontend/frontend.hpp"
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace json {
+void decodeRegion(const nlohmann::json& json, std::shared_ptr<Region> region){
+    region->name = json.at("#").template get<std::string>();
+    auto& blocksJson = json.at("blocks");
+    for (auto& blockJson : blocksJson) {
+        Block newBlock;
+        decodeBlock(blockJson, newBlock);
+        region->blocks.push_back(std::move(newBlock));
+    }
+}
+void decodeBlock(const nlohmann::json& json, Block& block) {
+    block.name = json.at("#").template get<std::string>();
+    auto argsJson = json.at("args");
+    for (auto& argJson : argsJson) {
+        Port newPort;
+        decodeArgs(argJson, newPort);
+        block.args.push_back(std::move(newPort));
+    }
+    auto& opsJson = json.at("ops");
+    for (auto& opJson : opsJson) {
+        OP newOp(opJson);
+        decodeOP(opJson, newOp);
+        block.ops.push_back(std::move(newOp));
+    }
+}
+void decodeOP(const nlohmann::json& json, OP& op) {
+    op.type = json.at("#").template get<std::string>();
+    if (op.type == "p") {
+        decodeConst(json, op);
+    } else {
+        size_t pos = op.type.find('.');
+        auto dialet = op.type.substr(0, pos);
+        if (pos != std::string::npos) {
+            op.type = op.type.substr(pos + 1);
+            std::cout << "dialet:" << dialet << " type:" << op.type << std::endl;
+            // the name sum is conflict with paddle 2.0 op sum
+            if (op.type == "sum") {
+                op.type = "reduce_sum";
+            } else if (op.type == "split" && dialet == "1") {
+                op.type = "split_with_num";
+            } else if (op.type == "if" || op.type == "while") {
+                //decode sub graph
+                auto& regionsJson = json.at("regions");
+                for (auto& regionJson : regionsJson) {
+                    auto sub_region = std::make_shared<Region>();
+                    json::decodeRegion(regionJson, sub_region);
+                    op.sub_region_vecs.push_back(sub_region);
+                }
+            }
+        }
+        auto& inputsJson = json.at("I");
+        for (auto& inputJson : inputsJson) {
+            auto inputId = inputJson.at("%").template get<uint64_t>();
+            op.inputIds.push_back(inputId);
+        }
+        // op.attrs = json.at("A").dump(); // decode in decode_json.cpp
+    }
+    // op.outAttrs = json.at("OA").dump();// save it, maybe need in future
+    decodeOutPorts(json, op);
+}
+void decodeConst(const nlohmann::json& json, OP& op) {
+    auto& attrJson = json.at("A");
+    op.is_distributed = (attrJson.at(0).template get<int32_t>() != 0);
+    op.is_parameter = (attrJson.at(1).template get<int32_t>() != 0);
+    op.need_clip = (attrJson.at(2).template get<int32_t>() != 0);
+    op.name = attrJson.at(3).template get<std::string>();
+    op.distAttrs = json.at("DA").dump();// save it, maybe need in future
+    op.quantAttrs = json.at("QA").dump();// save it, maybe need in future
+}
+void decodeOutPorts(const nlohmann::json& json, OP& op) {
+    auto& outPortsJson = json.at("O");
+    if (outPortsJson.is_array()) {
+        for (auto& outPortJson : outPortsJson) {
+            Port newPort;
+            decodePort(outPortJson, newPort);
+            op.outputPorts.push_back(std::move(newPort));
+        }
+    } else {
+        Port newPort;
+        decodePort(outPortsJson, newPort);
+        op.outputPorts.push_back(std::move(newPort));
+    }
+    // if (op.type = "while") {
+    //    // add a fake bool output
+    //    Port fakePort;
+    //    auto last_port = op.outputPorts.back()
+    //    fakePort.id = std::numeric_limits<std::int64_t>::max() - last_port.id
+    //    op.outputPorts.push_back(std::move(fakePort));
+    // }
+}
+
+void decodePortDesc(const nlohmann::json& json, PortDesc& desc) {
+    auto& data = json.at("D");
+    auto precisionString = data.at(0).at("#").template get<std::string>();
+    size_t pos = precisionString.find('.');
+    if (pos != std::string::npos) {
+        precisionString = precisionString.substr(pos + 1);
+    }
+    desc.precision = convertFromStringToType(precisionString);
+    desc.shapes = data.at(1).template get<std::vector<int64_t>>();
+    desc.layout = data.at(2).template get<std::string>();
+    if (data.size() > 3) {
+        //desc.lod = data.at(3).template get<std::vector<std::vector<size_t>>>(); // save it, maybe need in future
+        desc.offset = data.at(4).template get<size_t>();// save it, maybe need in future
+    }
+}
+void decodePort(const nlohmann::json& json, Port& port) {
+    port.id = json.at("%").template get<uint64_t>();
+    auto& typeTypeJson = json.at("TT");
+    auto port_type = typeTypeJson.at("#").template get<std::string>();
+    if (port_type == "NULL") {
+        return;
+    }
+    auto pos = port_type.find(".");
+    port.type = port_type.substr(pos + 1);
+    if (port.type == "t_vec") {
+        auto& data = typeTypeJson.at("D");
+        for (auto& portDescJson : data) {
+            PortDesc newPortDesc;
+            decodePortDesc(portDescJson, newPortDesc);
+            port.descs.push_back(std::move(newPortDesc));
+        }
+    } else {
+        PortDesc newPortDesc;
+        decodePortDesc(typeTypeJson, newPortDesc);
+        port.descs.push_back(std::move(newPortDesc));
+    }
+}
+void decodeArgs(const nlohmann::json& json, Port& port) {
+    port.id = json.at("#").template get<uint64_t>();
+    auto& typeTypeJson = json.at("TT");
+    auto port_type = typeTypeJson.at("#").template get<std::string>();
+    if (port_type == "NULL") {
+        return;
+    }
+    auto pos = port_type.find(".");
+    port.type = port_type.substr(pos + 1);
+    if (port.type == "t_vec") {
+        auto& data = typeTypeJson.at("D");
+        for (auto& portDescJson : data) {
+            PortDesc newPortDesc;
+            decodePortDesc(portDescJson, newPortDesc);
+            port.descs.push_back(std::move(newPortDesc));
+        }
+    } else {
+        PortDesc newPortDesc;
+        decodePortDesc(typeTypeJson, newPortDesc);
+        port.descs.push_back(std::move(newPortDesc));
+    }
+}
+
+TypeType convertFromStringToType(std::string type) {
+  const static std::map<std::string, TypeType> map = {
+      {"t_undefined", UNDEFINED},
+      {"t_bf16", BF16},
+      {"t_f16", F16},
+      {"t_f32", F32},
+      {"t_f64", F64},
+      {"t_i8", I8},
+      {"t_u8", U8},
+      {"t_i16", I16},
+      {"t_i32", I32},
+      {"t_i64", I64},
+      {"t_index", INDEX},
+      {"t_bool", BOOL},
+      {"t_c64", C64},
+      {"t_c128", C128},
+      {"t_f8e4m3fn", F8E4M3FN},
+      {"t_f8e5m2", F8E5M2},
+      {"t_dtensor", DTENSOR},
+      {"t_vec", VEC}
+  };
+  auto iter = map.find(type);
+  if (iter != map.end()) {
+     return  iter->second;
+  } else {
+     return UNDEFINED;
+  }
+}
+ov::Any decode_vector_attrs(const nlohmann::json& attrs) {
+    auto& attr_data = attrs.at("D");
+    for(auto& attr : attr_data) {
+        std::string attr_type = attr.at("#").template get<std::string>();
+        auto pos = attr_type.find(".");
+        attr_type = attr_type.substr(pos + 1);
+        if (attr_type  == "a_i32") {
+            return ov::Any(decode_vector_attrs_value<int32_t>(attrs));
+        } else if (attr_type  == "a_i64") {
+            return ov::Any(decode_vector_attrs_value<int64_t>(attrs));
+        } else if (attr_type  == "a_bool") {
+            return ov::Any(decode_vector_attrs_value<bool>(attrs));
+        } else if (attr_type  == "a_str") {
+            return ov::Any(decode_vector_attrs_value<std::string>(attrs));
+        } else if (attr_type  == "a_f32") {
+            return ov::Any(decode_vector_attrs_value<float>(attrs));
+        } else if (attr_type  == "a_f64") {
+            return ov::Any(decode_vector_attrs_value<double>(attrs));
+        } else {
+            FRONT_END_GENERAL_CHECK(false, "unsupport vector attr type:", attr_type);
+            break;
+        }
+    }
+    return {};
+}
+
+ov::Any decode_attr(const nlohmann::json& attr) {
+    auto& attr_type = attr.at("AT");
+    std::string attr_type_name = attr_type.at("#").template get<std::string>();
+    auto pos = attr_type_name.find(".");
+    attr_type_name = attr_type_name.substr(pos + 1);
+    if (attr_type_name  == "a_i32") {
+        return ov::Any(decode_simple_attr_value<int32_t>(attr_type));
+    } else if (attr_type_name  == "a_i64") {
+        return ov::Any(decode_simple_attr_value<int64_t>(attr_type));
+    } else if (attr_type_name  == "a_bool") {
+        return ov::Any(decode_simple_attr_value<bool>(attr_type));
+    } else if (attr_type_name  == "a_str") {
+        return ov::Any(decode_simple_attr_value<std::string>(attr_type));
+    } else if (attr_type_name  == "a_f32") {
+        return ov::Any(decode_simple_float_attr_value<float>(attr_type));
+    } else if (attr_type_name  == "a_f64") {
+        return ov::Any(decode_simple_float_attr_value<double>(attr_type));
+    } else if (attr_type_name  == "a_dtype") {
+        std::string dtype_str = decode_simple_attr_value<std::string>(attr_type);
+        return ov::Any(convert_to_ov_type_from_str(dtype_str));
+    } else if (attr_type_name  == "a_array") {
+        return ov::Any(decode_vector_attrs(attr_type));
+    }  else {
+        FRONT_END_GENERAL_CHECK(false, "unsupport attr type:", attr_type);
+    }
+    return {};
+}
+ov::element::Type convert_to_ov_type(TypeType type) {
+    static const std::map<json::TypeType, ov::element::Type> type_map{
+        {BOOL, ov::element::boolean},
+        {I16, ov::element::i16},
+        {I32, ov::element::i32},
+        {I64, ov::element::i64},
+        {F16, ov::element::f16},
+        {F32, ov::element::f32},
+        {F64, ov::element::f64},
+        {U8, ov::element::u8},
+        {I8, ov::element::i8},
+        {BF16, ov::element::bf16}};
+    auto it = type_map.find(type);
+    OPENVINO_ASSERT(it != type_map.end(), "Cannot convert PDPD type to ov::element::Type");
+    return it->second;
+}
+ov::element::Type convert_to_ov_type_from_str(const std::string& type) {
+    static const std::map<std::string, ov::element::Type> type_map{
+        {"int16", ov::element::i16},
+        {"int32", ov::element::i32},
+        {"int64", ov::element::i64},
+        {"float16", ov::element::f16},
+        {"float32", ov::element::f32},
+        {"float64", ov::element::f64},
+        {"bool", ov::element::boolean}
+    };
+    auto it = type_map.find(type);
+    OPENVINO_ASSERT(it != type_map.end(), "Cannot convert PDPD str ",  type, " to ov::element::Type");
+    return it->second;
+}
+
+TypeType Port::get_precision() const {
+   assert(descs.size() >= 1);
+   return descs[0].precision;
+}
+
+const std::vector<int64_t>& Port::get_shapes() const {
+   assert(descs.size() >= 1);
+   return descs[0].shapes;
+}
+
+const std::vector<size_t> Port::get_static_shapes() const {
+    assert(descs.size() >= 1);
+    std::vector<size_t> static_shapes(descs[0].shapes.size());
+    std::transform(descs[0].shapes.begin(), descs[0].shapes.end(), static_shapes.begin(), [](int64_t v) {
+        assert(v >= 0);
+        return static_cast<size_t>(v);
+    });
+    return static_shapes;
+}
+
+const std::string& Port::get_layout() const {
+   assert(descs.size() >= 1);
+   return descs[0].layout;
+}
+
+const std::vector<int64_t>& OP::get_sub_inputs_ids(const size_t block_idx) const {
+    for(auto& region : sub_region_vecs) {
+        for(auto& block : region->blocks) {
+           if (block_idx == block.id) {
+               return block.input_ids;
+           }
+        }
+    }
+    OPENVINO_ASSERT(false, "Cannot find block_idx: ",  block_idx);
+}
+const std::vector<int64_t>& OP::get_sub_outputs_ids(const size_t block_idx) const {
+    for(auto& region : sub_region_vecs) {
+        for(auto& block : region->blocks) {
+           if (block_idx == block.id) {
+               return block.output_ids;
+           }
+        }
+    }
+    OPENVINO_ASSERT(false, "Cannot find block_idx: ",  block_idx);
+}
+
+}  // namespace json
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/json_data.hpp b/src/frontends/paddle/src/json_data.hpp
new file mode 100644
index 0000000000..5b88092532
--- /dev/null
+++ b/src/frontends/paddle/src/json_data.hpp
@@ -0,0 +1,143 @@
+#pragma once
+#include "openvino/core/any.hpp"
+#include "openvino/core/type/element_type.hpp"
+#include <nlohmann/json.hpp>
+#include <limits>
+#include <type_traits>
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace json {
+enum Dialet {
+    Builtin = 0,
+    Operator = 1,
+    ControlFlow = 2,
+    CustomOp = 3,
+    Dist = 4
+};
+enum TypeType {
+    UNDEFINED,
+    BF16,
+    F16,
+    F32,
+    F64,
+    I8,
+    U8,
+    I16,
+    I32,
+    I64,
+    INDEX,
+    BOOL,
+    C64,
+    C128,
+    F8E4M3FN,
+    F8E5M2,
+    DTENSOR,
+    VEC
+};
+
+struct PortDesc {
+   TypeType precision;
+   std::vector<int64_t> shapes;
+   std::string layout;
+   std::vector<std::vector<size_t>> lod;
+   uint64_t offset = 0;
+};
+
+struct Port {
+   int64_t id = 0;
+   std::string type;
+   std::vector<PortDesc> descs;
+   TypeType get_precision() const;
+   const std::vector<int64_t>& get_shapes() const;
+   const std::vector<size_t> get_static_shapes() const;
+   const std::string& get_layout() const;
+   bool used = false;
+};
+struct Region;
+class OP {
+public:
+   OP(const nlohmann::json& json_data_):json_data(json_data_){}
+   const std::vector<int64_t>& get_sub_inputs_ids(const size_t block_idx) const;
+   const std::vector<int64_t>& get_sub_outputs_ids(const size_t block_idx) const;
+   std::string name;
+   std::string type;
+   std::vector<int64_t> inputIds;
+   std::set<int64_t> unusedInputIds;
+   std::vector<Port> outputPorts;
+   bool is_distributed = false;
+   bool is_parameter = false;
+   bool need_clip = false;
+   std::string distAttrs;
+   std::string attrs;
+   std::string outAttrs;
+   std::string quantAttrs;
+   std::vector<size_t> sub_block_idxs;
+   std::vector<std::shared_ptr<Region>> sub_region_vecs;
+   const nlohmann::json& json_data;
+};
+struct Block {
+    std::string name;
+    uint64_t id;
+    std::vector<Port> args;
+    std::vector<OP> ops;
+    std::vector<int64_t> input_ids;
+    std::vector<int64_t> output_ids;
+};
+struct Region{
+    std::string name;
+    std::vector<Block> blocks;
+};
+struct Graph {
+    std::string magic;
+    uint64_t version = 0;
+    bool trainable = false;
+    std::vector<std::shared_ptr<Region>> regions;
+    nlohmann::json json_data;
+};
+TypeType convertFromStringToType(std::string type);
+void decodeRegion(const nlohmann::json& json, std::shared_ptr<Region> region);
+void decodeBlock(const nlohmann::json& json, Block& block);
+void decodeOP(const nlohmann::json& json, OP& op);
+void decodeConst(const nlohmann::json& json, OP& op);
+void decodeOutPorts(const nlohmann::json& json, OP& op);
+void decodePort(const nlohmann::json& json, Port& port);
+void decodePortDesc(const nlohmann::json& json, PortDesc& desc);
+void decodeArgs(const nlohmann::json& json, Port& port);
+template<typename T>
+T decode_simple_float_attr_value(const nlohmann::json& json) {
+    if (json.contains("VD")) {
+        if (json["VD"] == "-INF") {
+             return -std::numeric_limits<T>::infinity();
+        } else if (json["VD"] == "INF") {
+             return std::numeric_limits<T>::infinity();
+        } else {
+            throw std::runtime_error("unkown VD value");
+            return std::numeric_limits<T>::infinity();
+        }
+    }
+    return json.at("D").template get<T>();
+};
+template<typename T>
+T decode_simple_attr_value(const nlohmann::json& json) {
+    return json.at("D").template get<T>();
+};
+
+template<typename T>
+std::vector<T> decode_vector_attrs_value(const nlohmann::json& attrs) {
+    auto& attr_data = attrs.at("D");
+    std::vector<T> result;
+    for(auto& attr : attr_data) {
+        T attr_value = attr.at("D").template get<T>();
+        result.push_back(std::move(attr_value));
+    }
+    return result;
+};
+ov::Any decode_vector_attrs(const nlohmann::json& attrs);
+ov::Any decode_attr(const nlohmann::json& attr);
+ov::element::Type convert_to_ov_type(TypeType type);
+ov::element::Type convert_to_ov_type_from_str(const std::string& type);
+}  // namespace json
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/json_input_model_imp.cpp b/src/frontends/paddle/src/json_input_model_imp.cpp
new file mode 100644
index 0000000000..9da6cc060f
--- /dev/null
+++ b/src/frontends/paddle/src/json_input_model_imp.cpp
@@ -0,0 +1,463 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+
+#include <fstream>
+#include <memory>
+#include <vector>
+#if defined(__MINGW32__) || defined(__MINGW64__)
+#    include <filesystem>
+#endif
+#include <queue>
+
+#include "decoder_proto.hpp"
+#include "framework.pb.h"
+#include "input_model.hpp"
+#include "openvino/frontend/paddle/node_context.hpp"
+#include "openvino/opsets/opset7.hpp"
+#include "openvino/util/common_util.hpp"
+#include "openvino/util/file_util.hpp"
+#include "paddle_utils.hpp"
+#include "place.hpp"
+#include "decoder_json.hpp"
+#include "json_input_model_imp.hpp"
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace json {
+using namespace ::paddle::framework::proto;
+void JsonInputModelImpl::load_places() {
+    auto& graph = m_fw_ptr->m_graph;
+    int cnt_of_regions = graph.regions.size();
+    std::map<std::string, uint64_t> op_statistics;
+    uint32_t cnt_of_blocks = 0;
+    // get block num of main graph
+    for (int region_idx = 0; region_idx < cnt_of_regions; region_idx++) {
+        auto& blocks = graph.regions[region_idx]->blocks;
+        for (auto& block : blocks) {
+            block.id = cnt_of_blocks;
+            cnt_of_blocks++;
+        }
+    }
+    // collect sub regions of op, if sublock has subblock?
+    std::vector<std::shared_ptr<Region>> sub_regions;
+    for (int region_idx = 0; region_idx < cnt_of_regions; region_idx++) {
+        const auto& blocks = graph.regions[region_idx]->blocks;
+        for (size_t block_idx = 0; block_idx < blocks.size(); block_idx++) {
+            const auto& block = blocks[block_idx];
+            for (const auto& op : block.ops) {
+                for (size_t i = 0; i < op.sub_region_vecs.size(); i++) {
+                    sub_regions.push_back(op.sub_region_vecs[i]);
+                    for (const auto& block : op.sub_region_vecs[i]->blocks) {
+                        // we need to modify the value of op and block
+                        auto* op_ptr = (json::OP*)(&op);
+                        op_ptr->sub_block_idxs.push_back(cnt_of_blocks);
+                        auto* block_ptr = (json::Block*)(&block);
+                        block_ptr->id = cnt_of_blocks;
+                        cnt_of_blocks++;
+                    }
+                }
+            }
+        }
+    }
+    // add sub regions to graph.regions
+    for (auto& item : sub_regions) {
+        graph.regions.push_back(item);
+    }
+    cnt_of_regions = graph.regions.size();
+    m_op_places.resize(cnt_of_blocks);
+    // collect all used port
+    std::set<size_t> usedInputIds;
+    for (int region_idx = 0; region_idx < cnt_of_regions; region_idx++) {
+        const auto& blocks = graph.regions[region_idx]->blocks;
+        for (size_t block_idx = 0; block_idx < blocks.size(); block_idx++) {
+            const auto& block = blocks[block_idx];
+            std::set<uint64_t> block_inputs;
+            std::set<uint64_t> outputs;
+            std::set<uint64_t> block_outputs;
+            for (const auto& op : block.ops) {
+                for (const auto& inputId : op.inputIds) {
+                    usedInputIds.insert(inputId);
+                    if (op.type == "fetch" || op.type == "yield") {
+                        block_outputs.insert(inputId);
+                    }
+                    if (outputs.find(inputId) != outputs.end()) {
+                        continue;
+                    }
+                    if (block_inputs.find(inputId) == block_inputs.end()) {
+                        block_inputs.insert(inputId);
+                    }
+                }
+                for (const auto& port : op.outputPorts) {
+                    outputs.insert(port.id);
+                }
+            }
+
+            auto* block_ptr = (json::Block*)(&block);
+            for (auto& item : block_inputs) {
+                block_ptr->input_ids.push_back(item);
+            }
+            for (auto& item : block_outputs) {
+                block_ptr->output_ids.push_back(item);
+            }
+        }
+    }
+    for (int region_idx = 0; region_idx < cnt_of_regions; region_idx++) {
+       const auto& blocks = graph.regions[region_idx]->blocks;
+       for (size_t block_idx = 0; block_idx < blocks.size(); block_idx++) {
+           const auto& block = blocks[block_idx];
+           for (const auto& arg : block.args) {
+               json::Port* outputPtr = (json::Port*)(&arg);
+               outputPtr->used = true;
+               auto out_port = std::make_shared<OutPortPlace>(m_input_model);
+               auto port_name = std::to_string(arg.id);
+               const std::vector<std::string> tensor_names = {port_name};
+               m_var_places[port_name] = std::make_shared<JsonTensorPlace>(m_input_model, tensor_names, arg);
+           }
+           for (const auto& op : block.ops) {
+               auto op_place = std::make_shared<JsonOpPlace>(m_input_model, op);
+               op_place->set_decoder(std::make_shared<DecoderJson>(op_place));
+               if (m_telemetry) {
+                   op_statistics[op.type]++;
+               }
+               m_op_places[block.id].push_back(op_place);
+               for (const auto& output : op.outputPorts) {
+                   auto it = usedInputIds.find(output.id);
+                   if (it == usedInputIds.end() ||  op.type == "fetch") {
+                       continue;
+                   }
+                   json::Port* outputPtr = (json::Port*)(&output);
+                   outputPtr->used = true;
+                   auto out_port = std::make_shared<OutPortPlace>(m_input_model);
+                   auto port_name = std::to_string(output.id);
+                   const std::vector<std::string> tensor_names = {port_name};
+                   m_var_places[port_name] = std::make_shared<JsonTensorPlace>(m_input_model, tensor_names, output);
+                   if (op.type == "data") {
+                       m_inputs.push_back(m_var_places[port_name]);
+                   }
+                   if (op.is_parameter) {
+                       m_const_name_to_id_map[op.name] = port_name;
+                   }
+                   // connect out_port and tensor
+                   m_var_places[port_name]->add_producing_port(out_port);
+                   out_port->set_target_tensor(m_var_places[port_name]);
+                   // connect out_port and op
+                   op_place->add_out_port(out_port, port_name);
+                   out_port->set_op(op_place);
+               }
+
+               for (const auto& inputId : op.inputIds) {
+                   auto in_port = std::make_shared<InPortPlace>(m_input_model);
+                   auto port_name = std::to_string(inputId);
+                   auto it = m_var_places.find(port_name);
+                   // connect in_port and tensor
+                   if (it != m_var_places.end()) {
+                       if (op.type == "fetch") {
+                           m_outputs.push_back(m_var_places[port_name]);
+                       }
+                       const auto& tensor = it->second;
+                       tensor->add_consuming_port(in_port);
+                       in_port->set_source_tensor(tensor);
+
+                       // connect in_port and op
+                       op_place->add_in_port(in_port, port_name);
+                       in_port->set_op(op_place);
+                   } else {
+                       json::OP* op_ptr = (json::OP*)(&op);
+                       op_ptr->unusedInputIds.insert(inputId);
+                       std::cout << "WARNING, can't find the input port " << port_name << " type:" << op.type << std::endl;
+                   }
+               }
+           }
+        }
+    }
+    // reverse inputs for input params of models
+    std::reverse(m_inputs.begin(), m_inputs.end());
+
+    if (m_telemetry) {
+        for (const auto& op : op_statistics) {
+            m_telemetry->send_event("op_count", "paddle_" + op.first, static_cast<int>(op.second));
+        }
+    }
+}
+
+std::vector<std::shared_ptr<BaseOpPlace>> JsonInputModelImpl::get_op_places(const int32_t blck_idx) const {
+    if (m_graph_changed) {
+        return determine_cut_nodes();
+    }
+    if (static_cast<size_t>(blck_idx) < m_op_places.size())
+        return m_op_places[blck_idx];
+    return {};
+}
+
+std::vector<std::shared_ptr<BaseOpPlace>> JsonInputModelImpl::determine_cut_nodes() const {
+    // std::queue<OpPlace*> q;
+    // std::unordered_set<OpPlace*> visited;
+    // std::vector<std::shared_ptr<JsonOpPlace>> new_op_places;
+    // new_op_places.reserve(m_op_places[0].size());
+    // // Marking nodes from outputs to inputs/constants
+    // for (const auto& output : get_outputs()) {
+    //     if (!output->is_input()) {
+    //         auto paddle_output_op = std::dynamic_pointer_cast<JsonOpPlace>(output->get_producing_operation());
+    //         FRONT_END_GENERAL_CHECK(paddle_output_op != nullptr, "Output doesn't have producing operation");
+    //         if (!visited.count(paddle_output_op.get())) {
+    //             visited.insert(paddle_output_op.get());
+    //             q.push(paddle_output_op.get());
+    //             new_op_places.push_back(paddle_output_op);
+    //         }
+    //     }
+    // }
+    // while (!q.empty()) {
+    //     auto p_op = q.front();
+    //     q.pop();
+    //     for (const auto& map_pair : p_op->get_input_ports()) {
+    //         for (const auto& port : map_pair.second) {
+    //             auto tensor = port->get_source_tensor();
+    //             if (tensor && !tensor->is_input() && !m_tensor_values.count(tensor->get_names()[0])) {
+    //                 std::shared_ptr<JsonOpPlace> paddle_op =
+    //                     std::dynamic_pointer_cast<JsonOpPlace>(tensor->get_producing_operation());
+    //                 if (paddle_op && !visited.count(paddle_op.get())) {
+    //                     visited.insert(paddle_op.get());
+    //                     q.push(paddle_op.get());
+    //                     new_op_places.push_back(paddle_op);
+    //                 }
+    //             }
+    //         }
+    //     }
+    // }
+    // std::reverse(new_op_places.begin(), new_op_places.end());
+    // return new_op_places;
+    return {};
+}
+
+// load_consts with stream is compatible with new PaddlePaddle API.
+void JsonInputModelImpl::load_consts(std::istream* weight_stream) {
+    std::set<std::string> param_names_set;
+    for (const auto& block_op_places : m_op_places) {
+        for (const auto& op_place : block_op_places) {
+            const auto& op = std::dynamic_pointer_cast<JsonOpPlace>(op_place)->get_op();
+            const auto& name = op.name;
+            //if (ov::util::ends_with(name, std::string{"data"}) || ov::util::ends_with(name, std::string{"fetch"}))
+            //    continue;
+
+            // var_desc.persistable() is used to mark node const value or not.
+            if (!op.is_parameter)
+               continue;
+            std::cout << "op.name:" << op.name << "op.type:" << op.type << std::endl;
+            param_names_set.insert(name);
+        }
+    }
+    for (auto& name : param_names_set) {
+        // FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
+        FRONT_END_GENERAL_CHECK(weight_stream != nullptr&& weight_stream->peek() != EOF,
+                                "PaddlePaddle *.pdiparams format weight file doesn't exist!");
+        /*
+            reference:
+            https://github.com/PaddlePaddle/Paddle2ONNX/blob/c14446437041a0aa3572994d085b7a35c5b0985c/paddle2onnx/parser/parser.cc#L261
+            When deserialize the proto, the header of each weight
+            [ 4 byte ]      -- version(not need)
+            [   8 byte   ]  -- lod_level(not need)
+            [ 4 byte ]      -- version(not need)
+            [ 4 byte ]      -- TensorDesc size
+            [ x byte ... ]  -- TensorDesc
+            [ y byte ... ]  -- weight
+        */
+        {
+            const size_t header_size = 16;
+            std::vector<char> header(header_size);
+            weight_stream->read(&header[0], header_size);
+        }
+
+        int32_t size;
+        weight_stream->read(reinterpret_cast<char*>(&size), sizeof(size));
+
+        std::unique_ptr<char[]> buf(new char[size]);
+        weight_stream->read(reinterpret_cast<char*>(buf.get()), size);
+
+        std::unique_ptr<::paddle::framework::proto::VarType_TensorDesc> tensor_desc(
+            new ::paddle::framework::proto::VarType_TensorDesc());
+        tensor_desc->ParseFromArray(buf.get(), size);
+        Shape shape(tensor_desc->dims().cbegin(), tensor_desc->dims().cend());
+        const auto& type = get_ov_type(tensor_desc->data_type());
+        const auto& data_length = shape_size(shape) * type.size();
+        // std::cout << "name:" << name << " data_length:" << data_length << std::endl;
+        std::vector<uint8_t> tensor_data(data_length);
+
+        bool read_succeed = read_tensor(*weight_stream, reinterpret_cast<char*>(&tensor_data[0]), data_length);
+        FRONT_END_GENERAL_CHECK(read_succeed,
+                                "File containing constant with name ",
+                                name,
+                                " wasn't successfully read.");
+
+        auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
+        // if (shape_size(shape) > 8 * 2) {
+        //     auto* data = (float*)(&tensor_data[0]);
+        //     float a  = *data;
+        //     float b  = *(data + 1);
+        //     std::cout << " "  << a << " " << b << std::endl;
+        // }
+        const_node->set_friendly_name(name);
+        m_tensor_values[m_const_name_to_id_map[name]] = const_node;
+    }
+}
+
+
+void JsonInputModelImpl::create_temp_consts() {
+    for (const auto& block_op_places : m_op_places) {
+        for (const auto& op_place : block_op_places) {
+            const auto& op = std::dynamic_pointer_cast<JsonOpPlace>(op_place)->get_op();
+            if (op.type != "full_int_array" && op.type != "full" && op.type != "create_array") {
+                continue;
+            }
+            auto* op_ptr = (json::OP*)&op;
+            op_ptr->is_parameter = true;
+            auto decoder = op_place->get_decoder();
+            if (op.type == "full_int_array") {
+                auto value_any = decoder->get_attribute("value");
+                auto value = std::vector<int64_t>{};
+                if (!value_any.empty()) {
+                    value = value_any.as<std::vector<int64_t>>();
+                }
+                for (auto& port : op.outputPorts) {
+                    auto type = convert_to_ov_type(port.get_precision());
+                    auto shape = ov::Shape(port.get_static_shapes());
+                    auto const_node = opset7::Constant::create(type, shape, (int8_t*)(&value[0]));
+                    auto port_name = std::to_string(port.id);
+                    const_node->set_friendly_name(port_name);
+                    m_tensor_values[port_name] = const_node;
+                    // std::cout << "full_int_array:" << port_name << std::endl;
+                }
+            } else if (op.type == "full") {
+                auto value = decoder->get_attribute("value").as<double>();
+                for (auto& port : op.outputPorts) {
+                    auto type = convert_to_ov_type(port.get_precision());
+                    auto shape = ov::Shape(port.get_static_shapes());
+                    auto const_node = std::make_shared<ov::op::v0::Constant>(type, shape);
+                    const_node->fill_data(type, value);
+                    auto port_name = std::to_string(port.id);
+                    const_node->set_friendly_name(port_name);
+                    m_tensor_values[port_name] = const_node;
+                    // std::cout << "full_int_array:" << port_name << std::endl;
+                }
+            } else if (op.type == "create_array") {
+                for (auto& port : op.outputPorts) {
+                    auto shape = ov::Shape(port.get_static_shapes());
+                    shape.insert(shape.begin(), 1);  // unsqueeze
+                    auto type = convert_to_ov_type(port.get_precision());
+                    auto const_node = std::make_shared<ov::op::v0::Constant>(type, shape);
+                    const_node->fill_data(type, 0);
+                    auto port_name = std::to_string(port.id);
+                    const_node->set_friendly_name(port_name);
+                    const_node->output(0).get_tensor().add_names({port_name});
+                    m_tensor_values[port_name] = const_node;
+                }
+            }
+        }
+    }
+}
+
+JsonInputModelImpl::JsonInputModelImpl(const std::vector<std::istream*>& streams,
+                                           const InputModel& input_model,
+                                           const std::shared_ptr<TelemetryExtension>& telemetry)
+    : m_fw_ptr{std::make_shared<JsonProgramDesc>()},
+      m_input_model(input_model),
+      m_telemetry(telemetry) {
+    if (streams.size() != 1) {
+        FRONT_END_GENERAL_CHECK(streams.size() == 2,
+                                "Two streams are needed to load a model: model and weights streams");
+    }
+    FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(*(streams[0])), "Model can't be parsed");
+    load_places();
+    if (streams.size() > 1)
+        load_consts(streams[1]);
+    create_temp_consts();
+}
+
+std::vector<Place::Ptr> JsonInputModelImpl::get_inputs() const {
+    return m_inputs;
+}
+
+std::vector<Place::Ptr> JsonInputModelImpl::get_outputs() const {
+    return m_outputs;
+}
+
+Place::Ptr JsonInputModelImpl::get_place_by_tensor_name(const std::string& tensorName) const {
+    if (m_var_places.count(tensorName))
+        return m_var_places.at(tensorName);
+    return nullptr;
+}
+
+namespace {
+std::shared_ptr<BaseTensorPlace> castToTensorPlace(const Place::Ptr& place) {
+    if (auto var_place = std::dynamic_pointer_cast<JsonTensorPlace>(place)) {
+        return var_place;
+    } else if (auto in_port_place = std::dynamic_pointer_cast<InPortPlace>(place)) {
+        return in_port_place->get_source_tensor_paddle();
+    } else if (auto out_port_place = std::dynamic_pointer_cast<OutPortPlace>(place)) {
+        return out_port_place->get_target_tensor_paddle();
+    }
+    FRONT_END_GENERAL_CHECK(false, "Cannot cast this Place to TensorPlacepaddle.");
+}
+
+}  // namespace
+
+void JsonInputModelImpl::override_all_inputs(const std::vector<Place::Ptr>& inputs) {
+    m_graph_changed = true;
+    m_inputs.clear();
+    for (const auto& inp : inputs) {
+        m_inputs.push_back(castToTensorPlace(inp));
+    }
+}
+
+void JsonInputModelImpl::override_all_outputs(const std::vector<Place::Ptr>& outputs) {
+    m_graph_changed = true;
+    m_outputs.clear();
+    for (const auto& outp : outputs) {
+        m_outputs.push_back(castToTensorPlace(outp));
+    }
+}
+
+void JsonInputModelImpl::extract_subgraph(const std::vector<Place::Ptr>& inputs,
+                                                  const std::vector<Place::Ptr>& outputs) {
+    m_graph_changed = true;
+    override_all_inputs(inputs);
+    override_all_outputs(outputs);
+}
+
+void JsonInputModelImpl::set_default_shape(Place::Ptr place, const ov::Shape& shape) {
+    FRONT_END_NOT_IMPLEMENTED("set_default_shape");
+}
+
+void JsonInputModelImpl::set_partial_shape(Place::Ptr place, const ov::PartialShape& p_shape) {
+    castToTensorPlace(place)->set_partial_shape(p_shape);
+}
+
+ov::PartialShape JsonInputModelImpl::get_partial_shape(Place::Ptr place) const {
+    return castToTensorPlace(place)->get_partial_shape();
+}
+
+void JsonInputModelImpl::set_element_type(Place::Ptr place, const ov::element::Type& type) {
+    castToTensorPlace(place)->set_element_type(type);
+}
+
+ov::element::Type JsonInputModelImpl::get_element_type(const Place::Ptr& place) const {
+    return castToTensorPlace(place)->get_element_type();
+}
+
+void JsonInputModelImpl::set_tensor_value(Place::Ptr place, const void* value) {
+    // std::cout << "============set_tensor_value============" << std::endl;
+    m_graph_changed = true;
+    auto tensor_place = castToTensorPlace(place);
+    auto p_shape = tensor_place->get_partial_shape();
+    auto type = tensor_place->get_element_type();
+    auto constant = opset7::Constant::create(type, p_shape.to_shape(), value);
+    auto name = tensor_place->get_names()[0];
+    constant->set_friendly_name(name);
+    m_tensor_values[name] = constant;
+}
+}  // namespace json
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/json_input_model_imp.hpp b/src/frontends/paddle/src/json_input_model_imp.hpp
new file mode 100644
index 0000000000..78def02d04
--- /dev/null
+++ b/src/frontends/paddle/src/json_input_model_imp.hpp
@@ -0,0 +1,213 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+
+#include "input_model.hpp"
+#include "json_data.hpp"
+#include "paddle_utils.hpp"
+#include "openvino/util/common_util.hpp"
+#include "openvino/util/file_util.hpp"
+
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace json {
+class JsonProgramDesc {
+public:
+    bool ParseFromIstream (std::istream& pb_stream) {
+        try {
+            m_graph.json_data = nlohmann::json::parse(pb_stream);
+        } catch (nlohmann::json::parse_error& e) {
+            return false;
+        }
+        auto& base_code = m_graph.json_data.at("base_code");
+        m_graph.magic = base_code.at("magic").template get<std::string>();
+        m_graph.trainable = base_code.at("trainable").template get<bool>();
+        m_graph.version = base_code.at("version").template get<uint64_t>();
+        auto& programJson = m_graph.json_data.at("program");
+        auto& regionsJson = programJson.at("regions");
+        for (auto& regionJson : regionsJson) {
+            auto region = std::make_shared<Region>();
+            json::decodeRegion(regionJson, region);
+            m_graph.regions.push_back(std::move(region));
+        }
+        return true;
+    }
+
+    // use for parse sub block in if node
+    bool ParseFromJson(const nlohmann::json& sub_json) {
+        m_graph.magic = "sub_pir";
+        m_graph.trainable = false;
+        m_graph.version = 1;
+        auto& regionsJson = sub_json.at("regions");
+        for (auto& regionJson : regionsJson) {
+            auto newRegion = std::make_shared<Region>();
+            json::decodeRegion(regionJson, newRegion);
+            m_graph.regions.push_back(std::move(newRegion));
+        }
+        return true;
+    }
+
+    int64_t version();
+    Graph m_graph;
+};
+
+class JsonInputModelImpl : public BaseInputModelImpl {
+public:
+    template <typename T>
+    JsonInputModelImpl(const std::basic_string<T>& path,
+                   const InputModel& input_model,
+                   const std::shared_ptr<TelemetryExtension>& telemetry)
+    : m_fw_ptr{std::make_shared<JsonProgramDesc>()},
+      m_input_model(input_model),
+      m_telemetry(telemetry) {
+    std::ifstream weights_stream;
+    std::ifstream pb_stream(get_json_model_path<T>(path, &weights_stream).c_str(), std::ios::in | std::ifstream::binary);
+
+    FRONT_END_GENERAL_CHECK(pb_stream && pb_stream.is_open(),
+                            "Could not open the file: \"",
+                            util::path_to_string(path),
+                            '"');
+    FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(pb_stream), "Model can't be parsed");
+    load_places();
+    if (is_json_model(path)) {
+        load_consts(&weights_stream);
+    } else {
+        load_consts(path);
+    }
+    create_temp_consts();
+    }
+
+    JsonInputModelImpl(const std::vector<std::istream*>& streams,
+                   const InputModel& input_model,
+                   const std::shared_ptr<TelemetryExtension>& telemetry);
+
+    std::vector<Place::Ptr> get_inputs() const;
+    std::vector<Place::Ptr> get_outputs() const;
+    int64_t get_version() const {
+        return m_fw_ptr->m_graph.version;
+    }
+    Place::Ptr get_place_by_tensor_name(const std::string& tensorName) const;
+    void override_all_outputs(const std::vector<Place::Ptr>& outputs);
+    void override_all_inputs(const std::vector<Place::Ptr>& inputs);
+    void extract_subgraph(const std::vector<Place::Ptr>& inputs, const std::vector<Place::Ptr>& outputs);
+    void set_default_shape(Place::Ptr place, const ov::Shape&);
+    void set_partial_shape(Place::Ptr place, const ov::PartialShape&);
+    ov::PartialShape get_partial_shape(Place::Ptr place) const;
+    void set_element_type(Place::Ptr place, const ov::element::Type&);
+    ov::element::Type get_element_type(const Place::Ptr& place) const;
+    void set_tensor_value(Place::Ptr place, const void* value);
+    std::vector<std::shared_ptr<BaseOpPlace>> get_op_places(const int32_t blck_idx) const;
+    std::map<std::string, std::shared_ptr<BaseTensorPlace>> get_var_places() const {
+        return m_var_places;
+    }
+    std::map<paddle::TensorName, Output<Node>> get_tensor_values() const {
+        return m_tensor_values;
+    };
+
+private:
+    void load_places();
+    template <typename T>
+    void load_consts(const std::basic_string<T>& folder_with_weights){
+        std::set<std::string> param_names_set;
+        for (const auto& block_op_places : m_op_places) {
+               for (const auto& op_place : block_op_places) {
+                   const auto& op = std::dynamic_pointer_cast<JsonOpPlace>(op_place)->get_op();
+                   const auto& name = op.name;
+                   //if (ov::util::ends_with(name, std::string{"data"}) || ov::util::ends_with(name, std::string{"fetch"}))
+                   //    continue;
+
+                   // var_desc.persistable() is used to mark node const value or not.
+                   if (!op.is_parameter)
+                      continue;
+                   std::cout << "op.name:" << op.name << "op.type:" << op.type << std::endl;
+                   param_names_set.insert(name);
+               }
+           }
+           for (auto& name : param_names_set) {
+               if (!folder_with_weights.empty()) {
+               #if defined(__MINGW32__) || defined(__MINGW64__)
+                   std::ifstream is(std::filesystem::path(get_const_path(folder_with_weights, name)),
+                                    std::ios::in | std::ifstream::binary);
+               #else
+                   std::ifstream is(get_const_path(folder_with_weights, name), std::ios::in | std::ifstream::binary);
+               #endif
+                   FRONT_END_GENERAL_CHECK(is && is.is_open(), "Cannot open file for constant value.");
+               auto* weight_stream = &is; // FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
+               FRONT_END_GENERAL_CHECK(weight_stream != nullptr&& weight_stream->peek() != EOF,
+                                       "PaddlePaddle *.pdiparams format weight file doesn't exist!");
+               /*
+                   reference:
+                   https://github.com/PaddlePaddle/Paddle2ONNX/blob/c14446437041a0aa3572994d085b7a35c5b0985c/paddle2onnx/parser/parser.cc#L261
+                   When deserialize the proto, the header of each weight
+                   [ 4 byte ]      -- version(not need)
+                   [   8 byte   ]  -- lod_level(not need)
+                   [ 4 byte ]      -- version(not need)
+                   [ 4 byte ]      -- TensorDesc size
+                   [ x byte ... ]  -- TensorDesc
+                   [ y byte ... ]  -- weight
+               */
+               {
+                   const size_t header_size = 16;
+                   std::vector<char> header(header_size);
+                   weight_stream->read(&header[0], header_size);
+               }
+
+               int32_t size;
+               weight_stream->read(reinterpret_cast<char*>(&size), sizeof(size));
+
+               std::unique_ptr<char[]> buf(new char[size]);
+               weight_stream->read(reinterpret_cast<char*>(buf.get()), size);
+
+               std::unique_ptr<::paddle::framework::proto::VarType_TensorDesc> tensor_desc(
+                   new ::paddle::framework::proto::VarType_TensorDesc());
+               tensor_desc->ParseFromArray(buf.get(), size);
+               Shape shape(tensor_desc->dims().cbegin(), tensor_desc->dims().cend());
+               const auto& type = get_ov_type(tensor_desc->data_type());
+               const auto& data_length = shape_size(shape) * type.size();
+               // std::cout << "name:" << name << " data_length:" << data_length << std::endl;
+               std::vector<uint8_t> tensor_data(data_length);
+
+               bool read_succeed = read_tensor(*weight_stream, reinterpret_cast<char*>(&tensor_data[0]), data_length);
+               FRONT_END_GENERAL_CHECK(read_succeed,
+                                       "File containing constant with name ",
+                                       name,
+                                       " wasn't successfully read.");
+
+               auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
+               // if (shape_size(shape) > 8 * 2) {
+               //     auto* data = (float*)(&tensor_data[0]);
+               //     float a  = *data;
+               //     float b  = *(data + 1);
+               //     std::cout << " "  << a << " " << b << std::endl;
+               // }
+               const_node->set_friendly_name(name);
+               m_tensor_values[m_const_name_to_id_map[name]] = const_node;
+            }
+        }
+    }
+    void load_consts(std::istream* weight_stream);
+    void create_temp_consts();
+    std::vector<std::shared_ptr<BaseOpPlace>> determine_cut_nodes() const;
+
+    std::vector<std::vector<std::shared_ptr<BaseOpPlace>>> m_op_places;
+    std::map<std::string, std::shared_ptr<BaseTensorPlace>> m_var_places;
+    std::map<std::string, std::string> m_const_name_to_id_map;
+    std::shared_ptr<JsonProgramDesc> m_fw_ptr;
+    const InputModel& m_input_model;
+    std::vector<Place::Ptr> m_inputs;
+    std::vector<Place::Ptr> m_outputs;
+    std::map<paddle::TensorName, Output<Node>> m_tensor_values;
+
+    std::shared_ptr<TelemetryExtension> m_telemetry;
+
+    // shows if some nodes might be deleted from graph
+    bool m_graph_changed = false;
+};
+}  // namespace json
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/op/argmax.cpp b/src/frontends/paddle/src/op/argmax.cpp
index 6bb48a540c..8937e0e530 100644
--- a/src/frontends/paddle/src/op/argmax.cpp
+++ b/src/frontends/paddle/src/op/argmax.cpp
@@ -4,6 +4,7 @@
 
 #include "openvino/frontend/paddle/node_context.hpp"
 #include "openvino/opsets/opset6.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -16,7 +17,17 @@ NamedOutputs argmax(const NodeContext& node) {
     const Output<ov::Node> k = ov::opset6::Constant::create(dtype, {}, {1});
 
     if (!flatten) {
-        auto axis = node.get_attribute<int64_t>("axis");
+        int64_t axis;
+        if (node.is_json_format()) {
+            auto full = node.get_input("full");
+            auto axis_node = full.get_node_shared_ptr();
+            auto axis_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(axis_node);
+            auto axis_value_vector = axis_const->cast_vector<int64_t>();
+            assert(axis_value_vector.size() == 1);
+            axis = axis_value_vector[0];
+        } else {
+            axis = node.get_attribute<int64_t>("axis");
+        }
         const auto axis_to_remove = ov::opset6::Constant::create(element::u64, Shape{}, {axis});
         auto node_topk = std::make_shared<ov::opset6::TopK>(data, k, axis, "max", "index", dtype);
         const auto reshaped_indices = std::make_shared<ov::opset6::Squeeze>(node_topk->output(1), axis_to_remove);
diff --git a/src/frontends/paddle/src/op/assign_value.cpp b/src/frontends/paddle/src/op/assign_value.cpp
index b7545e0f6c..6d10123e15 100644
--- a/src/frontends/paddle/src/op/assign_value.cpp
+++ b/src/frontends/paddle/src/op/assign_value.cpp
@@ -9,13 +9,20 @@ namespace frontend {
 namespace paddle {
 namespace op {
 NamedOutputs assign_value(const NodeContext& node) {
-    std::vector<int32_t> shape = node.get_attribute<std::vector<int32_t>>("shape");
+    std::vector<int32_t> shape = node.get_attribute<std::vector<int32_t>>("shape", {});
     auto dtype = node.get_attribute<ov::element::Type>("dtype");
     std::shared_ptr<Node> const_node;
 
     switch (dtype) {
     case element::i32: {
-        if (node.has_attribute("int32_values")) {
+        if (node.is_json_format()) {
+            auto values = node.get_attribute<std::vector<double>>("values");
+            std::vector<int32_t> int32_values(values.size());
+            std::transform(values.begin(), values.end(), int32_values.begin(), [](double v) {
+                return static_cast<int32_t>(v);
+            });
+            const_node = {opset6::Constant::create(dtype, Shape{shape.begin(), shape.end()}, int32_values)};
+        } else if (node.has_attribute("int32_values")) {
             auto values = node.get_attribute<std::vector<int32_t>>("int32_values");
             const_node = {opset6::Constant::create(dtype, Shape{shape.begin(), shape.end()}, values)};
         } else {
@@ -58,9 +65,18 @@ NamedOutputs assign_value(const NodeContext& node) {
         break;
     }
     case element::i64: {
-        auto values = node.has_attribute("int64_values") ? node.get_attribute<std::vector<int64_t>>("int64_values")
-                                                         : node.get_attribute<std::vector<int64_t>>("values");
-        const_node = {opset6::Constant::create(dtype, Shape{shape.begin(), shape.end()}, values)};
+        if (node.is_json_format()) {
+            auto values = node.get_attribute<std::vector<double>>("values");
+            std::vector<int64_t> int64_values(values.size());
+            std::transform(values.begin(), values.end(), int64_values.begin(), [](double v) {
+                    return static_cast<int64_t>(v);
+                    });
+            const_node = {opset6::Constant::create(dtype, Shape{shape.begin(), shape.end()}, int64_values)};
+        } else {
+            auto values = node.has_attribute("int64_values") ? node.get_attribute<std::vector<int64_t>>("int64_values")
+                : node.get_attribute<std::vector<int64_t>>("values");
+            const_node = {opset6::Constant::create(dtype, Shape{shape.begin(), shape.end()}, values)};
+        }
         break;
     }
     default: {
diff --git a/src/frontends/paddle/src/op/clip.cpp b/src/frontends/paddle/src/op/clip.cpp
index 029aaf1a4a..21b7ca15a2 100644
--- a/src/frontends/paddle/src/op/clip.cpp
+++ b/src/frontends/paddle/src/op/clip.cpp
@@ -4,6 +4,7 @@
 
 #include "openvino/frontend/paddle/node_context.hpp"
 #include "openvino/opsets/opset6.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -11,8 +12,25 @@ namespace paddle {
 namespace op {
 NamedOutputs clip(const NodeContext& node) {
     auto data = node.get_input("X");
-    auto min = node.get_attribute<float>("min");
-    auto max = node.get_attribute<float>("max");
+    float min;
+    float max;
+    if (node.is_json_format()) {
+        auto min_input = node.get_input("min");
+        auto min_node = min_input.get_node_shared_ptr();
+        auto min_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(min_node);
+        auto min_value_vector = min_const->get_vector<float>();
+        assert(min_value_vector.size() == 1);
+        min = min_value_vector[0];
+        auto max_input = node.get_input("max");
+        auto max_node = max_input.get_node_shared_ptr();
+        auto max_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(max_node);
+        auto max_value_vector = max_const->get_vector<float>();
+        assert(max_value_vector.size() == 1);
+        max = max_value_vector[0];
+    } else {
+        min = node.get_attribute<float>("min");
+        max = node.get_attribute<float>("max");
+    }
     PADDLE_OP_CHECK(node, max >= min, "clip: max value must greater than min value!");
 
     return node.default_single_output_mapping({std::make_shared<ov::opset6::Clamp>(data, min, max)}, {"Out"});
diff --git a/src/frontends/paddle/src/op/concat.cpp b/src/frontends/paddle/src/op/concat.cpp
index 243d7f11c5..3ef0873da1 100644
--- a/src/frontends/paddle/src/op/concat.cpp
+++ b/src/frontends/paddle/src/op/concat.cpp
@@ -4,15 +4,25 @@
 
 #include "openvino/frontend/paddle/node_context.hpp"
 #include "openvino/opsets/opset6.hpp"
-
+#include <cassert>
 namespace ov {
 namespace frontend {
 namespace paddle {
 namespace op {
 NamedOutputs concat(const NodeContext& node) {
     auto data = node.get_ng_inputs("X");
-    auto axis = node.get_attribute<int>("axis");
-    return node.default_single_output_mapping({std::make_shared<ov::opset6::Concat>(data, axis)}, {"Out"});
+    if (node.is_json_format()) {
+        auto full = node.get_input("full");
+        auto axis_node = full.get_node_shared_ptr();
+        auto axis_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(axis_node);
+        auto axis_value_vector = axis_const->cast_vector<int32_t>();
+        assert(axis_value_vector.size() == 1);
+        int axis = axis_value_vector[0];
+        return node.default_single_output_mapping({std::make_shared<ov::opset6::Concat>(data, axis)}, {"Out"});
+    } else {
+        auto axis = node.get_attribute<int>("axis");
+        return node.default_single_output_mapping({std::make_shared<ov::opset6::Concat>(data, axis)}, {"Out"});
+    }
 }
 
 }  // namespace op
diff --git a/src/frontends/paddle/src/op/conv2d_utils.hpp b/src/frontends/paddle/src/op/conv2d_utils.hpp
index db67d70b98..b23372f470 100644
--- a/src/frontends/paddle/src/op/conv2d_utils.hpp
+++ b/src/frontends/paddle/src/op/conv2d_utils.hpp
@@ -18,7 +18,14 @@ template <typename T1, typename T2>
 NamedOutputs conv2d_base(const NodeContext& node) {
     auto data = node.get_input("Input");
     auto filters = node.get_input("Filter");
-
+    // if (node.is_json_format()) {
+    //     auto ksize_op = filters.get_node_shared_ptr();
+    //     auto ksize_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(ksize_op);
+    //     auto filter_value = ksize_const->get_vector<float>();
+    //     for (int i = 0; i < 2; i++) {
+    //          std::cout << "value[0]" << filter_value[i] << std::endl;
+    //     }
+    // }
     const auto strides = node.get_attribute<std::vector<int32_t>>("strides");
     const auto dilations = node.get_attribute<std::vector<int32_t>>("dilations");
     const auto auto_pad_type = get_auto_pad(node);
diff --git a/src/frontends/paddle/src/op/cumsum.cpp b/src/frontends/paddle/src/op/cumsum.cpp
index a13d98fe72..161f8f460d 100644
--- a/src/frontends/paddle/src/op/cumsum.cpp
+++ b/src/frontends/paddle/src/op/cumsum.cpp
@@ -4,6 +4,7 @@
 
 #include "default_opset.hpp"
 #include "openvino/frontend/paddle/node_context.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -11,7 +12,17 @@ namespace paddle {
 namespace op {
 NamedOutputs cumsum(const NodeContext& node) {
     const auto x = node.get_input("X");
-    const auto axis = node.get_attribute<int32_t>("axis", -1);
+    int32_t axis= -1;
+    if (node.is_json_format()) {
+        auto full = node.get_input("full");
+        auto axis_node = full.get_node_shared_ptr();
+        auto axis_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(axis_node);
+        auto axis_value_vector = axis_const->get_vector<int32_t>();
+        assert(axis_value_vector.size() == 1);
+        axis = axis_value_vector[0];
+    } else {
+        axis = node.get_attribute<int32_t>("axis", -1);
+    }
     const auto flatten = node.get_attribute<bool>("flatten", false);
     const auto reverse = node.get_attribute<bool>("reverse", false);
     const auto exclusive = node.get_attribute<bool>("exclusive", false);
diff --git a/src/frontends/paddle/src/op/deformable_conv.cpp b/src/frontends/paddle/src/op/deformable_conv.cpp
index 2c1ae4b585..09d7d26bad 100644
--- a/src/frontends/paddle/src/op/deformable_conv.cpp
+++ b/src/frontends/paddle/src/op/deformable_conv.cpp
@@ -11,8 +11,14 @@ namespace paddle {
 namespace op {
 NamedOutputs deformable_conv(const NodeContext& node) {
     auto input = node.get_input("Input");
+    auto tmp_node = input.get_node_shared_ptr();
+    std::cout << tmp_node->get_shape() << std::endl;
     auto filter = node.get_input("Filter");
+    tmp_node = filter.get_node_shared_ptr();
+    std::cout << tmp_node->get_shape() << std::endl;
     auto offset = node.get_input("Offset");
+    tmp_node = offset.get_node_shared_ptr();
+    std::cout << tmp_node->get_shape() << std::endl;
 
     auto strides = node.get_attribute<std::vector<int>>("strides");
     auto dilations = node.get_attribute<std::vector<int>>("dilations");
diff --git a/src/frontends/paddle/src/op/dropout.cpp b/src/frontends/paddle/src/op/dropout.cpp
index 4859b6ee98..73d5ee2fcb 100644
--- a/src/frontends/paddle/src/op/dropout.cpp
+++ b/src/frontends/paddle/src/op/dropout.cpp
@@ -4,6 +4,7 @@
 
 #include "openvino/frontend/paddle/node_context.hpp"
 #include "openvino/opsets/opset6.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -16,8 +17,19 @@ NamedOutputs dropout(const NodeContext& node) {
                     (dropout_implementation == "downgrade_in_infer" || dropout_implementation == "upscale_in_train"),
                     "Unsupported dropout mode!");
     if (dropout_implementation == "downgrade_in_infer") {
+        float dropout_prob_value = 0;
+        if (node.is_json_format()) {
+            auto full = node.get_input("full");
+            auto dropout_prob_node = full.get_node_shared_ptr();
+            auto dropout_prob_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(dropout_prob_node);
+            auto dropout_prob_value_vector = dropout_prob_const->get_vector<float>();
+            assert(dropout_prob_value_vector.size() == 1);
+            dropout_prob_value = dropout_prob_value_vector[0];
+        } else {
+            dropout_prob_value = node.get_attribute<float>("dropout_prob");
+        }
         auto dropout_prob =
-            ov::opset6::Constant::create(ov::element::f32, {1}, {1 - node.get_attribute<float>("dropout_prob")});
+            ov::opset6::Constant::create(ov::element::f32, {1}, {1 - dropout_prob_value});
         return node.default_single_output_mapping({std::make_shared<ov::opset6::Multiply>(data, dropout_prob)},
                                                   {"Out"});
     } else {
diff --git a/src/frontends/paddle/src/op/elementwise_ops.cpp b/src/frontends/paddle/src/op/elementwise_ops.cpp
index 8adad19387..74e77a9d73 100644
--- a/src/frontends/paddle/src/op/elementwise_ops.cpp
+++ b/src/frontends/paddle/src/op/elementwise_ops.cpp
@@ -71,7 +71,7 @@ NamedOutputs elementwise_floordiv(const NodeContext& node_context) {
     int64_t pd_version = node_context.get_version();
 
     bool python_div = false;
-    if (pd_version >= 2005000 || pd_version == 0) {
+    if (pd_version >= 2005000 || pd_version == 0 || pd_version == 1) {
         python_div = true;
     }
     x = get_tensor_safe(x);
diff --git a/src/frontends/paddle/src/op/elementwise_ops.hpp b/src/frontends/paddle/src/op/elementwise_ops.hpp
index 18f7d228b2..6efc994a0b 100644
--- a/src/frontends/paddle/src/op/elementwise_ops.hpp
+++ b/src/frontends/paddle/src/op/elementwise_ops.hpp
@@ -16,8 +16,12 @@ template <typename T>
 NamedOutputs elementwise_ops(const NodeContext& node) {
     auto x = node.get_input("X");
     auto y = node.get_input("Y");
-
-    auto axis = node.get_attribute<int>("axis");
+    int axis = 0;
+    if (node.is_json_format()) {
+        axis = -1;
+    } else {
+        axis = node.get_attribute<int>("axis");
+    }
 
     PADDLE_OP_CHECK(node, x.get_partial_shape().rank().is_static(), "elementwise_ops: X rank must be static!");
     PADDLE_OP_CHECK(node, y.get_partial_shape().rank().is_static(), "elementwise_ops: Y rank must be static!");
diff --git a/src/frontends/paddle/src/op/expand_v2.cpp b/src/frontends/paddle/src/op/expand_v2.cpp
index c11e6823c1..c1501f26b2 100644
--- a/src/frontends/paddle/src/op/expand_v2.cpp
+++ b/src/frontends/paddle/src/op/expand_v2.cpp
@@ -15,6 +15,12 @@ NamedOutputs expand_v2(const NodeContext& node) {
     Output<Node> shape_expected_node;
     if (node.has_input("Shape")) {
         shape_expected_node = node.get_input("Shape");
+        if (node.is_json_format()) {
+            ov::NodeVector node_vec;
+            auto cast = std::make_shared<Convert>(shape_expected_node, element::i32);
+            node_vec.emplace_back(cast);
+            shape_expected_node = std::make_shared<Concat>(node_vec, 0);
+        }
     } else if (node.has_input("expand_shapes_tensor")) {
         auto inputs = node.get_ng_inputs("expand_shapes_tensor");
         ov::NodeVector node_vec;
@@ -62,4 +68,4 @@ NamedOutputs expand_v2(const NodeContext& node) {
 }  // namespace op
 }  // namespace paddle
 }  // namespace frontend
-}  // namespace ov
\ No newline at end of file
+}  // namespace ov
diff --git a/src/frontends/paddle/src/op/eye.cpp b/src/frontends/paddle/src/op/eye.cpp
index 3b433c5b9a..84bc1ac56a 100644
--- a/src/frontends/paddle/src/op/eye.cpp
+++ b/src/frontends/paddle/src/op/eye.cpp
@@ -4,14 +4,32 @@
 
 #include "default_opset.hpp"
 #include "openvino/frontend/paddle/node_context.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
 namespace paddle {
 namespace op {
 NamedOutputs eye(const NodeContext& node) {
-    auto row = node.get_attribute<int64_t>("num_rows");
-    auto col = node.get_attribute<int64_t>("num_columns", row);
+    int64_t row;
+    int64_t col;
+    if (node.is_json_format()) {
+        auto full = node.get_input("num_rows");
+        auto rows_node = full.get_node_shared_ptr();
+        auto rows_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(rows_node);
+        auto row_vector = rows_const->cast_vector<int64_t>();
+        assert(row_vector.size() == 1);
+        row = row_vector[0];
+        full = node.get_input("num_columns");
+        auto cols_node = full.get_node_shared_ptr();
+        auto cols_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(cols_node);
+        auto col_vector = cols_const->cast_vector<int64_t>();
+        assert(col_vector.size() == 1);
+        col = col_vector[0];
+    } else {
+        row = node.get_attribute<int64_t>("num_rows");
+        col = node.get_attribute<int64_t>("num_columns", row);
+    }
     auto dtype = node.get_attribute<ov::element::Type>("dtype", ov::element::f32);
 
     const auto& row_node = std::make_shared<default_opset::Constant>(ov::element::i64, Shape{}, (row));
diff --git a/src/frontends/paddle/src/op/fill_any_like.cpp b/src/frontends/paddle/src/op/fill_any_like.cpp
index d629f10af2..8f7744b746 100644
--- a/src/frontends/paddle/src/op/fill_any_like.cpp
+++ b/src/frontends/paddle/src/op/fill_any_like.cpp
@@ -5,6 +5,7 @@
 #include "default_opset.hpp"
 #include "op_utils.hpp"
 #include "openvino/frontend/paddle/node_context.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -13,7 +14,17 @@ namespace op {
 NamedOutputs fill_any_like(const NodeContext& node) {
     auto x = node.get_input("X");
     auto dtype = node.get_attribute<ov::element::Type>("dtype", element::dynamic);
-    const auto value = node.get_attribute<float>("value");
+    float value = 0;
+    if (node.is_json_format()) {
+        auto full = node.get_input("full");
+        auto value_node = full.get_node_shared_ptr();
+        auto value_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(value_node);
+        auto value_vector = value_const->cast_vector<float>();
+        assert(value_vector.size() == 1);
+        value = value_vector[0];
+    } else {
+        value = node.get_attribute<float>("value");
+    }
     if (dtype.is_dynamic()) {
         // when type does not define, use the input type
         dtype = x.get_element_type();
diff --git a/src/frontends/paddle/src/op/fill_constant.cpp b/src/frontends/paddle/src/op/fill_constant.cpp
index 69a09f522f..3b021f0be6 100644
--- a/src/frontends/paddle/src/op/fill_constant.cpp
+++ b/src/frontends/paddle/src/op/fill_constant.cpp
@@ -4,16 +4,20 @@
 
 #include "openvino/frontend/paddle/node_context.hpp"
 #include "openvino/opsets/opset6.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
 namespace paddle {
 namespace op {
 NamedOutputs fill_constant(const NodeContext& node) {
-    auto shape = node.get_attribute<std::vector<int64_t>>("shape");
+    std::vector<int64_t> shape;
+    Output<Node> shape_node;
+    if (!node.is_json_format()) {
+        shape = node.get_attribute<std::vector<int64_t>>("shape");
+    }
     auto dtype = node.get_attribute<ov::element::Type>("dtype");
     Output<Node> value_node;
-    Output<Node> shape_node;
     if (node.has_input("ValueTensor")) {
         value_node = node.get_input("ValueTensor");
     } else if (dtype == element::boolean) {
@@ -40,7 +44,8 @@ NamedOutputs fill_constant(const NodeContext& node) {
     }
 
     PADDLE_OP_CHECK(node,
-                    node.has_attribute("shape") || node.has_input("ShapeTensor") || node.has_input("ShapeTensorList"),
+                    node.has_attribute("shape") || node.has_input("ShapeTensor") ||
+                    node.has_input("ShapeTensorList"),
                     "fill_constant shape not set");
 
     if (node.has_input("ShapeTensor")) {
diff --git a/src/frontends/paddle/src/op/if_else_block.cpp b/src/frontends/paddle/src/op/if_else_block.cpp
new file mode 100644
index 0000000000..c66478aac4
--- /dev/null
+++ b/src/frontends/paddle/src/op/if_else_block.cpp
@@ -0,0 +1,47 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "internal/op/if_else_block.hpp"
+
+#include "default_opset.hpp"
+#include "internal/op/while.hpp"
+#include "openvino/frontend/paddle/node_context.hpp"
+#include <cassert>
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace op {
+NamedOutputs if_else_block(const NodeContext& node) {
+    const auto cond = node.get_input("Cond");
+    const auto if_inputs = node.get_ng_inputs("if_inputs");
+    const auto else_inputs = node.get_ng_inputs("else_inputs");
+    const auto sub_block_index = node.get_input("sub_block_indexs");
+    auto sub_block_index_node = sub_block_index.get_node_shared_ptr();
+    auto sub_block_index_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(sub_block_index_node);
+    auto sub_block_index_value_vector = sub_block_index_const->cast_vector<int32_t>();
+    assert(sub_block_index_value_vector.size() == 2);
+    const auto outputs_info = node.get_output_port_infos("Out");
+    std::shared_ptr<Node> placehodler;
+    placehodler = std::make_shared<ov::op::internal::IfElseBlock>(cond,
+                  if_inputs, else_inputs,
+                  sub_block_index_value_vector,
+                  outputs_info);
+    const auto outputs = placehodler->outputs();
+
+    auto out_names = node.get_output_names();
+    auto it = std::find(out_names.begin(), out_names.end(), "Out");
+    PADDLE_OP_CHECK(node, it != out_names.end(), "Expected output not found");
+
+    NamedOutputs named_outputs;
+    for (const auto& output : outputs) {
+        named_outputs[*it].push_back(output);
+    }
+    return named_outputs;
+}
+
+}  // namespace op
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/op/interp.cpp b/src/frontends/paddle/src/op/interp.cpp
index 4e39ce5199..87f6ee3600 100644
--- a/src/frontends/paddle/src/op/interp.cpp
+++ b/src/frontends/paddle/src/op/interp.cpp
@@ -76,7 +76,7 @@ static NamedOutputs interpolate(const NodeContext& node,
     auto out_w = node.get_attribute<int>("out_w");
     auto out_h = node.get_attribute<int>("out_h");
     auto out_d = node.get_attribute<int>("out_d");
-    auto scale = node.get_attribute<std::vector<float>>("scale");
+    auto scale = node.get_attribute<std::vector<float>>("scale", {});
     Output<Node> scales;
     Output<Node> target_spatial_shape;
     bool out_flag = out_w <= 0;
diff --git a/src/frontends/paddle/src/op/matmul.cpp b/src/frontends/paddle/src/op/matmul.cpp
index fc803353d0..d990f2876c 100644
--- a/src/frontends/paddle/src/op/matmul.cpp
+++ b/src/frontends/paddle/src/op/matmul.cpp
@@ -16,7 +16,18 @@ NamedOutputs matmul(const NodeContext& node) {
     auto transpose_b = node.get_attribute<bool>("transpose_Y", false);
     auto mm = std::make_shared<ov::opset6::MatMul>(x, y, transpose_a, transpose_b);
     if (alpha == 1) {
-        return node.default_single_output_mapping({mm}, {"Out"});
+        if (node.is_json_format()) {
+            std::shared_ptr<Node> result = mm;
+            const auto output_info = node.get_output_port_infos("Out");
+            size_t output_size = output_info[0].second.size();
+            if (is_scalar(mm->get_output_partial_shape(0)) && output_size) {
+                auto unsqueeze_scalar = ov::opset6::Constant::create(ov::element::i64, {}, {0});
+                auto result = std::make_shared<ov::op::v0::Unsqueeze>(mm, unsqueeze_scalar);
+            }
+            return node.default_single_output_mapping({result}, {"Out"});
+        } else {
+            return node.default_single_output_mapping({mm}, {"Out"});
+        }
     } else {
         auto alpha_node = ov::opset6::Constant::create(ov::element::f32, {1}, {alpha});
         return node.default_single_output_mapping({std::make_shared<ov::opset6::Multiply>(mm, alpha_node)}, {"Out"});
diff --git a/src/frontends/paddle/src/op/matrix_nms.cpp b/src/frontends/paddle/src/op/matrix_nms.cpp
index b3f6488b39..e83d0fa410 100644
--- a/src/frontends/paddle/src/op/matrix_nms.cpp
+++ b/src/frontends/paddle/src/op/matrix_nms.cpp
@@ -42,6 +42,9 @@ NamedOutputs matrix_nms(const NodeContext& node) {
     } else {
         return_rois_num = false;
     }
+    // if (node.is_json_format()) {
+    //     return_rois_num = true;
+    // }
 
     auto type_index = node.get_out_port_type("Index");
     PADDLE_OP_CHECK(node,
diff --git a/src/frontends/paddle/src/op/p_norm.cpp b/src/frontends/paddle/src/op/p_norm.cpp
index 50297eced2..5536cabdbe 100644
--- a/src/frontends/paddle/src/op/p_norm.cpp
+++ b/src/frontends/paddle/src/op/p_norm.cpp
@@ -16,7 +16,18 @@ NamedOutputs p_norm(const NodeContext& node) {
     const auto keepdim = node.get_attribute<bool>("keepdim", false);
 
     const auto absNode = std::make_shared<default_opset::Abs>(data);
-    const auto axisNode = default_opset::Constant::create(ov::element::i32, {1}, {axis});
+    auto axisNode = default_opset::Constant::create(ov::element::i32, {1}, {axis});
+    if (node.is_json_format()) {
+        const auto asvector = node.get_attribute<bool>("asvector", false);
+        if (asvector) {
+            const auto input_shape = data.get_partial_shape();
+            std::vector<int32_t> all_axis;
+            for (int i = 0; i < input_shape.size(); i++) {
+                all_axis.push_back(i);
+            }
+            axisNode = default_opset::Constant::create(ov::element::i32, {input_shape.size()}, all_axis);
+        }
+    }
 
     std::shared_ptr<Node> p_norm_node;
     const auto input_shape = data.get_partial_shape();
diff --git a/src/frontends/paddle/src/op/pad3d.cpp b/src/frontends/paddle/src/op/pad3d.cpp
index 5d4b984f72..5f9cf5c164 100644
--- a/src/frontends/paddle/src/op/pad3d.cpp
+++ b/src/frontends/paddle/src/op/pad3d.cpp
@@ -19,7 +19,14 @@ NamedOutputs pad3d(const NodeContext& node) {
 
     // padding of type int feature only supported by paddle 'develop'
     // version(>=2.1.0)
-    if (node.has_attribute("paddings")) {
+    if (node.is_json_format()) {
+        auto full = node.get_input("full");
+        auto pad_node = full.get_node_shared_ptr();
+        auto pad_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(pad_node);
+        auto pad_value_vector = pad_const->cast_vector<int32_t>();
+        PADDLE_OP_CHECK(node, pad_value_vector.size() == 6, "paddings Params size should be 6 in pad3d!");
+        paddings = pad_value_vector;
+    } else if (node.has_attribute("paddings")) {
         auto paddings_vector = node.get_attribute<std::vector<int32_t>>("paddings");
         PADDLE_OP_CHECK(node, paddings_vector.size() == 6, "paddings Params size should be 6 in pad3d!");
         paddings = paddings_vector;
diff --git a/src/frontends/paddle/src/op/pool2d.cpp b/src/frontends/paddle/src/op/pool2d.cpp
index 6332959191..da3366c8ee 100644
--- a/src/frontends/paddle/src/op/pool2d.cpp
+++ b/src/frontends/paddle/src/op/pool2d.cpp
@@ -63,7 +63,19 @@ NamedOutputs pool2d(const NodeContext& node) {
     auto pooling_type = node.get_attribute<std::string>("pooling_type", {});
     auto global_pooling = node.get_attribute<bool>("global_pooling");
     auto adaptive = node.get_attribute<bool>("adaptive");
-    auto kernel_shape = node.get_attribute<std::vector<int32_t>>("ksize");
+    std::vector<int32_t> kernel_shape;
+    if (node.is_json_format()) {
+        if (node.has_input("ksize")) {
+            auto full_int_array = node.get_input("ksize");
+            auto ksize_op = full_int_array.get_node_shared_ptr();
+            auto ksize_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(ksize_op);
+            kernel_shape = ksize_const->cast_vector<int32_t>();
+        } else {
+            kernel_shape = node.get_attribute<std::vector<int32_t>>("ksize");
+        }
+    } else {
+        kernel_shape = node.get_attribute<std::vector<int32_t>>("ksize");
+    }
 
     auto rounding_type =
         node.get_attribute<bool>("ceil_mode", false) ? ov::op::RoundingType::CEIL : ov::op::RoundingType::FLOOR;
diff --git a/src/frontends/paddle/src/op/reduce_ops.hpp b/src/frontends/paddle/src/op/reduce_ops.hpp
index 092a4dd351..40a983038b 100644
--- a/src/frontends/paddle/src/op/reduce_ops.hpp
+++ b/src/frontends/paddle/src/op/reduce_ops.hpp
@@ -21,7 +21,16 @@ NamedOutputs reduce_ops(const NodeContext& node) {
     std::vector<int64_t> dims(input_rank);
 
     auto any = node.get_attribute_as_any("dim");
-    if (any.is<std::vector<int32_t>>()) {
+    if (node.is_json_format() && any.empty()) {
+        if (node.has_input("full")) {
+            auto full = node.get_input("full");
+            auto axis_node = full.get_node_shared_ptr();
+            auto axis_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(axis_node);
+            dims = axis_const->cast_vector<int64_t>();
+        } else {
+            dims.resize(0);
+        }
+    } else if (any.is<std::vector<int32_t>>()) {
         auto dim = any.as<std::vector<int32_t>>();
         dims.resize(dim.size());
         std::transform(dim.begin(), dim.end(), dims.begin(), [](int32_t value) {
diff --git a/src/frontends/paddle/src/op/roll.cpp b/src/frontends/paddle/src/op/roll.cpp
index 5b87b9af47..a5f824786f 100644
--- a/src/frontends/paddle/src/op/roll.cpp
+++ b/src/frontends/paddle/src/op/roll.cpp
@@ -21,7 +21,7 @@ NamedOutputs roll(const NodeContext& node) {
         shifts_node = default_opset::Constant::create(element::i64, {shifts.size()}, shifts);
     }
 
-    std::vector<int64_t> axis = node.get_attribute<std::vector<int64_t>>("axis");
+    std::vector<int64_t> axis = node.get_attribute<std::vector<int64_t>>("axis", {});
     if (axis.empty()) {
         const auto const_minus_1 = default_opset::Constant::create(element::i64, Shape{1}, {-1});
         Output<Node> axis_node = default_opset::Constant::create(element::i64, Shape{1}, {0});
diff --git a/src/frontends/paddle/src/op/set_value.cpp b/src/frontends/paddle/src/op/set_value.cpp
index 4a9889d940..2b6ea91f7d 100644
--- a/src/frontends/paddle/src/op/set_value.cpp
+++ b/src/frontends/paddle/src/op/set_value.cpp
@@ -40,7 +40,7 @@ NamedOutputs set_value(const NodeContext& node) {
     PADDLE_OP_CHECK(node, (input_node.get_partial_shape().rank().is_static()), "rank must be static");
     const auto dims = static_cast<int64_t>(input_node.get_partial_shape().rank().get_length());
     auto axes = node.get_attribute<std::vector<int64_t>>("axes");
-    auto decrease_axes = node.get_attribute<std::vector<int64_t>>("decrease_axes");
+    auto decrease_axes = node.get_attribute<std::vector<int64_t>>("decrease_axes", {});
 
     // const auto input_shape_ = input_node.get_partial_shape().get_shape();
     // auto input_shape = default_opset::Constant::create(element::i64, {input_shape_.size()}, input_shape_);
diff --git a/src/frontends/paddle/src/op/slice_ops.hpp b/src/frontends/paddle/src/op/slice_ops.hpp
index ca874c6bf0..4073d94657 100644
--- a/src/frontends/paddle/src/op/slice_ops.hpp
+++ b/src/frontends/paddle/src/op/slice_ops.hpp
@@ -28,7 +28,25 @@ Output<Node> idx_node(const std::string& tensor_alias,
 }
 NamedOutputs slice_op(const NodeContext& node, const bool& stride_input) {
     const auto data = node.get_input("Input");
-    const auto axes = node.get_attribute<std::vector<int32_t>>("axes");
+    std::vector<int32_t> axes;
+    if (node.is_json_format()) {
+        auto axes_any = node.get_attribute_as_any("axes");
+        if (axes_any.is<std::vector<int64_t>>()) {
+            auto axes_64 = axes_any.as<std::vector<int64_t>>();
+            axes.resize(axes_64.size());
+            std::transform(axes_64.begin(), axes_64.end(), axes.begin(), [](int64_t value) {
+                    return static_cast<int32_t>(value);
+                    });
+        } else if (axes_any.is<std::vector<int32_t>>()) {
+            axes = axes_any.as<std::vector<int32_t>>();
+        } else {
+            PADDLE_OP_CHECK(node,
+                    false,
+                    "axes format is not i32 or i64.");
+        }
+    } else {
+        axes = node.get_attribute<std::vector<int32_t>>("axes");
+    }
 
     Output<Node> start_idx_node = idx_node("StartsTensor", "StartsTensorList", "starts", node);
     Output<Node> end_idx_node = idx_node("EndsTensor", "EndsTensorList", "ends", node);
@@ -42,7 +60,16 @@ NamedOutputs slice_op(const NodeContext& node, const bool& stride_input) {
     const auto axes_node = default_opset::Constant::create(element::i32, {axes.size()}, axes);
     const auto slice_node =
         std::make_shared<default_opset::Slice>(data, start_idx_node, end_idx_node, strides_idx_node, axes_node);
-    const auto decrease_axis = node.get_attribute<std::vector<int32_t>>("decrease_axis");
+    std::vector<int32_t> decrease_axis;
+    if (node.is_json_format()) {
+        auto decrease_axis_64 = node.get_attribute<std::vector<int64_t>>("decrease_axis", {});
+        for (auto& item : decrease_axis_64) {
+            decrease_axis.push_back(item);
+        }
+    } else {
+        decrease_axis = node.get_attribute<std::vector<int32_t>>("decrease_axis");
+    }
+
     if (decrease_axis.size() > 0) {
         PartialShape input_shape = data.get_partial_shape();
         PADDLE_OP_CHECK(node,
@@ -76,4 +103,4 @@ NamedOutputs slice_op(const NodeContext& node, const bool& stride_input) {
 }  // namespace op
 }  // namespace paddle
 }  // namespace frontend
-}  // namespace ov
\ No newline at end of file
+}  // namespace ov
diff --git a/src/frontends/paddle/src/op/split.cpp b/src/frontends/paddle/src/op/split.cpp
index 4dac9f8260..91548c8a30 100644
--- a/src/frontends/paddle/src/op/split.cpp
+++ b/src/frontends/paddle/src/op/split.cpp
@@ -14,8 +14,19 @@ NamedOutputs split(const NodeContext& node) {
     using namespace opset7;
     const auto& data = node.get_input("X");
     Output<Node> axis;
-    if (node.has_input("AxisTensor")) {
-        auto input = node.get_input("AxisTensor");
+    std::string axis_name = "AxisTensor";
+    std::string sections_name = "SectionsTensorList";
+    if (node.is_json_format()) {
+        auto inputs = node.get_all_ng_inputs();
+        if (inputs.size() == 2) {
+            axis_name = "Input1";
+        } else if (inputs.size() == 3) {
+            axis_name = "Input2";
+            sections_name = "Input1";
+        }
+    }
+    if (node.has_input(axis_name)) {
+        auto input = node.get_input(axis_name);
         auto zero_node = Constant::create(element::i32, {1}, {0});
         axis = std::make_shared<ReduceMin>(input, zero_node, false);
     } else {
@@ -25,13 +36,13 @@ NamedOutputs split(const NodeContext& node) {
         }
         axis = std::make_shared<Constant>(ov::element::i32, Shape{}, dim);
     }
-    auto num_or_sections = node.get_attribute<int32_t>("num");
+    auto num_or_sections = node.get_attribute<int32_t>("num", 0);
     NamedOutputs named_outputs;
     std::vector<Output<Node>> split_outputs;
     if (num_or_sections == 0) {
         Output<Node> sections_node;
-        if (node.has_input("SectionsTensorList")) {
-            auto inputs = node.get_ng_inputs("SectionsTensorList");
+        if (node.has_input(sections_name)) {
+            auto inputs = node.get_ng_inputs(sections_name);
             sections_node = std::make_shared<ov::opset7::Concat>(inputs, 0);
         } else {
             PADDLE_OP_CHECK(node, node.has_attribute("sections"), "split: num==0 && no sections is invalid.");
diff --git a/src/frontends/paddle/src/op/squeeze.cpp b/src/frontends/paddle/src/op/squeeze.cpp
index b87b966238..e5e4ae2622 100644
--- a/src/frontends/paddle/src/op/squeeze.cpp
+++ b/src/frontends/paddle/src/op/squeeze.cpp
@@ -12,7 +12,12 @@ namespace op {
 NamedOutputs squeeze(const NodeContext& node) {
     auto data = node.get_input("X");
     std::vector<int32_t> axes;
-    if (node.has_attribute("axes")) {
+    if (node.is_json_format()) {
+        auto full = node.get_input("full");
+        auto axis_node = full.get_node_shared_ptr();
+        auto axis_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(axis_node);
+        axes = axis_const->cast_vector<int32_t>();
+    } else if (node.has_attribute("axes")) {
         axes = node.get_attribute<std::vector<int32_t>>("axes");
     }
 
diff --git a/src/frontends/paddle/src/op/tensor_array_to_tensor.cpp b/src/frontends/paddle/src/op/tensor_array_to_tensor.cpp
index c6f42ff963..00c888de25 100644
--- a/src/frontends/paddle/src/op/tensor_array_to_tensor.cpp
+++ b/src/frontends/paddle/src/op/tensor_array_to_tensor.cpp
@@ -24,6 +24,7 @@ namespace op {
 NamedOutputs tensor_array_to_tensor(const NodeContext& node) {
     using namespace default_opset;
     const auto x = node.get_input("X");
+    std::cout << x.get_partial_shape() << std::endl;
     auto axis = node.get_attribute<int32_t>("axis", 0);
     PADDLE_OP_CHECK(node, axis == 0, "axis should be 0, got: ", axis);
 
diff --git a/src/frontends/paddle/src/op/tril_triu.cpp b/src/frontends/paddle/src/op/tril_triu.cpp
index 06480f91e4..a5cbce4ab1 100644
--- a/src/frontends/paddle/src/op/tril_triu.cpp
+++ b/src/frontends/paddle/src/op/tril_triu.cpp
@@ -13,7 +13,7 @@ namespace op {
 NamedOutputs tril_triu(const NodeContext& node) {
     auto x = node.get_input("X");
     const int diagonal = node.get_attribute<int>("diagonal");
-    const auto lower = node.get_attribute<bool>("lower");
+    const auto lower = node.get_attribute<bool>("lower", false);
     PADDLE_OP_CHECK(node, x.get_partial_shape().rank().get_length() == 2, "partial ops only support 2-D Tensor");
 
     const auto diag_node = default_opset::Constant::create(element::i32, Shape{}, {diagonal});
@@ -49,4 +49,4 @@ NamedOutputs tril_triu(const NodeContext& node) {
 }  // namespace op
 }  // namespace paddle
 }  // namespace frontend
-}  // namespace ov
\ No newline at end of file
+}  // namespace ov
diff --git a/src/frontends/paddle/src/op/unique.cpp b/src/frontends/paddle/src/op/unique.cpp
index b19dd5d225..41c7d99cae 100644
--- a/src/frontends/paddle/src/op/unique.cpp
+++ b/src/frontends/paddle/src/op/unique.cpp
@@ -15,7 +15,7 @@ NamedOutputs unique(const NodeContext& node) {
 
     std::vector<Output<Node>> outputs;
 
-    auto axis = node.get_attribute<std::vector<int32_t>>("axis");
+    auto axis = node.get_attribute<std::vector<int32_t>>("axis", {});
     auto dtype = node.get_attribute<ov::element::Type>("dtype");
 
     if (axis.size() != 0) {
@@ -24,11 +24,25 @@ NamedOutputs unique(const NodeContext& node) {
     } else {
         outputs = std::make_shared<ov::opset10::Unique>(x, true, dtype, dtype)->outputs();
     }
-
-    return NamedOutputs{{"Out", {outputs[0]}},
-                        {"Indices", {outputs[1]}},
-                        {"Index", {outputs[2]}},
-                        {"Counts", {outputs[3]}}};
+    if (node.is_json_format()) {
+        NamedOutputs result;
+        result.insert({"Out", {outputs[0]}});
+        if (node.get_attribute<bool>("return_index")) {
+            result.insert({"Index", {outputs[1]}});
+        }
+        if (node.get_attribute<bool>("return_inverse")) {
+            result.insert({"Inverse", {outputs[2]}});
+        }
+        if (node.get_attribute<bool>("return_counts")) {
+            result.insert({"Counts", {outputs[3]}});
+        }
+        return result;
+    } else {
+        return NamedOutputs{{"Out", {outputs[0]}},
+            {"Indices", {outputs[1]}},
+            {"Index", {outputs[2]}},
+            {"Counts", {outputs[3]}}};
+    }
 }
 
 }  // namespace op
diff --git a/src/frontends/paddle/src/op/while.cpp b/src/frontends/paddle/src/op/while.cpp
index 8a8b101bbb..5998fdf5fb 100644
--- a/src/frontends/paddle/src/op/while.cpp
+++ b/src/frontends/paddle/src/op/while.cpp
@@ -6,6 +6,7 @@
 
 #include "default_opset.hpp"
 #include "openvino/frontend/paddle/node_context.hpp"
+#include <cassert>
 
 namespace ov {
 namespace frontend {
@@ -17,13 +18,24 @@ using namespace default_opset;
 NamedOutputs while_(const NodeContext& node) {
     const auto data = node.get_ng_inputs("X");
     const auto cond = node.get_input("Condition");
-    const auto sub_block = node.get_attribute<int32_t>("sub_block");
+    int32_t sub_block = -1;
+    if (node.is_json_format()) {
+        auto sub_block_index = node.get_input("sub_block_index");
+        auto sub_block_index_node = sub_block_index.get_node_shared_ptr();
+        auto sub_block_index_const = std::dynamic_pointer_cast<ov::op::v0::Constant>(sub_block_index_node);
+        auto sub_block_index_value_vector = sub_block_index_const->cast_vector<int32_t>();
+        assert(sub_block_index_value_vector.size() == 1);
+        sub_block = sub_block_index_value_vector[0];
+    } else {
+        sub_block = node.get_attribute<int32_t>("sub_block");
+    }
     auto outputs_info = node.get_output_port_infos("Out");
 
     ov::OutputVector inputs = data;
     inputs.push_back(cond);
     NamedOutputs named_outputs;
-    named_outputs["Out"] = std::make_shared<ov::op::internal::While>(inputs, sub_block, outputs_info)->outputs();
+    named_outputs["Out"] = std::make_shared<ov::op::internal::While>(inputs, sub_block, outputs_info,
+                                                                     node.is_json_format())->outputs();
     return named_outputs;
 }
 }  // namespace op
diff --git a/src/frontends/paddle/src/op_table.cpp b/src/frontends/paddle/src/op_table.cpp
index 58ad4aa909..b21f91fec7 100644
--- a/src/frontends/paddle/src/op_table.cpp
+++ b/src/frontends/paddle/src/op_table.cpp
@@ -21,6 +21,7 @@ OP_CONVERTER(ceil);
 OP_CONVERTER(clip);
 OP_CONVERTER(concat);
 OP_CONVERTER(conditional_block);
+OP_CONVERTER(if_else_block);
 OP_CONVERTER(conv2d);
 OP_CONVERTER(conv2d_transpose);
 OP_CONVERTER(cos);
@@ -196,6 +197,7 @@ std::map<std::string, CreatorFunction> get_supported_ops() {
             {"fill_constant", op::fill_constant},
             {"fill_constant_batch_size_like", op::fill_constant_batch_size_like},
             {"flatten_contiguous_range", op::flatten_contiguous_range},
+            {"flatten", op::flatten_contiguous_range},
             {"flip", op::flip},
             {"floor", op::floor},
             {"gather", op::gather},
@@ -301,8 +303,472 @@ std::map<std::string, CreatorFunction> get_supported_ops() {
             {"scatter", op::scatter},
             {"scatter_nd_add", op::scatter_nd_add},
             {"take_along_axis", op::take_along_axis},
-            {"reduce_any", op::reduce_any}};
+            {"reduce_any", op::reduce_any},
+            // paddle3.0
+            {"batch_norm_", op::batch_norm},
+            {"add", op::elementwise_add},
+            {"argmax", op::argmax},
+            {"multiply", op::elementwise_mul},
+            {"reshape", op::reshape2},
+            {"divide", op::elementwise_div},
+            {"maximum", op::elementwise_max},
+            {"minimum", op::elementwise_min},
+            {"remainder", op::elementwise_mod},
+            {"subtract", op::elementwise_sub},
+            {"floor_divide", op::elementwise_floordiv},
+            {"expand", op::expand_v2},
+            {"assign_value_", op::assign_value},
+            {"expand_as", op::expand_as_v2},
+            {"full_like", op::fill_any_like},
+            {"full_with_tensor", op::fill_constant},
+            {"grid_sample", op::grid_sampler},
+            {"hardsigmoid", op::hard_sigmoid},
+            {"hardswish", op::hard_swish},
+            {"one_hot", op::one_hot_v2},
+            {"arange", op::range},
+            {"all", op::reduce_all},
+            {"max", op::reduce_max},
+            {"mean", op::reduce_mean},
+            {"min", op::reduce_min},
+            {"prod", op::reduce_prod},
+            // the type name sum already replaced by reduce_sum when decode json
+            {"reduce_sum", op::reduce_sum},
+            {"transpose", op::transpose2},
+            {"set_value_with_tensor_", op::set_value},
+            {"shape64", op::shape},
+            {"share_data_", op::share_data},
+            {"split_with_num", op::split},
+            {"squeeze", op::squeeze},
+            {"topk", op::top_k_v2},
+            {"triu", op::tril_triu},
+            {"tril", op::tril_triu},
+            {"unsqueeze", op::unsqueeze},
+            {"nonzero", op::where_index},
+            {"any", op::reduce_any},
+            {"add_n", op::sum},
+            {"if", op::if_else_block},
+            {"bicubic_interp", op::bicubic_interp_v2},
+            {"embedding", op::embedding},
+            {"generate_proposals", op::generate_proposals_v2},
+            {"linear_interp", op::linear_interp_v2},
+            {"trilinear_interp", op::trilinear_interp_v2},
+            {"array_to_tensor", op::tensor_array_to_tensor},
+            {"array_length", op::lod_array_length},
+            {"array_write_", op::write_to_array},
+    };
 };
+const std::string& get_input_name_by_op_type(const std::string& type, size_t index) {
+      static std::map<const std::string, const std::vector<std::string>> map = {
+            {"argmax", {"X", "full"}},
+            {"arg_min", {}},
+            {"assign", {"X"}},
+            {"assign_value", {}},
+            {"assign_value_", {"X"}},
+            {"batch_norm_", {"X", "Mean", "Variance", "Scale", "Bias"}},
+            {"bicubic_interp_v2", {}},
+            {"bicubic_interp", {"X", "SizeTensor"}},
+            {"bilinear_interp_v2", {}},
+            {"bilinear_interp", {"X", "SizeTensor"}},
+            {"bmm", {"X", "Y"}},
+            {"box_coder", {"PriorBox", "PriorBoxVar", "TargetBox"}},
+            {"cast", {"X"}},
+            {"ceil", {"X"}},
+            {"clip", {"X", "min", "max"}},
+            {"concat", {"X", "full"}},
+            {"conditional_block", {}},
+            {"if", {"Cond", "if_inputs", "else_inputs", "sub_block_indexs"}},
+            {"conv2d", {"Input", "Filter"}},
+            {"conv2d_transpose", {"Input", "Filter", "full"}},
+            {"cos", {}},
+            {"cumsum", {"X", "full"}},
+            {"deformable_conv", {"Input", "Offset", "Filter", "Mask"}},
+            {"deformable_conv_v1", {}},
+            {"depthwise_conv2d", {"Input", "Filter"}},
+            {"depthwise_conv2d_transpose", {"Input", "Filter", "full"}},
+            {"dequantize_linear", {}},
+            {"elementwise_add", {}},
+            {"add", {"X", "Y"}},
+            {"elementwise_div", {}},
+            {"divide", {"X", "Y"}},
+            {"elementwise_floordiv", {}},
+            {"floor_divide", {"X", "Y"}},
+            {"elementwise_mod", {}},
+            {"remainder", {"X", "Y"}},
+            {"elementwise_mul", {}},
+            {"multiply", {"X", "Y"}},
+            {"elementwise_max", {}},
+            {"maximum", {"X", "Y"}},
+            {"elementwise_min", {}},
+            {"minimum", {"X", "Y"}},
+            {"elementwise_sub", {}},
+            {"subtract", {"X", "Y"}},
+            {"dropout", {"X", "unused_input", "full"}},
+            {"elementwise_pow", {"X", "Y"}},
+            {"elu", {"X"}},
+            {"equal", {"X", "Y"}},
+            {"exp", {"X"}},
+            {"expand_v2", {}},
+            {"expand", {"X", "Shape"}},
+            {"expand_as_v2", {}},
+            {"expand_as", {"X", "Y"}},
+            {"eye", {"num_rows", "num_columns"}},
+            {"fill_any_like", {}},
+            {"full_like", {"X", "full"}},
+            {"fill_constant", {}},
+            {"full_with_tensor", {"ValueTensor", "ShapeTensor"}},
+            {"fill_constant_batch_size_like", {}},
+            {"flatten_contiguous_range", {}},
+            {"flatten", {"X"}},
+            {"flip", {"X"}},
+            {"floor", {"X"}},
+            {"gather", {"X", "Index", "Axis"}},
+            {"gather_nd", {"X", "Index"}},
+            {"gelu", {"X"}},
+            {"generate_proposals_v2", {}},
+            {"generate_proposals", {"Scores", "BboxDeltas", "ImShape", "Anchors", "Variances"}},
+            {"greater_equal", {"X", "Y"}},
+            {"greater_than", {"X", "Y"}},
+            {"grid_sample", {"X", "Grid"}},
+            {"group_norm", {"X", "Scale", "Bias"}},
+            {"hardsigmoid", {"X"}},
+            {"hardswish", {"X"}},
+            {"index_select", {"X", "Index"}},
+            {"layer_norm", {"X", "Scale", "Bias"}},
+            {"leaky_relu", {"X"}},
+            {"less_than", {"X", "Y"}},
+            {"less_equal", {"X", "Y"}},
+            {"linear_interp_v2", {}},
+            {"linear_interp", {"X", "SizeTensor"}},
+            {"linspace", {"Start", "Stop", "Num"}},
+            {"lod_array_length", {""}},
+            {"array_length", {"X"}},
+            {"log", {"X"}},
+            {"logical_and", {"X", "Y"}},
+            {"logical_not", {"X"}},
+            {"logical_or", {"X", "Y"}},
+            {"logical_xor", {"X", "Y"}},
+            {"lookup_table_v2", {}},
+            {"embedding", {"Ids", "W"}},
+            {"matmul", {"X", "Y"}},
+            {"matmul_v2", {}},
+            {"max_pool2d_with_index", {"X"}},
+            {"max_pool3d_with_index", {"X"}},
+            {"matrix_nms", {"BBoxes", "Scores"}},
+            {"memcpy", {"X"}},
+            {"meshgrid", {"X"}},
+            {"multiclass_nms3", {"BBoxes", "Scores", "RoisNum"}},
+            {"nearest_interp_v2", {}},
+            {"nearest_interp", {"X", "SizeTensor"}},
+            {"not_equal", {"X", "Y"}},
+            {"one_hot_v2", {}},
+            {"one_hot", {"X", "depth_tensor"}},
+            {"p_norm", {"X"}},
+            {"pad3d", {"X", "full"}},
+            {"partial_concat", {}},
+            {"partial_sum", {}},
+            {"pow", {"X"}},
+            {"pool2d", {"X", "ksize"}},
+            {"pool3d", {"X"}},
+            {"prior_box", {"Input", "Image"}},
+            {"quantize_linear", {}},
+            {"range", {}},
+            {"arange", {"Start", "End", "Step"}},
+            {"reduce_all", {}},
+            {"all", {"X"}},
+            {"reduce_max", {}},
+            {"max", {"X", "full"}},
+            {"reduce_mean", {}},
+            {"mean", {"X", "full"}},
+            {"reduce_min", {}},
+            {"min", {"X", "full"}},
+            {"reduce_prod", {}},
+            {"prod", {"X", "full"}},
+            // the type name sum already replaced by reduce_sum when decode json
+            {"reduce_sum", {"X", "full"}},
+            {"relu", {"X"}},
+            {"relu6", {"X"}},
+            {"reshape2", {}},
+            {"reshape", {"X", "ShapeTensor"}},
+            {"reverse", {}},
+            {"rnn", {"Input", "PreState", "WeightList", "SequenceLength", "unused"}},
+            {"roi_align", {"X", "ROIs", "ROI_NUM"}},
+            {"roll", {"X", "ShiftsTensor"}},
+            {"round", {"X"}},
+            {"rsqrt", {}},
+            {"scale", {"X", "ScaleTensor"}},
+            {"select_input", {}},
+            {"set_value", {}},
+            {"set_value_with_tensor_", {"Input", "ValueTensor", "StartsTensorList", "EndsTensorList", "StepsTensorList"}},
+            {"shape", {}},
+            {"shape64", {"Input"}},
+            {"share_data", {}},
+            {"share_data_", {"X"}},
+            {"sigmoid", {"X"}},
+            {"silu", {"X"}},
+            {"sin", {"X"}},
+            {"slice", {"Input", "StartsTensor", "EndsTensor"}},
+            {"softmax", {"X"}},
+            {"softplus", {"X"}},
+            {"softshrink", {"X"}},
+            {"split", {}},
+            {"split_with_num", {"X", "Input1", "Input2"}},
+            {"sqrt", {"X"}},
+            {"squeeze2", {}},
+            {"squeeze", {"X", "full"}},
+            {"stack", {"X"}},
+            {"strided_slice", {"Input", "StartsTensor", "EndsTensor", "StridesTensor"}},
+            {"sum", {}},
+            {"add_n", {"X"}},
+            {"swish", {"X"}},
+            {"sync_batch_norm", {}},
+            {"tanh", {"X"}},
+            {"tanh_shrink", {"X"}},
+            {"tensor_array_to_tensor", {}},
+            {"array_to_tensor", {"X"}},
+            {"tile", {"X", "RepeatTimes"}},
+            {"top_k_v2", {}},
+            {"topk", {"X", "K"}},
+            {"transpose2", {}},
+            {"transpose", {"X"}},
+            {"tril_triu", {}},
+            {"triu", {"X"}},
+            {"tril", {"X"}},
+            {"trilinear_interp_v2", {}},
+            {"trilinear_interp", {"X", "SizeTensor"}},
+            {"unsqueeze2", {}},
+            {"unsqueeze", {"X", "AxesTensor"}},
+            {"unique", {"X"}},
+            {"unstack", {"X"}},
+            {"where", {"Condition", "X", "Y"}},
+            {"while", {"Condition", "X"}},
+            {"write_to_array", {}},
+            {"array_write_", {"X", "I", "ARRAY_LEN"}},
+            {"where_index", {}},
+            {"nonzero", {"Condition"}},
+            {"yolo_box", {"X", "ImgSize"}},
+            {"abs", {"X"}},
+            {"elu", {"Out"}},
+            {"atan2", {"X1", "X2"}},
+            {"scatter", {"X", "Ids", "Updates"}},
+            {"scatter_nd_add", {"X", "Index", "Updates"}},
+            {"take_along_axis", {}},
+            {"reduce_any", {}},
+            {"any", {"X", "full"}}
+      };
+      auto it = map.find(type);
+      auto size = it->second.size();
+      const static std::set<std::string> unknow_input_num_ops = {"sum", "while"};
+      auto unknow_it = unknow_input_num_ops.find(type);
+      if (unknow_it != unknow_input_num_ops.end() && index >= size) {
+          return it->second[size - 1];
+      }
+      bool success = (it != map.end() && (it->second.size() > index));
+      FRONT_END_OP_CONVERSION_CHECK(success, "No input name found for ", type, " node.", " index:", index);
+      return it->second[index];
+}
+
+const std::vector<std::string>& get_output_name_by_op_type(const std::string& type) {
+      static std::map<const std::string, const std::vector<std::string>> map = {
+            {"argmax", {"Out"}},
+            {"arg_min", {}},
+            {"assign", {"Out"}},
+            {"assign_value", {}},
+            {"assign_value_", {"Out"}},
+            {"batch_norm_", {"Y"}},
+            {"bicubic_interp_v2", {}},
+            {"bicubic_interp", {"Out"}},
+            {"bilinear_interp_v2", {}},
+            {"bilinear_interp", {"Out"}},
+            {"bmm", {"Out"}},
+            {"box_coder", {"OutputBox"}},
+            {"cast", {"Out"}},
+            {"ceil", {"Out"}},
+            {"clip", {"Out"}},
+            {"concat", {"Out"}},
+            {"conditional_block", {}},
+            {"if", {"Out"}},
+            {"conv2d", {"Output"}},
+            {"conv2d_transpose", {"Output"}},
+            {"cos", {}},
+            {"cumsum", {"Out"}},
+            {"deformable_conv", {"Output"}},
+            {"deformable_conv_v1", {}},
+            {"depthwise_conv2d", {"Output"}},
+            {"depthwise_conv2d_transpose", {"Output"}},
+            {"dequantize_linear", {}},
+            {"elementwise_add", {}},
+            {"add", {"Out"}},
+            {"elementwise_div", {}},
+            {"divide", {"Out"}},
+            {"elementwise_floordiv", {}},
+            {"floor_divide", {"Out"}},
+            {"elementwise_mod", {}},
+            {"remainder", {"Out"}},
+            {"elementwise_mul", {}},
+            {"multiply", {"Out"}},
+            {"elementwise_max", {}},
+            {"maximum", {"Out"}},
+            {"elementwise_min", {}},
+            {"minimum", {"Out"}},
+            {"elementwise_sub", {}},
+            {"subtract", {"Out"}},
+            {"dropout", {"Out"}},
+            {"elementwise_pow", {"Out"}},
+            {"equal", {"Out"}},
+            {"exp", {"Out"}},
+            {"expand_v2", {}},
+            {"expand", {"Out"}},
+            {"expand_as_v2", {}},
+            {"expand_as", {"Out"}},
+            {"eye", {"Out"}},
+            {"fill_any_like", {}},
+            {"full_like", {"Out"}},
+            {"fill_constant", {}},
+            {"full_with_tensor", {"Out"}},
+            {"fill_constant_batch_size_like", {}},
+            {"flatten_contiguous_range", {}},
+            {"flatten", {"Out"}},
+            {"flip", {"Out"}},
+            {"floor", {"Out"}},
+            {"gather", {"Out"}},
+            {"gather_nd", {"Out"}},
+            {"gelu", {"Out"}},
+            {"generate_proposals_v2", {}},
+            {"generate_proposals", {"RpnRois", "RpnRoiProbs", "RpnRoisNum"}},
+            {"greater_equal", {"Out"}},
+            {"greater_than", {"Out"}},
+            {"grid_sample", {"Output"}},
+            {"group_norm", {"Y"}},
+            {"hardsigmoid", {"Out"}},
+            {"hardswish", {"Out"}},
+            {"index_select", {"Out"}},
+            {"layer_norm", {"Y"}},
+            {"leaky_relu", {"Out"}},
+            {"less_than", {"Out"}},
+            {"less_equal", {"Out"}},
+            {"linear_interp_v2", {}},
+            {"linear_interp", {"Out"}},
+            {"linspace", {"Out"}},
+            {"lod_array_length", {""}},
+            {"array_length", {"Out"}},
+            {"log", {"Out"}},
+            {"logical_and", {"Out"}},
+            {"logical_not", {"Out"}},
+            {"logical_or", {"Out"}},
+            {"logical_xor", {"Out"}},
+            {"lookup_table_v2", {}},
+            {"embedding", {"Out"}},
+            {"matmul", {"Out"}},
+            {"matmul_v2", {}},
+            {"max_pool2d_with_index", {"Out"}},
+            {"max_pool3d_with_index", {"Out", "Mask"}},
+            {"matrix_nms", {"Out", "Index", "RoisNum"}},
+            {"memcpy", {"Out"}},
+            {"meshgrid", {"Out"}},
+            {"multiclass_nms3", {"Out", "Index", "NmsRoisNum"}},
+            {"nearest_interp_v2", {}},
+            {"nearest_interp", {"Out"}},
+            {"not_equal", {"Out"}},
+            {"one_hot_v2", {}},
+            {"one_hot", {"Out"}},
+            {"p_norm", {"Out"}},
+            {"pad3d", {"Out"}},
+            {"partial_concat", {}},
+            {"partial_sum", {}},
+            {"pow", {"Out"}},
+            {"pool2d", {"Out"}},
+            {"pool3d", {"Out"}},
+            {"prior_box", {"Boxes", "Variances"}},
+            {"quantize_linear", {}},
+            {"range", {}},
+            {"arange", {"Out"}},
+            {"reduce_all", {}},
+            {"all", {"Out"}},
+            {"reduce_max", {}},
+            {"max", {"Out"}},
+            {"reduce_mean", {}},
+            {"mean", {"Out"}},
+            {"reduce_min", {}},
+            {"min", {"Out"}},
+            {"reduce_prod", {}},
+            {"prod", {"Out"}},
+            // the type name sum already replaced by reduce_sum when decode json
+            {"reduce_sum", {"Out"}},
+            {"relu", {"Out"}},
+            {"relu6", {"Out"}},
+            {"reshape2", {}},
+            {"reshape", {"Out"}},
+            {"reverse", {}},
+            {"rnn", {"Out", "State"}},
+            {"roi_align", {"Out"}},
+            {"roll", {"Out"}},
+            {"round", {"Out"}},
+            {"rsqrt", {}},
+            {"scale", {"Out"}},
+            {"select_input", {}},
+            {"set_value", {}},
+            {"set_value_with_tensor_", {"Out"}},
+            {"shape", {}},
+            {"shape64", {"Out"}},
+            {"share_data", {}},
+            {"share_data_", {"Out"}},
+            {"sigmoid", {"Out"}},
+            {"silu", {"Out"}},
+            {"sin", {"Out"}},
+            {"slice", {"Out"}},
+            {"softmax", {"Out"}},
+            {"softplus", {"Out"}},
+            {"softshrink", {"Out"}},
+            {"split", {}},
+            {"split_with_num", {"Out"}},
+            {"sqrt", {"Out"}},
+            {"squeeze2", {}},
+            {"squeeze", {"Out"}},
+            {"stack", {"Y"}},
+            {"strided_slice", {"Out"}},
+            {"sum", {}},
+            {"add_n", {"Out"}},
+            {"swish", {"Out"}},
+            {"sync_batch_norm", {}},
+            {"tanh", {"Out"}},
+            {"tanh_shrink", {"Out"}},
+            {"tensor_array_to_tensor", {}},
+            {"array_to_tensor", {"Out"}},
+            {"tile", {"Out"}},
+            {"top_k_v2", {}},
+            {"topk", {"Out", "Indices"}},
+            {"transpose2", {}},
+            {"transpose", {"Out"}},
+            {"tril_triu", {}},
+            {"triu", {"Out"}},
+            {"tril", {"Out"}},
+            {"trilinear_interp_v2", {}},
+            {"trilinear_interp", {"Out"}},
+            {"unsqueeze2", {}},
+            {"unsqueeze", {"Out"}},
+            {"unique", {"Out", "Index", "Inverse", "Counts"}},
+            {"unstack", {"Y"}},
+            {"where", {"Out"}},
+            {"while", {"Out"}},
+            {"write_to_array", {}},
+            {"array_write_", {"Out"}},
+            {"where_index", {}},
+            {"nonzero", {"Out"}},
+            {"yolo_box", {"Boxes", "Scores"}},
+            {"abs", {"Out"}},
+            {"elu", {"Out"}},
+            {"atan2", {"Out"}},
+            {"scatter", {"Out"}},
+            {"scatter_nd_add", {"Out"}},
+            {"take_along_axis", {}},
+            {"reduce_any", {}},
+            {"any", {"Out"}}
+      };
+      auto it = map.find(type);
+      bool success = (it != map.end()) && (it->second.size() > 0);
+      FRONT_END_OP_CONVERSION_CHECK(success, "No output name found for ", type, " node.");
+      return it->second;
+}
 
 }  // namespace paddle
 }  // namespace frontend
diff --git a/src/frontends/paddle/src/op_table.hpp b/src/frontends/paddle/src/op_table.hpp
index 3e01fb686b..af71306b31 100644
--- a/src/frontends/paddle/src/op_table.hpp
+++ b/src/frontends/paddle/src/op_table.hpp
@@ -16,7 +16,8 @@ namespace paddle {
 using CreatorFunction = std::function<NamedOutputs(const NodeContext&)>;
 
 std::map<std::string, CreatorFunction> get_supported_ops();
-
+const std::string& get_input_name_by_op_type(const std::string& type, size_t index);
+const std::vector<std::string>& get_output_name_by_op_type(const std::string& type);
 }  // namespace paddle
 }  // namespace frontend
 }  // namespace ov
diff --git a/src/frontends/paddle/src/paddle_fw_node.hpp b/src/frontends/paddle/src/paddle_fw_node.hpp
index 14b5604d55..4dd89a6731 100644
--- a/src/frontends/paddle/src/paddle_fw_node.hpp
+++ b/src/frontends/paddle/src/paddle_fw_node.hpp
@@ -14,7 +14,7 @@ class FrameworkNode : public ov::op::util::FrameworkNode {
 public:
     OPENVINO_OP("FrameworkNode", "util", ov::op::util::FrameworkNode);
 
-    FrameworkNode(const std::shared_ptr<DecoderProto>& decoder,
+    FrameworkNode(const std::shared_ptr<paddle::DecoderBase>& decoder,
                   const OutputVector& inputs,
                   const std::vector<std::string>& inputs_names)
         : ov::op::util::FrameworkNode(inputs, decoder->get_output_size()),
@@ -23,7 +23,8 @@ public:
         ov::op::util::FrameworkNodeAttrs attrs;
         attrs.set_type_name(m_decoder->get_op_type());
         set_attrs(attrs);
-
+        auto json_decoder = std::dynamic_pointer_cast<paddle::DecoderBase>(decoder);
+        m_is_json_decoder = (json_decoder != nullptr);
         validate_and_infer_types();
     }
 
@@ -37,17 +38,22 @@ public:
         return m_decoder->get_op_type();
     }
 
-    const std::shared_ptr<DecoderProto> get_decoder() const {
+    const std::shared_ptr<DecoderBase> get_decoder() const {
         return m_decoder;
     }
 
+    bool is_json_decoder() const {
+        return m_is_json_decoder;
+    }
+
     std::map<std::string, OutputVector> get_named_inputs() const;
 
     std::map<std::string, OutputVector> return_named_outputs();
 
 private:
-    const std::shared_ptr<DecoderProto> m_decoder;
+    const std::shared_ptr<DecoderBase> m_decoder;
     std::vector<std::string> m_inputs_names;
+    bool m_is_json_decoder;
 };
 }  // namespace paddle
 }  // namespace frontend
diff --git a/src/frontends/paddle/src/place.cpp b/src/frontends/paddle/src/place.cpp
index 66b7f7d5d7..52edf090ac 100644
--- a/src/frontends/paddle/src/place.cpp
+++ b/src/frontends/paddle/src/place.cpp
@@ -28,89 +28,106 @@ bool Place::is_output() const {
     return std::find_if(model_outs.begin(), model_outs.end(), cmp) != model_outs.end();
 }
 
-OpPlace::OpPlace(const ov::frontend::InputModel& input_model,
+BaseOpPlace::BaseOpPlace(const ov::frontend::InputModel& input_model, const std::vector<std::string>& names)
+    : Place(input_model, names){}
+
+ProtoOpPlace::ProtoOpPlace(const ov::frontend::InputModel& input_model,
                  const ::paddle::framework::proto::OpDesc& op_desc,
                  const std::vector<std::string>& names)
-    : Place(input_model, names),
+    : BaseOpPlace(input_model, names),
       m_op_desc(op_desc) {}
 
-OpPlace::OpPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::OpDesc& op_desc)
-    : OpPlace(input_model, op_desc, {}) {}
+ProtoOpPlace::ProtoOpPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::OpDesc& op_desc)
+    : ProtoOpPlace(input_model, op_desc, {}) {}
+
+JsonOpPlace::JsonOpPlace(const ov::frontend::InputModel& input_model,
+                 const json::OP& op,
+                 const std::vector<std::string>& names)
+    : BaseOpPlace(input_model, names),
+      m_op(op) {}
 
-const std::map<std::string, std::vector<std::shared_ptr<OutPortPlace>>>& OpPlace::get_output_ports() const {
+JsonOpPlace::JsonOpPlace(const ov::frontend::InputModel& input_model, const json::OP& op)
+    : BaseOpPlace(input_model, {}),
+      m_op(op) {}
+
+const std::map<std::string, std::vector<std::shared_ptr<OutPortPlace>>>& BaseOpPlace::get_output_ports() const {
     return m_output_ports;
 }
 
-const std::map<std::string, std::vector<std::shared_ptr<InPortPlace>>>& OpPlace::get_input_ports() const {
+const std::map<std::string, std::vector<std::shared_ptr<InPortPlace>>>& BaseOpPlace::get_input_ports() const {
     return m_input_ports;
 }
 
-std::shared_ptr<OutPortPlace> OpPlace::get_output_port_paddle(const std::string& outputName,
+std::shared_ptr<OutPortPlace> BaseOpPlace::get_output_port_paddle(const std::string& outputName,
                                                               int outputPortIndex) const {
     FRONT_END_GENERAL_CHECK((size_t)outputPortIndex <= m_output_ports.at(outputName).size(),
                             "outputPortIndex is out of bounds.");
     return m_output_ports.at(outputName)[outputPortIndex];
 }
 
-std::shared_ptr<InPortPlace> OpPlace::get_input_port_paddle(const std::string& inputName, int inputPortIndex) const {
+std::shared_ptr<InPortPlace> BaseOpPlace::get_input_port_paddle(const std::string& inputName, int inputPortIndex) const {
     FRONT_END_GENERAL_CHECK((size_t)inputPortIndex <= m_input_ports.at(inputName).size(),
                             "inputPortIndex is out of bounds.");
     return m_input_ports.at(inputName)[inputPortIndex];
 }
 
-const ::paddle::framework::proto::OpDesc& OpPlace::get_desc() const {
+const ::paddle::framework::proto::OpDesc& ProtoOpPlace::get_desc() const {
     return m_op_desc;
 }
 
-const std::shared_ptr<DecoderBase> OpPlace::get_decoder() const {
+const json::OP& JsonOpPlace::get_op() const {
+    return m_op;
+}
+
+const std::shared_ptr<DecoderBase> BaseOpPlace::get_decoder() const {
     return m_op_decoder;
 }
 
-void OpPlace::set_decoder(const std::shared_ptr<DecoderBase> op_decoder) {
+void BaseOpPlace::set_decoder(const std::shared_ptr<DecoderBase> op_decoder) {
     m_op_decoder = op_decoder;
 }
 
-void OpPlace::add_out_port(const std::shared_ptr<OutPortPlace>& output, const std::string& name) {
+void BaseOpPlace::add_out_port(const std::shared_ptr<OutPortPlace>& output, const std::string& name) {
     m_output_ports[name].push_back(output);
 }
 
-void OpPlace::add_in_port(const std::shared_ptr<InPortPlace>& input, const std::string& name) {
+void BaseOpPlace::add_in_port(const std::shared_ptr<InPortPlace>& input, const std::string& name) {
     m_input_ports[name].push_back(input);
 }
 
-Place::Ptr OpPlace::get_output_port(const std::string& name) const {
+Place::Ptr BaseOpPlace::get_output_port(const std::string& name) const {
     FRONT_END_GENERAL_CHECK(m_output_ports.at(name).size() == 1, "Only one output port should exist.");
     return m_output_ports.at(name)[0];
 }
 
-Place::Ptr OpPlace::get_input_port(const std::string& name) const {
+Place::Ptr BaseOpPlace::get_input_port(const std::string& name) const {
     FRONT_END_GENERAL_CHECK(m_input_ports.at(name).size() == 1, "Only one input port should exist.");
     return m_input_ports.at(name)[0];
 }
 
-Place::Ptr OpPlace::get_input_port(int outputPortIndex) const {
+Place::Ptr BaseOpPlace::get_input_port(int outputPortIndex) const {
     FRONT_END_GENERAL_CHECK(m_input_ports.size() == 1, "Only one named input port should exist.");
     return m_input_ports.begin()->second[outputPortIndex];
 }
 
-Place::Ptr OpPlace::get_output_port(int outputPortIndex) const {
+Place::Ptr BaseOpPlace::get_output_port(int outputPortIndex) const {
     FRONT_END_GENERAL_CHECK(m_output_ports.size() == 1, "Only one named output port should exist.");
     return m_output_ports.begin()->second[outputPortIndex];
 }
 
-Place::Ptr OpPlace::get_output_port() const {
+Place::Ptr BaseOpPlace::get_output_port() const {
     FRONT_END_GENERAL_CHECK(m_output_ports.size() == 1 && m_output_ports.begin()->second.size() == 1,
                             "Only one output port should exist.");
     return m_output_ports.begin()->second[0];
 }
 
-Place::Ptr OpPlace::get_input_port() const {
+Place::Ptr BaseOpPlace::get_input_port() const {
     FRONT_END_GENERAL_CHECK(m_input_ports.size() == 1 && m_input_ports.begin()->second.size() == 1,
                             "Only one input port should exist.");
     return m_input_ports.begin()->second[0];
 }
 
-std::vector<Place::Ptr> OpPlace::get_consuming_operations() const {
+std::vector<Place::Ptr> BaseOpPlace::get_consuming_operations() const {
     std::vector<Place::Ptr> consuming_ops;
     for (const auto& out_port : m_output_ports) {
         for (const auto& out_port_place : out_port.second) {
@@ -121,20 +138,20 @@ std::vector<Place::Ptr> OpPlace::get_consuming_operations() const {
     return consuming_ops;
 }
 
-std::vector<Place::Ptr> OpPlace::get_consuming_operations(const std::string& outputPortName,
+std::vector<Place::Ptr> BaseOpPlace::get_consuming_operations(const std::string& outputPortName,
                                                           int outputPortIndex) const {
     return get_output_port(outputPortName, outputPortIndex)->get_consuming_operations();
 }
 
-std::vector<Place::Ptr> OpPlace::get_consuming_operations(int outputPortIndex) const {
+std::vector<Place::Ptr> BaseOpPlace::get_consuming_operations(int outputPortIndex) const {
     return get_output_port(outputPortIndex)->get_consuming_operations();
 }
 
-std::vector<Place::Ptr> OpPlace::get_consuming_operations(const std::string& outputPortName) const {
+std::vector<Place::Ptr> BaseOpPlace::get_consuming_operations(const std::string& outputPortName) const {
     return get_output_port(outputPortName)->get_consuming_operations();
 }
 
-std::vector<Place::Ptr> OpPlace::get_consuming_ports() const {
+std::vector<Place::Ptr> BaseOpPlace::get_consuming_ports() const {
     std::vector<Place::Ptr> consuming_ports;
     for (const auto& out_port : m_output_ports) {
         for (const auto& out_port_place : out_port.second) {
@@ -145,70 +162,74 @@ std::vector<Place::Ptr> OpPlace::get_consuming_ports() const {
     return consuming_ports;
 }
 
-Place::Ptr OpPlace::get_output_port(const std::string& outputName, int outputPortIndex) const {
+Place::Ptr BaseOpPlace::get_output_port(const std::string& outputName, int outputPortIndex) const {
     FRONT_END_GENERAL_CHECK((size_t)outputPortIndex <= m_output_ports.at(outputName).size(),
                             "outputPortIndex is Out of bounds.");
     return m_output_ports.at(outputName)[outputPortIndex];
 }
 
-Place::Ptr OpPlace::get_input_port(const std::string& inputName, int inputPortIndex) const {
+Place::Ptr BaseOpPlace::get_input_port(const std::string& inputName, int inputPortIndex) const {
     FRONT_END_GENERAL_CHECK((size_t)inputPortIndex <= m_input_ports.at(inputName).size(),
                             "inputPortIndex is out of bounds.");
     return m_input_ports.at(inputName)[inputPortIndex];
 }
 
-Place::Ptr OpPlace::get_source_tensor() const {
+Place::Ptr BaseOpPlace::get_source_tensor() const {
     return get_input_port()->get_source_tensor();
 }
 
-Place::Ptr OpPlace::get_source_tensor(const std::string& inputName) const {
+Place::Ptr BaseOpPlace::get_source_tensor(const std::string& inputName) const {
     return get_input_port(inputName)->get_source_tensor();
 }
 
-Place::Ptr OpPlace::get_source_tensor(int inputPortIndex) const {
+Place::Ptr BaseOpPlace::get_source_tensor(int inputPortIndex) const {
     return get_input_port(inputPortIndex)->get_source_tensor();
 }
 
-Place::Ptr OpPlace::get_source_tensor(const std::string& inputName, int inputPortIndex) const {
+Place::Ptr BaseOpPlace::get_source_tensor(const std::string& inputName, int inputPortIndex) const {
     return get_input_port(inputName, inputPortIndex)->get_source_tensor();
 }
 
-Place::Ptr OpPlace::get_target_tensor() const {
+Place::Ptr BaseOpPlace::get_target_tensor() const {
     return get_output_port()->get_target_tensor();
 }
 
-Place::Ptr OpPlace::get_target_tensor(const std::string& outputName) const {
+Place::Ptr BaseOpPlace::get_target_tensor(const std::string& outputName) const {
     return get_output_port(outputName)->get_target_tensor();
 }
 
-Place::Ptr OpPlace::get_target_tensor(const std::string& outputName, int outputPortIndex) const {
+Place::Ptr BaseOpPlace::get_target_tensor(const std::string& outputName, int outputPortIndex) const {
     return get_output_port(outputName, outputPortIndex)->get_target_tensor();
 }
 
-Place::Ptr OpPlace::get_producing_operation(const std::string& inputName) const {
+Place::Ptr BaseOpPlace::get_producing_operation(const std::string& inputName) const {
     return get_input_port(inputName)->get_producing_operation();
 }
 
-Place::Ptr OpPlace::get_producing_operation(const std::string& inputName, int inputPortIndex) const {
+Place::Ptr BaseOpPlace::get_producing_operation(const std::string& inputName, int inputPortIndex) const {
     return get_input_port(inputName, inputPortIndex)->get_producing_operation();
 }
 
-Place::Ptr OpPlace::get_producing_operation() const {
+Place::Ptr BaseOpPlace::get_producing_operation() const {
     return get_input_port()->get_producing_operation();
 }
 
-Place::Ptr OpPlace::get_producing_operation(int inputPortIndex) const {
+Place::Ptr BaseOpPlace::get_producing_operation(int inputPortIndex) const {
     return get_input_port(inputPortIndex)->get_producing_operation();
 }
 
-Place::Ptr OpPlace::get_target_tensor(int outputPortIndex) const {
+Place::Ptr BaseOpPlace::get_target_tensor(int outputPortIndex) const {
     return get_output_port(outputPortIndex)->get_target_tensor();
 }
 
-TensorPlace::TensorPlace(const ov::frontend::InputModel& input_model,
+BaseTensorPlace::BaseTensorPlace(const ov::frontend::InputModel& input_model,
+                         const std::vector<std::string>& names)
+    : Place(input_model, names){}
+
+ProtoTensorPlace::ProtoTensorPlace(const ov::frontend::InputModel& input_model,
                          const std::vector<std::string>& names,
                          const ::paddle::framework::proto::VarDesc& var_desc)
-    : Place(input_model, names),
+    : BaseTensorPlace(input_model, names),
       m_var_desc(var_desc) {
     const auto& var_type = var_desc.type();
     if (var_type.type() == ::paddle::framework::proto::VarType::LOD_TENSOR) {
@@ -218,11 +239,28 @@ TensorPlace::TensorPlace(const ov::frontend::InputModel& input_model,
     }
 }
 
-TensorPlace::TensorPlace(const ov::frontend::InputModel& input_model,
+ProtoTensorPlace::ProtoTensorPlace(const ov::frontend::InputModel& input_model,
                          const ::paddle::framework::proto::VarDesc& var_desc)
-    : TensorPlace(input_model, {var_desc.name()}, var_desc) {}
+    : ProtoTensorPlace(input_model, {var_desc.name()}, var_desc) {}
 
-std::vector<Place::Ptr> TensorPlace::get_consuming_ports() const {
+JsonTensorPlace::JsonTensorPlace(const ov::frontend::InputModel& input_model,
+                         const std::vector<std::string>& names,
+                         const json::Port& port)
+    : BaseTensorPlace(input_model, names),
+      m_port(port) {
+    // const auto& var_type = var_desc.type();
+    // if (var_type.type() == ::paddle::framework::proto::VarType::LOD_TENSOR) {
+    //     const auto& tensor_desc = var_type.lod_tensor().tensor();
+    //     m_type = get_ov_type(tensor_desc.data_type());
+    //     m_pshape = PartialShape(std::vector<Dimension>(tensor_desc.dims().begin(), tensor_desc.dims().end()));
+    // }
+}
+
+JsonTensorPlace::JsonTensorPlace(const ov::frontend::InputModel& input_model,
+        const json::Port& port)
+    : JsonTensorPlace(input_model, {}, port) {}
+
+std::vector<Place::Ptr> BaseTensorPlace::get_consuming_ports() const {
     std::vector<Place::Ptr> consuming_ports;
     for (const auto& consuming_port : m_consuming_ports) {
         if (const auto& locked = consuming_port.lock()) {
@@ -234,7 +272,7 @@ std::vector<Place::Ptr> TensorPlace::get_consuming_ports() const {
     return consuming_ports;
 }
 
-Place::Ptr TensorPlace::get_producing_port() const {
+Place::Ptr BaseTensorPlace::get_producing_port() const {
     FRONT_END_GENERAL_CHECK(m_producing_ports.size() == 1, "Only one producing port is supported.");
     if (const auto& producing_port = m_producing_ports[0].lock()) {
         return producing_port;
@@ -242,19 +280,23 @@ Place::Ptr TensorPlace::get_producing_port() const {
     FRONT_END_THROW("Producing Port has expired.");
 }
 
-void TensorPlace::add_producing_port(const std::shared_ptr<OutPortPlace>& out_port) {
+void BaseTensorPlace::add_producing_port(const std::shared_ptr<OutPortPlace>& out_port) {
     m_producing_ports.push_back(out_port);
 }
 
-void TensorPlace::add_consuming_port(const std::shared_ptr<InPortPlace>& in_port) {
+void BaseTensorPlace::add_consuming_port(const std::shared_ptr<InPortPlace>& in_port) {
     m_consuming_ports.push_back(in_port);
 }
 
-const ::paddle::framework::proto::VarDesc& TensorPlace::get_desc() const {
+const ::paddle::framework::proto::VarDesc& ProtoTensorPlace::get_desc() const {
     return m_var_desc;
 }
 
-std::vector<Place::Ptr> TensorPlace::get_consuming_operations() const {
+const json::Port& JsonTensorPlace::get_port() const {
+    return m_port;
+}
+
+std::vector<Place::Ptr> BaseTensorPlace::get_consuming_operations() const {
     std::vector<Place::Ptr> consuming_ops;
     for (const auto& consuming_port : m_consuming_ports) {
         if (auto port_ptr = consuming_port.lock()) {
@@ -267,7 +309,7 @@ std::vector<Place::Ptr> TensorPlace::get_consuming_operations() const {
     return consuming_ops;
 }
 
-bool TensorPlace::is_equal_data(const Place::Ptr& another) const {
+bool BaseTensorPlace::is_equal_data(const Place::Ptr& another) const {
     auto consuming_ports = get_consuming_ports();
     bool eq_to_consuming_port =
         std::any_of(consuming_ports.begin(), consuming_ports.end(), [&another](const Ptr& place) {
@@ -276,25 +318,25 @@ bool TensorPlace::is_equal_data(const Place::Ptr& another) const {
     return is_equal(another) || get_producing_port()->is_equal(another) || eq_to_consuming_port;
 }
 
-Place::Ptr TensorPlace::get_producing_operation() const {
+Place::Ptr BaseTensorPlace::get_producing_operation() const {
     return get_producing_port()->get_producing_operation();
 }
 
-std::shared_ptr<TensorPlace> InPortPlace::get_source_tensor_paddle() const {
+std::shared_ptr<BaseTensorPlace> InPortPlace::get_source_tensor_paddle() const {
     if (const auto& tensor = m_source_tensor.lock()) {
         return tensor;
     }
     FRONT_END_THROW("Source Tensor has expired.");
 }
 
-std::shared_ptr<OpPlace> InPortPlace::get_op() {
+std::shared_ptr<BaseOpPlace> InPortPlace::get_op() {
     if (const auto& op = m_op.lock()) {
         return op;
     }
     FRONT_END_THROW("Operation has expired.");
 }
 
-void InPortPlace::set_source_tensor(const std::weak_ptr<TensorPlace>& source_tensor) {
+void InPortPlace::set_source_tensor(const std::weak_ptr<BaseTensorPlace>& source_tensor) {
     m_source_tensor = source_tensor;
 }
 
@@ -324,7 +366,7 @@ Place::Ptr InPortPlace::get_producing_operation() const {
     return get_producing_port()->get_producing_operation();
 }
 
-std::shared_ptr<TensorPlace> OutPortPlace::get_target_tensor_paddle() const {
+std::shared_ptr<BaseTensorPlace> OutPortPlace::get_target_tensor_paddle() const {
     if (const auto& target_tensor = m_target_tensor.lock()) {
         return target_tensor;
     }
@@ -338,7 +380,7 @@ std::vector<Place::Ptr> OutPortPlace::get_consuming_operations() const {
     FRONT_END_THROW("Tensor has expired.");
 }
 
-void OutPortPlace::set_target_tensor(const std::weak_ptr<TensorPlace>& target_tensor) {
+void OutPortPlace::set_target_tensor(const std::weak_ptr<BaseTensorPlace>& target_tensor) {
     m_target_tensor = target_tensor;
 }
 
diff --git a/src/frontends/paddle/src/place.hpp b/src/frontends/paddle/src/place.hpp
index 2b66328512..243fc8b68b 100644
--- a/src/frontends/paddle/src/place.hpp
+++ b/src/frontends/paddle/src/place.hpp
@@ -6,6 +6,7 @@
 
 #include "input_model.hpp"
 #include "openvino/frontend/manager.hpp"
+#include "json_data.hpp"
 
 namespace paddle {
 namespace framework {
@@ -21,8 +22,8 @@ namespace ov {
 namespace frontend {
 namespace paddle {
 
-class TensorPlace;
-class OpPlace;
+class BaseTensorPlace;
+class BaseOpPlace;
 
 class Place : public ov::frontend::Place {
 public:
@@ -57,14 +58,14 @@ class InPortPlace : public Place {
 public:
     explicit InPortPlace(const ov::frontend::InputModel& input_model) : Place(input_model) {}
 
-    void set_op(const std::weak_ptr<OpPlace>& op) {
+    void set_op(const std::weak_ptr<BaseOpPlace>& op) {
         m_op = op;
     }
-    void set_source_tensor(const std::weak_ptr<TensorPlace>& source_tensor);
+    void set_source_tensor(const std::weak_ptr<BaseTensorPlace>& source_tensor);
 
     // Internal usage
-    std::shared_ptr<TensorPlace> get_source_tensor_paddle() const;
-    std::shared_ptr<OpPlace> get_op();
+    std::shared_ptr<BaseTensorPlace> get_source_tensor_paddle() const;
+    std::shared_ptr<BaseOpPlace> get_op();
 
     // External usage
     std::vector<Ptr> get_consuming_operations() const override;
@@ -75,20 +76,20 @@ public:
     bool is_equal_data(const Ptr& another) const override;
 
 private:
-    std::weak_ptr<TensorPlace> m_source_tensor;
-    std::weak_ptr<OpPlace> m_op;
+    std::weak_ptr<BaseTensorPlace> m_source_tensor;
+    std::weak_ptr<BaseOpPlace> m_op;
 };
 
 class OutPortPlace : public Place {
 public:
     explicit OutPortPlace(const ov::frontend::InputModel& input_model) : Place(input_model) {}
 
-    void set_op(const std::weak_ptr<OpPlace>& op) {
+    void set_op(const std::weak_ptr<BaseOpPlace>& op) {
         m_op = op;
     }
-    void set_target_tensor(const std::weak_ptr<TensorPlace>& target_tensor);
+    void set_target_tensor(const std::weak_ptr<BaseTensorPlace>& target_tensor);
 
-    std::shared_ptr<TensorPlace> get_target_tensor_paddle() const;
+    std::shared_ptr<BaseTensorPlace> get_target_tensor_paddle() const;
 
     // External usage
     std::vector<Ptr> get_consuming_operations() const override;
@@ -98,17 +99,13 @@ public:
     bool is_equal_data(const Ptr& another) const override;
 
 private:
-    std::weak_ptr<OpPlace> m_op;
-    std::weak_ptr<TensorPlace> m_target_tensor;
+    std::weak_ptr<BaseOpPlace> m_op;
+    std::weak_ptr<BaseTensorPlace> m_target_tensor;
 };
 
-class OpPlace : public Place {
+class BaseOpPlace : public Place {
 public:
-    OpPlace(const ov::frontend::InputModel& input_model,
-            const ::paddle::framework::proto::OpDesc& op_desc,
-            const std::vector<std::string>& names);
-
-    OpPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::OpDesc& op_desc);
+    BaseOpPlace(const ov::frontend::InputModel& input_model, const std::vector<std::string>& names);
 
     void add_in_port(const std::shared_ptr<InPortPlace>& input, const std::string& name);
     void add_out_port(const std::shared_ptr<OutPortPlace>& output, const std::string& name);
@@ -118,7 +115,6 @@ public:
     const std::map<std::string, std::vector<std::shared_ptr<InPortPlace>>>& get_input_ports() const;
     std::shared_ptr<OutPortPlace> get_output_port_paddle(const std::string& outputName, int outputPortIndex) const;
     std::shared_ptr<InPortPlace> get_input_port_paddle(const std::string& inputName, int inputPortIndex) const;
-    const ::paddle::framework::proto::OpDesc& get_desc() const;
     const std::shared_ptr<DecoderBase> get_decoder() const;
     void set_decoder(const std::shared_ptr<DecoderBase> op_decoder);
 
@@ -155,20 +151,39 @@ public:
     Ptr get_target_tensor(const std::string& outputName) const override;
     Ptr get_target_tensor(const std::string& outputName, int outputPortIndex) const override;
 
-private:
-    const ::paddle::framework::proto::OpDesc& m_op_desc;  // TODO: to conceal it behind decoder.
+protected:
     std::shared_ptr<DecoderBase> m_op_decoder;
     std::map<std::string, std::vector<std::shared_ptr<InPortPlace>>> m_input_ports;
     std::map<std::string, std::vector<std::shared_ptr<OutPortPlace>>> m_output_ports;
 };
 
-class TensorPlace : public Place {
+class ProtoOpPlace : public BaseOpPlace {
+public:
+    ProtoOpPlace(const ov::frontend::InputModel& input_model,
+            const ::paddle::framework::proto::OpDesc& op_desc,
+            const std::vector<std::string>& names);
+    ProtoOpPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::OpDesc& op_desc);
+    const ::paddle::framework::proto::OpDesc& get_desc() const;
+private:
+    const ::paddle::framework::proto::OpDesc& m_op_desc;  // TODO: to conceal it behind decoder.
+};
+
+class JsonOpPlace : public BaseOpPlace {
 public:
-    TensorPlace(const ov::frontend::InputModel& input_model,
-                const std::vector<std::string>& names,
-                const ::paddle::framework::proto::VarDesc& var_desc);
+    JsonOpPlace(const ov::frontend::InputModel& input_model,
+            const json::OP& op_desc,
+            const std::vector<std::string>& names);
+
+    JsonOpPlace(const ov::frontend::InputModel& input_model, const json::OP& op_desc);
+    const json::OP& get_op() const;
+private:
+    const json::OP& m_op;
+};
 
-    TensorPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::VarDesc& var_desc);
+class BaseTensorPlace : public Place {
+public:
+    BaseTensorPlace(const ov::frontend::InputModel& input_model,
+                const std::vector<std::string>& names);
 
     void add_producing_port(const std::shared_ptr<OutPortPlace>& out_port);
     void add_consuming_port(const std::shared_ptr<InPortPlace>& in_port);
@@ -186,7 +201,6 @@ public:
     void set_element_type(const element::Type& type) {
         m_type = type;
     }
-    const ::paddle::framework::proto::VarDesc& get_desc() const;
 
     // External usage
     Ptr get_producing_operation() const override;
@@ -195,8 +209,7 @@ public:
     Ptr get_producing_port() const override;
     bool is_equal_data(const Ptr& another) const override;
 
-private:
-    const ::paddle::framework::proto::VarDesc& m_var_desc;
+protected:
     PartialShape m_pshape;
     element::Type m_type;
 
@@ -204,6 +217,28 @@ private:
     std::vector<std::weak_ptr<InPortPlace>> m_consuming_ports;
 };
 
+class ProtoTensorPlace : public BaseTensorPlace {
+public:
+    ProtoTensorPlace(const ov::frontend::InputModel& input_model,
+            const std::vector<std::string>& names,
+            const ::paddle::framework::proto::VarDesc& var_desc);
+    ProtoTensorPlace(const ov::frontend::InputModel& input_model, const ::paddle::framework::proto::VarDesc& var_desc);
+    const ::paddle::framework::proto::VarDesc& get_desc() const;
+private:
+    const ::paddle::framework::proto::VarDesc& m_var_desc;
+};
+
+class JsonTensorPlace : public BaseTensorPlace {
+public:
+    JsonTensorPlace(const ov::frontend::InputModel& input_model,
+            const std::vector<std::string>& names,
+            const json::Port& port);
+    JsonTensorPlace(const ov::frontend::InputModel& input_model, const json::Port& port);
+    const json::Port& get_port() const;
+private:
+    const json::Port& m_port;
+};
+
 }  // namespace paddle
 }  // namespace frontend
 }  // namespace ov
diff --git a/src/frontends/paddle/src/proto_input_model_imp.cpp b/src/frontends/paddle/src/proto_input_model_imp.cpp
new file mode 100644
index 0000000000..4db2fc5a31
--- /dev/null
+++ b/src/frontends/paddle/src/proto_input_model_imp.cpp
@@ -0,0 +1,380 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "proto_input_model_imp.hpp"
+
+#include <fstream>
+#include <memory>
+#if defined(__MINGW32__) || defined(__MINGW64__)
+#    include <filesystem>
+#endif
+#include <queue>
+
+#include "decoder_proto.hpp"
+#include "input_model.hpp"
+#include "openvino/frontend/paddle/node_context.hpp"
+#include "openvino/util/common_util.hpp"
+#include "openvino/util/file_util.hpp"
+#include "paddle_utils.hpp"
+#include "place.hpp"
+
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace proto {
+
+using namespace ::paddle::framework::proto;
+
+void ProtoInputModelImpl::load_places() {
+    const int cnt_of_blocks = m_fw_ptr->blocks_size();
+    const auto& blocks = m_fw_ptr->blocks();
+    std::map<std::string, uint64_t> op_statistics;
+
+    m_op_places.resize(cnt_of_blocks);
+
+    for (int block_idx = 0; block_idx < cnt_of_blocks; block_idx++) {
+        const auto& block = blocks[block_idx];
+
+        for (const auto& var : block.vars()) {
+            m_var_places[var.name()] = std::make_shared<ProtoTensorPlace>(m_input_model, var);
+        }
+
+        for (const auto& op : block.ops()) {
+            auto op_place = std::make_shared<ProtoOpPlace>(m_input_model, op);
+            op_place->set_decoder(std::make_shared<DecoderProto>(op_place));
+
+            if (m_telemetry) {
+                op_statistics[op.type()]++;
+            }
+
+            m_op_places[block_idx].push_back(op_place);
+
+            for (const auto& output : op.outputs()) {
+                for (const auto& var_name : output.arguments()) {
+                    auto out_port = std::make_shared<OutPortPlace>(m_input_model);
+
+                    // connect out_port and tensor
+                    const auto& tensor = m_var_places.at(var_name);
+                    tensor->add_producing_port(out_port);
+                    out_port->set_target_tensor(tensor);
+
+                    // connect out_port and op
+                    std::cout << "in op.type:" << op.type() << " " << output.parameter() << std::endl;
+                    op_place->add_out_port(out_port, output.parameter());
+                    out_port->set_op(op_place);
+                }
+            }
+
+            for (const auto& input : op.inputs()) {
+                for (const auto& var_name : input.arguments()) {
+                    auto in_port = std::make_shared<InPortPlace>(m_input_model);
+
+                    // connect in_port and tensor
+                    const auto& tensor = m_var_places.at(var_name);
+                    tensor->add_consuming_port(in_port);
+                    in_port->set_source_tensor(tensor);
+
+                    // connect in_port and op
+                    std::cout << "out op.type:" << op.type() << " " << input.parameter() << std::endl;
+                    op_place->add_in_port(in_port, input.parameter());
+                    in_port->set_op(op_place);
+                }
+            }
+
+            // Determine outputs and inputs
+            if (op.type() == "feed") {
+                const auto& place = op_place->get_output_port_paddle("Out", 0);
+                const auto& var_place = std::dynamic_pointer_cast<ProtoTensorPlace>(place->get_target_tensor_paddle());
+                const auto& tensor_desc = var_place->get_desc().type().lod_tensor().tensor();
+                const auto& dims = tensor_desc.dims();
+
+                var_place->set_element_type(get_ov_type(tensor_desc.data_type()));
+                var_place->set_partial_shape(PartialShape(std::vector<Dimension>(dims.begin(), dims.end())));
+                m_inputs.push_back(var_place);
+            } else if (op.type() == "fetch") {
+                auto place = op_place->get_input_port_paddle("X", 0);
+                m_outputs.push_back(place->get_source_tensor_paddle());
+            }
+        }
+    }
+    if (m_telemetry) {
+        for (const auto& op : op_statistics) {
+            m_telemetry->send_event("op_count", "paddle_" + op.first, static_cast<int>(op.second));
+        }
+    }
+}
+
+
+std::vector<std::shared_ptr<BaseOpPlace>> ProtoInputModelImpl::get_op_places(const int32_t blck_idx) const {
+    if (m_graph_changed) {
+        return determine_cut_nodes();
+    }
+    if (static_cast<size_t>(blck_idx) < m_op_places.size())
+        return m_op_places[blck_idx];
+    return {};
+}
+
+std::vector<std::shared_ptr<BaseOpPlace>> ProtoInputModelImpl::determine_cut_nodes() const {
+    std::queue<BaseOpPlace*> q;
+    std::unordered_set<BaseOpPlace*> visited;
+    std::vector<std::shared_ptr<BaseOpPlace>> new_op_places;
+    new_op_places.reserve(m_op_places[0].size());
+    // Marking nodes from outputs to inputs/constants
+    for (const auto& output : get_outputs()) {
+        if (!output->is_input()) {
+            auto paddle_output_op = std::dynamic_pointer_cast<BaseOpPlace>(output->get_producing_operation());
+            FRONT_END_GENERAL_CHECK(paddle_output_op != nullptr, "Output doesn't have producing operation");
+            if (!visited.count(paddle_output_op.get())) {
+                visited.insert(paddle_output_op.get());
+                q.push(paddle_output_op.get());
+                new_op_places.push_back(paddle_output_op);
+            }
+        }
+    }
+    while (!q.empty()) {
+        auto p_op = q.front();
+        q.pop();
+        for (const auto& map_pair : p_op->get_input_ports()) {
+            for (const auto& port : map_pair.second) {
+                auto tensor = port->get_source_tensor();
+                if (tensor && !tensor->is_input() && !m_tensor_values.count(tensor->get_names()[0])) {
+                    std::shared_ptr<BaseOpPlace> paddle_op =
+                        std::dynamic_pointer_cast<BaseOpPlace>(tensor->get_producing_operation());
+                    if (paddle_op && !visited.count(paddle_op.get())) {
+                        visited.insert(paddle_op.get());
+                        q.push(paddle_op.get());
+                        new_op_places.push_back(paddle_op);
+                    }
+                }
+            }
+        }
+    }
+    std::reverse(new_op_places.begin(), new_op_places.end());
+    return new_op_places;
+}
+
+// load_consts with stream is compatible with new PaddlePaddle API.
+void ProtoInputModelImpl::load_consts(std::istream* weight_stream) {
+    for (const auto& item : m_var_places) {
+        const auto& var_desc = std::dynamic_pointer_cast<ProtoTensorPlace>(item.second)->get_desc();
+        const auto& name = item.first;
+        if (ov::util::ends_with(name, std::string{"feed"}) || ov::util::ends_with(name, std::string{"fetch"}))
+            continue;
+
+        // var_desc.persistable() is used to mark node const value or not.
+        if (!var_desc.persistable())
+            continue;
+
+        FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
+        FRONT_END_GENERAL_CHECK(weight_stream != nullptr && weight_stream->peek() != EOF,
+                                "PaddlePaddle *.pdiparams format weight file doesn't exist!");
+        /*
+            reference:
+            https://github.com/PaddlePaddle/Paddle2ONNX/blob/c14446437041a0aa3572994d085b7a35c5b0985c/paddle2onnx/parser/parser.cc#L261
+            When deserialize the proto, the header of each weight
+            [ 4 byte ]      -- version(not need)
+            [   8 byte   ]  -- lod_level(not need)
+            [ 4 byte ]      -- version(not need)
+            [ 4 byte ]      -- TensorDesc size
+            [ x byte ... ]  -- TensorDesc
+            [ y byte ... ]  -- weight
+        */
+        {
+            const size_t header_size = 16;
+            std::vector<char> header(header_size);
+            weight_stream->read(&header[0], header_size);
+        }
+
+        int32_t size;
+        weight_stream->read(reinterpret_cast<char*>(&size), sizeof(size));
+
+        std::unique_ptr<char[]> buf(new char[size]);
+        weight_stream->read(reinterpret_cast<char*>(buf.get()), size);
+
+        std::unique_ptr<::paddle::framework::proto::VarType_TensorDesc> tensor_desc(
+            new ::paddle::framework::proto::VarType_TensorDesc());
+        tensor_desc->ParseFromArray(buf.get(), size);
+        Shape shape(tensor_desc->dims().cbegin(), tensor_desc->dims().cend());
+        const auto& type = get_ov_type(tensor_desc->data_type());
+        const auto& data_length = shape_size(shape) * type.size();
+        // std::cout << "name:" << name << " data_length:" << data_length << std::endl;
+        std::vector<uint8_t> tensor_data(data_length);
+
+        bool read_succeed = read_tensor(*weight_stream, reinterpret_cast<char*>(&tensor_data[0]), data_length);
+        FRONT_END_GENERAL_CHECK(read_succeed,
+                                "File containing constant with name ",
+                                name,
+                                " wasn't successfully read.");
+
+        // if (shape_size(shape) > 8 * 2) {
+        //     auto* data = (float*)(&tensor_data[0]);
+        //     float a  = *data;
+        //     float b  = *(data + 1);
+        //     std::cout << " "  << a << " " << b << std::endl;
+        // }
+
+        auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
+        const_node->set_friendly_name(name);
+        m_tensor_values[name] = const_node;
+    }
+}
+
+void ProtoInputModelImpl::create_temp_consts() {
+    for (const auto& item : m_var_places) {
+        const auto& var_place = item.second;
+        const auto& var_desc = std::dynamic_pointer_cast<ProtoTensorPlace>(var_place)->get_desc();
+        const auto& name = item.first;
+        if (var_desc.persistable())
+            continue;
+
+        // The node with tensorarray as its input may be created before the node with this tensorarray
+        // as its output. e.g. the tensorarray is both the input and output of the same node.
+        // So we have to create a fake empty node here.
+        // Problem is, we have no idea which axis should be 0.
+        // Since the models (faster/mask rcnn) are either concating tensors in tensorarray along the dynamic
+        // dimension, or concating static shape tensors. So we make the dynamic dimension to be 0. In case of static
+        // shape, we simply the the first dimension be 0.
+        if (var_desc.type().has_tensor_array()) {
+            const auto& tensor = var_desc.type().tensor_array().tensor();
+            const auto& type = get_ov_type(tensor.data_type());
+
+            std::cout << "WARNING: The PaddlePaddle model has \"TENSOR_ARRAY\" variables, which is supported "
+                      << " under limited situations.\n";
+
+            PartialShape tensor_ps(std::vector<Dimension>(tensor.dims().cbegin(), tensor.dims().cend()));
+            tensor_ps.insert(tensor_ps.begin(), 1);  // unsqueeze
+            // also update the place for following initialize the graph connection
+            var_place->set_element_type(type);
+            var_place->set_partial_shape(tensor_ps);
+
+            Shape shape(tensor_ps.size(), 0);
+            for (size_t i = 0; i < tensor_ps.size(); i++) {
+                const auto& dim = tensor_ps[i];
+                if (dim.is_static()) {
+                    shape[i] = dim.get_length();
+                }
+            }
+
+            if (tensor_ps.is_static()) {
+                // this tensorarray tensor originally could be scalar, then
+                // tensor_ps size would be 1 after unsqueeze.
+                auto idx = tensor_ps.size() > 1 ? 1 : 0;
+                shape[idx] = 0;
+            }
+
+            auto node = opset7::Constant::create(type, shape, {0});
+            node->set_friendly_name(name);
+            node->output(0).get_tensor().add_names({name});
+
+            m_tensor_values[name] = node;
+        }
+    }
+}
+
+ProtoInputModelImpl::ProtoInputModelImpl(const std::vector<std::istream*>& streams,
+                                           const InputModel& input_model,
+                                           const std::shared_ptr<TelemetryExtension>& telemetry)
+    : m_fw_ptr{std::make_shared<ProgramDesc>()},
+      m_input_model(input_model),
+      m_telemetry(telemetry) {
+    if (streams.size() != 1) {
+        FRONT_END_GENERAL_CHECK(streams.size() == 2,
+                                "Two streams are needed to load a model: model and weights streams");
+    }
+    FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(streams[0]), "Model can't be parsed");
+    int64_t version = m_fw_ptr->version().version();
+    FRONT_END_GENERAL_CHECK(
+        version >= 2000000 || version == 0,
+        "[Frontend]Only Support Paddle greater than 2.0.0, current version " + std::to_string(version));
+    load_places();
+    if (streams.size() > 1)
+        load_consts(streams[1]);
+    create_temp_consts();
+}
+
+std::vector<Place::Ptr> ProtoInputModelImpl::get_inputs() const {
+    return m_inputs;
+}
+
+std::vector<Place::Ptr> ProtoInputModelImpl::get_outputs() const {
+    return m_outputs;
+}
+
+Place::Ptr ProtoInputModelImpl::get_place_by_tensor_name(const std::string& tensorName) const {
+    if (m_var_places.count(tensorName))
+        return m_var_places.at(tensorName);
+    return nullptr;
+}
+
+namespace {
+std::shared_ptr<BaseTensorPlace> castToTensorPlace(const Place::Ptr& place) {
+    if (auto var_place = std::dynamic_pointer_cast<BaseTensorPlace>(place)) {
+        return var_place;
+    } else if (auto in_port_place = std::dynamic_pointer_cast<InPortPlace>(place)) {
+        return std::dynamic_pointer_cast<BaseTensorPlace>(in_port_place->get_source_tensor_paddle());
+    } else if (auto out_port_place = std::dynamic_pointer_cast<OutPortPlace>(place)) {
+        return std::dynamic_pointer_cast<BaseTensorPlace>(out_port_place->get_target_tensor_paddle());
+    }
+    FRONT_END_GENERAL_CHECK(false, "Cannot cast this Place to TensorPlacepaddle.");
+}
+
+}  // namespace
+
+void ProtoInputModelImpl::override_all_inputs(const std::vector<Place::Ptr>& inputs) {
+    m_graph_changed = true;
+    m_inputs.clear();
+    for (const auto& inp : inputs) {
+        m_inputs.push_back(castToTensorPlace(inp));
+    }
+}
+
+void ProtoInputModelImpl::override_all_outputs(const std::vector<Place::Ptr>& outputs) {
+    m_graph_changed = true;
+    m_outputs.clear();
+    for (const auto& outp : outputs) {
+        m_outputs.push_back(castToTensorPlace(outp));
+    }
+}
+
+void ProtoInputModelImpl::extract_subgraph(const std::vector<Place::Ptr>& inputs,
+                                                  const std::vector<Place::Ptr>& outputs) {
+    m_graph_changed = true;
+    override_all_inputs(inputs);
+    override_all_outputs(outputs);
+}
+
+void ProtoInputModelImpl::set_default_shape(Place::Ptr place, const ov::Shape& shape) {
+    FRONT_END_NOT_IMPLEMENTED("set_default_shape");
+}
+
+void ProtoInputModelImpl::set_partial_shape(Place::Ptr place, const ov::PartialShape& p_shape) {
+    castToTensorPlace(place)->set_partial_shape(p_shape);
+}
+
+ov::PartialShape ProtoInputModelImpl::get_partial_shape(Place::Ptr place) const {
+    return castToTensorPlace(place)->get_partial_shape();
+}
+
+void ProtoInputModelImpl::set_element_type(Place::Ptr place, const ov::element::Type& type) {
+    castToTensorPlace(place)->set_element_type(type);
+}
+
+ov::element::Type ProtoInputModelImpl::get_element_type(const Place::Ptr& place) const {
+    return castToTensorPlace(place)->get_element_type();
+}
+
+void ProtoInputModelImpl::set_tensor_value(Place::Ptr place, const void* value) {
+    m_graph_changed = true;
+    auto tensor_place = castToTensorPlace(place);
+    auto p_shape = tensor_place->get_partial_shape();
+    auto type = tensor_place->get_element_type();
+    auto constant = opset7::Constant::create(type, p_shape.to_shape(), value);
+    auto name = tensor_place->get_names()[0];
+    constant->set_friendly_name(name);
+    m_tensor_values[name] = constant;
+}
+}  // namespace proto
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/src/proto_input_model_imp.hpp b/src/frontends/paddle/src/proto_input_model_imp.hpp
new file mode 100644
index 0000000000..846647992e
--- /dev/null
+++ b/src/frontends/paddle/src/proto_input_model_imp.hpp
@@ -0,0 +1,150 @@
+// Copyright (C) 2018-2025 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+#include "input_model.hpp"
+#include "place.hpp"
+#include "framework.pb.h"
+#include "paddle_utils.hpp"
+#include "openvino/util/common_util.hpp"
+#include "openvino/util/file_util.hpp"
+#include "decoder_proto.hpp"
+#include "openvino/opsets/opset7.hpp"
+namespace ov {
+namespace frontend {
+namespace paddle {
+namespace proto {
+
+using namespace ::paddle::framework::proto;
+
+class ProtoInputModelImpl : public BaseInputModelImpl {
+public:
+    template <typename T>
+    ProtoInputModelImpl(const std::basic_string<T>& path,
+                   const InputModel& input_model,
+                   const std::shared_ptr<TelemetryExtension>& telemetry)
+    : m_fw_ptr{std::make_shared<ProgramDesc>()},
+      m_input_model(input_model),
+      m_telemetry(telemetry) {
+          std::ifstream weights_stream;
+          std::ifstream pb_stream(get_model_path<T>(path, &weights_stream).c_str(), std::ios::in | std::ifstream::binary);
+
+          FRONT_END_GENERAL_CHECK(pb_stream && pb_stream.is_open(),
+                  "Could not open the file: \"",
+                  util::path_to_string(path),
+                  '"');
+          FRONT_END_GENERAL_CHECK(m_fw_ptr->ParseFromIstream(&pb_stream), "Model can't be parsed");
+          // According to Paddle, the saved model has the framework version
+          // For example Paddle 2.1.0 is encoded as 2001000. 0 means the latest framework.
+          // https://github.com/paddle/Paddle/blob/develop/cmake/version.cmake
+          // https://github.com/paddle/Paddle/blob/2100816c5190693cc7dee181e96af72e9f0fbd1d/paddle/fluid/framework/program_desc.cc#L52
+          int64_t version = m_fw_ptr->version().version();
+          FRONT_END_GENERAL_CHECK(
+                  version >= 2000000 || version == 0,
+                  "[Frontend]Only Support Paddle greater than 2.0.0, current version " + std::to_string(version));
+          load_places();
+          if (is_pdmodel(path)) {
+              load_consts(&weights_stream);
+          } else {
+              load_consts(path);
+          }
+          create_temp_consts();
+      }
+
+    ProtoInputModelImpl(const std::vector<std::istream*>& streams,
+                   const InputModel& input_model,
+                   const std::shared_ptr<TelemetryExtension>& telemetry);
+    std::vector<Place::Ptr> get_inputs() const;
+    std::vector<Place::Ptr> get_outputs() const;
+    int64_t get_version() const {
+        return m_fw_ptr->version().version();
+    }
+    Place::Ptr get_place_by_tensor_name(const std::string& tensorName) const;
+    void override_all_outputs(const std::vector<Place::Ptr>& outputs);
+    void override_all_inputs(const std::vector<Place::Ptr>& inputs);
+    void extract_subgraph(const std::vector<Place::Ptr>& inputs, const std::vector<Place::Ptr>& outputs);
+    void set_default_shape(Place::Ptr place, const ov::Shape&);
+    void set_partial_shape(Place::Ptr place, const ov::PartialShape&);
+    ov::PartialShape get_partial_shape(Place::Ptr place) const;
+    void set_element_type(Place::Ptr place, const ov::element::Type&);
+    ov::element::Type get_element_type(const Place::Ptr& place) const;
+    void set_tensor_value(Place::Ptr place, const void* value);
+    std::vector<std::shared_ptr<BaseOpPlace>> get_op_places(const int32_t blck_idx) const;
+    std::map<std::string, std::shared_ptr<BaseTensorPlace>> get_var_places() const {
+        return m_var_places;
+    }
+    std::map<paddle::TensorName, Output<Node>> get_tensor_values() const {
+        return m_tensor_values;
+    };
+
+private:
+    void load_places();
+    template <typename T>
+        void load_consts(const std::basic_string<T>& folder_with_weights) {
+            for (const auto& item : m_var_places) {
+                const auto& var_desc = std::dynamic_pointer_cast<ProtoTensorPlace>(item.second)->get_desc();
+                const auto& name = item.first;
+                if (ov::util::ends_with(name, std::string{"feed"}) || ov::util::ends_with(name, std::string{"fetch"}))
+                    continue;
+                if (!var_desc.persistable())
+                    continue;
+
+                FRONT_END_GENERAL_CHECK(var_desc.type().type() == ::paddle::framework::proto::VarType::LOD_TENSOR);
+                const auto& tensor = var_desc.type().lod_tensor().tensor();
+                Shape shape(tensor.dims().cbegin(), tensor.dims().cend());
+                const auto& type = get_ov_type(tensor.data_type());
+                const auto& data_length = shape_size(shape) * type.size();
+                std::vector<uint8_t> tensor_data(data_length);
+
+                bool read_succeed = false;
+                if (!folder_with_weights.empty()) {
+#if defined(__MINGW32__) || defined(__MINGW64__)
+                    std::ifstream is(std::filesystem::path(get_const_path(folder_with_weights, name)),
+                            std::ios::in | std::ifstream::binary);
+#else
+                    std::ifstream is(get_const_path(folder_with_weights, name), std::ios::in | std::ifstream::binary);
+#endif
+                    FRONT_END_GENERAL_CHECK(is && is.is_open(), "Cannot open file for constant value.");
+                    const size_t header_size = 16;
+                    std::vector<char> header(header_size);
+                    is.read(&header[0], header_size);
+
+                    uint32_t dims_len = 0;
+                    is.read(reinterpret_cast<char*>(&dims_len), 4);
+                    std::vector<char> dims_struct(dims_len);
+                    is.read(&dims_struct[0], dims_len);
+                    read_succeed = read_tensor(is, reinterpret_cast<char*>(&tensor_data[0]), data_length);
+                } else {
+                    FRONT_END_GENERAL_CHECK(false, "Folder with weights must be provided.");
+                }
+                FRONT_END_GENERAL_CHECK(read_succeed,
+                        "File containing constant with name ",
+                        name,
+                        " wasn't successfully read.");
+                auto const_node = opset7::Constant::create(type, shape, &tensor_data[0]);
+                const_node->set_friendly_name(name);
+                m_tensor_values[name] = const_node;
+            }
+        }
+    void load_consts(std::istream* weight_stream);
+    void create_temp_consts();
+    std::vector<std::shared_ptr<BaseOpPlace>> determine_cut_nodes() const;
+
+    std::vector<std::vector<std::shared_ptr<BaseOpPlace>>> m_op_places;
+    std::map<std::string, std::shared_ptr<BaseTensorPlace>> m_var_places;
+    std::shared_ptr<ProgramDesc> m_fw_ptr;
+    const InputModel& m_input_model;
+    std::vector<Place::Ptr> m_inputs;
+    std::vector<Place::Ptr> m_outputs;
+    std::map<paddle::TensorName, Output<Node>> m_tensor_values;
+
+    std::shared_ptr<TelemetryExtension> m_telemetry;
+
+    // shows if some nodes might be deleted from graph
+    bool m_graph_changed = false;
+};
+}  // namespace proto
+}  // namespace paddle
+}  // namespace frontend
+}  // namespace ov
diff --git a/src/frontends/paddle/tests/CMakeLists.txt b/src/frontends/paddle/tests/CMakeLists.txt
index 5340c41271..687e20c3ac 100644
--- a/src/frontends/paddle/tests/CMakeLists.txt
+++ b/src/frontends/paddle/tests/CMakeLists.txt
@@ -1,5 +1,7 @@
 # Copyright (C) 2018-2025 Intel Corporation
 # SPDX-License-Identifier: Apache-2.0
 #
+
 add_subdirectory(standalone_build)
 add_subdirectory(pdmodel_test)
+add_subdirectory(jsonmodel_test)
diff --git a/src/frontends/paddle/tests/basic_api.cpp b/src/frontends/paddle/tests/basic_api.cpp
index 5ed258fcdd..82484ce022 100644
--- a/src/frontends/paddle/tests/basic_api.cpp
+++ b/src/frontends/paddle/tests/basic_api.cpp
@@ -11,11 +11,11 @@ using namespace ov::frontend;
 using PaddleBasicTest = FrontEndBasicTest;
 
 static const std::vector<std::string> models{
-    std::string("conv2d/conv2d.pdmodel"),
-    std::string("conv2d_relu/conv2d_relu.pdmodel"),
-    std::string("2in_2out/2in_2out.pdmodel"),
-    std::string("multi_tensor_split/multi_tensor_split.pdmodel"),
-    std::string("2in_2out_dynbatch/2in_2out_dynbatch.pdmodel"),
+    std::string("conv2d/conv2d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_relu/conv2d_relu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multi_tensor_split/multi_tensor_split" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("2in_2out_dynbatch/2in_2out_dynbatch" + std::string(TEST_PADDLE_MODEL_EXT)),
 };
 
 INSTANTIATE_TEST_SUITE_P(PaddleBasicTest,
diff --git a/src/frontends/paddle/tests/common.cmake b/src/frontends/paddle/tests/common.cmake
index 06c4758947..e5e32787b8 100644
--- a/src/frontends/paddle/tests/common.cmake
+++ b/src/frontends/paddle/tests/common.cmake
@@ -79,7 +79,8 @@ target_compile_definitions(${TARGET_NAME} PRIVATE -D TEST_GEN_TAG=\"${paddle_gen
 # If 'paddlepaddle' is not found, code will still be compiled, but models will not be generated and tests will fail
 # This is done this way for 'code style' and check cases - cmake shall pass, but CI machine doesn't need to have
 # 'paddlepaddle' installed to check code style
-if(PADDLEDET_RESULT)
+set(GEN FALSE)
+if(PADDLEDET_RESULT AND ${GEN})
     set(TEST_PADDLE_MODELS ${TEST_MODEL_ZOO_OUTPUT_DIR}/paddle_test_models/${PD_MODEL_TAG}/)
 
     file(GLOB_RECURSE PADDLE_ALL_SCRIPTS ${CODE_ROOT_DIR}/*.py)
@@ -113,4 +114,3 @@ if(ENABLE_INTEL_CPU)
 endif()
 
 ov_build_target_faster(${TARGET_NAME} PCH)
-
diff --git a/src/frontends/paddle/tests/conversion.cpp b/src/frontends/paddle/tests/conversion.cpp
index dee29e5a7f..a8d81e0d88 100644
--- a/src/frontends/paddle/tests/conversion.cpp
+++ b/src/frontends/paddle/tests/conversion.cpp
@@ -41,7 +41,7 @@ static ConversionExtensionFEParam getTestData() {
     ConversionExtensionFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_modelName = "relu/relu.pdmodel";
+    res.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_translatorName = translator_name;
     res.m_frontend = std::make_shared<PaddleFrontendWrapper>();
     return res;
diff --git a/src/frontends/paddle/tests/convert_model.cpp b/src/frontends/paddle/tests/convert_model.cpp
index 9a48c2fb53..126fb03416 100644
--- a/src/frontends/paddle/tests/convert_model.cpp
+++ b/src/frontends/paddle/tests/convert_model.cpp
@@ -12,12 +12,12 @@ using namespace ov::frontend;
 using PaddleConvertModelTest = FrontEndConvertModelTest;
 
 static const std::vector<std::string> models{
-    std::string("conv2d/conv2d.pdmodel"),
-    std::string("conv2d_relu/conv2d_relu.pdmodel"),
-    std::string("2in_2out/2in_2out.pdmodel"),
-    std::string("multi_tensor_split/multi_tensor_split.pdmodel"),
-    std::string("2in_2out_dynbatch/2in_2out_dynbatch.pdmodel"),
-    std::string("pool2d_dyn_hw/pool2d_dyn_hw.pdmodel"),
+    std::string("conv2d/conv2d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_relu/conv2d_relu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multi_tensor_split/multi_tensor_split" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("2in_2out_dynbatch/2in_2out_dynbatch" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pool2d_dyn_hw/pool2d_dyn_hw" + std::string(TEST_PADDLE_MODEL_EXT)),
 };
 
 INSTANTIATE_TEST_SUITE_P(PaddleConvertModelTest,
diff --git a/src/frontends/paddle/tests/convert_unsupported.cpp b/src/frontends/paddle/tests/convert_unsupported.cpp
index 20a13c7949..27fb58fe3f 100644
--- a/src/frontends/paddle/tests/convert_unsupported.cpp
+++ b/src/frontends/paddle/tests/convert_unsupported.cpp
@@ -19,21 +19,24 @@ TEST(FrontEndConvertModelTest, test_unsupported_op) {
     OV_ASSERT_NO_THROW(frontEnd = fem.load_by_framework(PADDLE_FE));
     ASSERT_NE(frontEnd, nullptr);
     auto model_filename = FrontEndTestUtils::make_model_path(std::string(TEST_PADDLE_MODELS_DIRNAME) +
-                                                             std::string("relu_unsupported/relu_unsupported.pdmodel"));
+                                                             std::string("relu_unsupported/relu_unsupported" + std::string(TEST_PADDLE_MODEL_EXT)));
     OV_ASSERT_NO_THROW(inputModel = frontEnd->load(model_filename));
     ASSERT_NE(inputModel, nullptr);
     std::shared_ptr<ov::Model> model;
     ASSERT_THROW(model = frontEnd->convert(inputModel), OpConversionFailure);
     ASSERT_EQ(model, nullptr);
-    OV_ASSERT_NO_THROW(model = frontEnd->decode(inputModel));
-    ASSERT_THROW(frontEnd->convert(model), OpConversionFailure);
-    OV_ASSERT_NO_THROW(model = frontEnd->convert_partially(inputModel));
-    ASSERT_THROW(frontEnd->convert(model), OpConversionFailure);
-
-    for (auto& node : model->get_ordered_ops()) {
-        if (node->get_friendly_name() == "rxyz_0.tmp_0") {
-            model->replace_node(node, std::make_shared<ov::opset6::Relu>(node->input(0).get_source_output()));
+    if (TEST_PADDLE_MODEL_EXT == ".json") {
+        ASSERT_THROW(model = frontEnd->decode(inputModel), OpConversionFailure);
+    } else {
+        OV_ASSERT_NO_THROW(model = frontEnd->decode(inputModel));
+        ASSERT_THROW(frontEnd->convert(model), OpConversionFailure);
+        OV_ASSERT_NO_THROW(model = frontEnd->convert_partially(inputModel));
+        ASSERT_THROW(frontEnd->convert(model), OpConversionFailure);
+        for (auto& node : model->get_ordered_ops()) {
+            if (node->get_friendly_name() == "rxyz_0.tmp_0") {
+                model->replace_node(node, std::make_shared<ov::opset6::Relu>(node->input(0).get_source_output()));
+            }
         }
+        OV_ASSERT_NO_THROW(frontEnd->convert(model));
     }
-    OV_ASSERT_NO_THROW(frontEnd->convert(model));
 }
diff --git a/src/frontends/paddle/tests/incorrect_cut_model.cpp b/src/frontends/paddle/tests/incorrect_cut_model.cpp
index d2dc7eb0c1..e2d22381ee 100644
--- a/src/frontends/paddle/tests/incorrect_cut_model.cpp
+++ b/src/frontends/paddle/tests/incorrect_cut_model.cpp
@@ -19,7 +19,7 @@ TEST(FrontEndIncorrectCutModelTest, test_incorrect_cut) {
     OV_ASSERT_NO_THROW(frontEnd = fem.load_by_framework(PADDLE_FE));
     ASSERT_NE(frontEnd, nullptr);
     auto model_filename = FrontEndTestUtils::make_model_path(std::string(TEST_PADDLE_MODELS_DIRNAME) +
-                                                             std::string("2in_2out/2in_2out.pdmodel"));
+                                                             std::string("2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT)));
     OV_ASSERT_NO_THROW(inputModel = frontEnd->load(model_filename));
     ASSERT_NE(inputModel, nullptr);
 
diff --git a/src/frontends/paddle/tests/jsonmodel_test/CMakeLists.txt b/src/frontends/paddle/tests/jsonmodel_test/CMakeLists.txt
new file mode 100644
index 0000000000..c605fee0df
--- /dev/null
+++ b/src/frontends/paddle/tests/jsonmodel_test/CMakeLists.txt
@@ -0,0 +1,8 @@
+# Copyright (C) 2018-2025 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+#
+set(TARGET_NAME "paddle_json_tests")
+set(PD_MODEL_TAG "json")
+set(TEST_PADDLE_MODEL_EXT ".json")
+set(ENABLE_PIR "1")
+include(../common.cmake)
diff --git a/src/frontends/paddle/tests/lib_close.cpp b/src/frontends/paddle/tests/lib_close.cpp
index 29c8cbc215..119079ced7 100644
--- a/src/frontends/paddle/tests/lib_close.cpp
+++ b/src/frontends/paddle/tests/lib_close.cpp
@@ -9,11 +9,18 @@
 
 using namespace testing;
 using namespace ov::util;
+auto get_tensor_name = [] -> const std::string {
+    if (std::string(TEST_ENABLE_PIR) == "1") {
+        return "3";
+    } else {
+        return "conv2d_0.tmp_0";
+    }
+};
 
 INSTANTIATE_TEST_SUITE_P(
     Paddle,
     FrontendLibCloseTest,
     Values(std::make_tuple("paddle",
-                           path_join({TEST_PADDLE_MODELS_DIRNAME, "conv2d_relu/conv2d_relu.pdmodel"}).string(),
-                           "conv2d_0.tmp_0")),
+                           path_join({TEST_PADDLE_MODELS_DIRNAME, "conv2d_relu/conv2d_relu" + std::string(TEST_PADDLE_MODEL_EXT)}).string(),
+                           get_tensor_name())),
     FrontendLibCloseTest::get_test_case_name);
diff --git a/src/frontends/paddle/tests/library_extension.cpp b/src/frontends/paddle/tests/library_extension.cpp
index 38376042a5..9602d5bd56 100644
--- a/src/frontends/paddle/tests/library_extension.cpp
+++ b/src/frontends/paddle/tests/library_extension.cpp
@@ -14,7 +14,7 @@ static FrontendLibraryExtensionTestParams getTestData() {
     FrontendLibraryExtensionTestParams params;
     params.m_frontEndName = PADDLE_FE;
     params.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    params.m_modelName = "relu/relu.pdmodel";
+    params.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     return params;
 }
 
diff --git a/src/frontends/paddle/tests/load_from.cpp b/src/frontends/paddle/tests/load_from.cpp
index 025443ae5c..76202dd330 100644
--- a/src/frontends/paddle/tests/load_from.cpp
+++ b/src/frontends/paddle/tests/load_from.cpp
@@ -14,10 +14,10 @@ static LoadFromFEParam getTestData() {
     LoadFromFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_file = "conv2d/conv2d.pdmodel";
-    res.m_files = {"2in_2out/2in_2out.pdmodel", "2in_2out/2in_2out.pdiparams"};
-    res.m_stream = "relu/relu.pdmodel";
-    res.m_streams = {"2in_2out/2in_2out.pdmodel", "2in_2out/2in_2out.pdiparams"};
+    res.m_file = "conv2d/conv2d" + std::string(TEST_PADDLE_MODEL_EXT);
+    res.m_files = {"2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT), "2in_2out/2in_2out.pdiparams"};
+    res.m_stream = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
+    res.m_streams = {"2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT), "2in_2out/2in_2out.pdiparams"};
     return res;
 }
 
diff --git a/src/frontends/paddle/tests/op_extension.cpp b/src/frontends/paddle/tests/op_extension.cpp
index cbd05bb1f1..999cc1b44d 100644
--- a/src/frontends/paddle/tests/op_extension.cpp
+++ b/src/frontends/paddle/tests/op_extension.cpp
@@ -57,7 +57,7 @@ static OpExtensionFEParam getTestDataOpExtensionViaUserClass() {
     OpExtensionFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_modelName = "relu/relu.pdmodel";
+    res.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     // use core OpExtension
     res.m_extensions = std::vector<std::shared_ptr<ov::Extension>>{std::make_shared<ov::OpExtension<Relu1>>(),
                                                                    std::make_shared<ov::OpExtension<Relu2>>(),
@@ -70,7 +70,7 @@ static OpExtensionFEParam getTestDataOpExtensionViaPaddleConstructor() {
     OpExtensionFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_modelName = "relu/relu.pdmodel";
+    res.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     // use ov::frontend::paddle OpExtension
     res.m_extensions = std::vector<std::shared_ptr<ov::Extension>>{
         std::make_shared<ov::frontend::paddle::OpExtension<>>("CustomRelu_5",
@@ -111,7 +111,7 @@ static OpExtensionFEParam getTestDataOpExtensionViaCommonConstructor() {
     OpExtensionFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_modelName = "relu/relu.pdmodel";
+    res.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     // use ov::frontend::OpExtension
     res.m_extensions = std::vector<std::shared_ptr<ov::Extension>>{
         std::make_shared<ov::frontend::OpExtension<>>("CustomRelu_9",
@@ -168,7 +168,7 @@ TEST(FrontEndOpExtensionTest, paddle_opextension_relu) {
     const auto extensions = std::vector<std::shared_ptr<ov::Extension>>{std::make_shared<ov::OpExtension<Relu1>>()};
     core.add_extension(extensions);
     std::string m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    std::string m_modelName = "relu/relu.pdmodel";
+    std::string m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     auto model = core.read_model(FrontEndTestUtils::make_model_path(m_modelsPath + m_modelName));
     bool has_relu = false;
     for (const auto& op : model->get_ops()) {
diff --git a/src/frontends/paddle/tests/op_fuzzy.cpp b/src/frontends/paddle/tests/op_fuzzy.cpp
index dd17be8095..d24446e885 100644
--- a/src/frontends/paddle/tests/op_fuzzy.cpp
+++ b/src/frontends/paddle/tests/op_fuzzy.cpp
@@ -14,677 +14,677 @@ using namespace ov::frontend;
 using PaddleFuzzyOpTest = FrontEndFuzzyOpTest;
 
 static const std::vector<std::string> models{
-    std::string("argmax/argmax.pdmodel"),
-    std::string("argmax1/argmax1.pdmodel"),
-    std::string("assign_none/assign_none.pdmodel"),
-    std::string("assign_output/assign_output.pdmodel"),
-    std::string("assign_value_boolean/assign_value_boolean.pdmodel"),
-    std::string("assign_value_fp32/assign_value_fp32.pdmodel"),
-    std::string("assign_value_int32/assign_value_int32.pdmodel"),
-    std::string("assign_value_int64/assign_value_int64.pdmodel"),
-    std::string("avgAdaptivePool2D_test1/avgAdaptivePool2D_test1.pdmodel"),
-    std::string("avgPool_test1/avgPool_test1.pdmodel"),
-    std::string("avgPool_test10/avgPool_test10.pdmodel"),
-    std::string("avgPool_test11/avgPool_test11.pdmodel"),
-    std::string("avgPool_test2/avgPool_test2.pdmodel"),
-    std::string("avgPool_test3/avgPool_test3.pdmodel"),
-    std::string("avgPool_test4/avgPool_test4.pdmodel"),
-    std::string("avgPool_test5/avgPool_test5.pdmodel"),
+    std::string("argmax/argmax" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("argmax1/argmax1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_none/assign_none" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_output/assign_output" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_value_boolean/assign_value_boolean" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_value_fp32/assign_value_fp32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_value_int32/assign_value_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("assign_value_int64/assign_value_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgAdaptivePool2D_test1/avgAdaptivePool2D_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test1/avgPool_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test10/avgPool_test10" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test11/avgPool_test11" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test2/avgPool_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test3/avgPool_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test4/avgPool_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test5/avgPool_test5" + std::string(TEST_PADDLE_MODEL_EXT)),
     // avgPool_test6<nchw support is disabled now>,
-    std::string("avgPool_test7/avgPool_test7.pdmodel"),
-    std::string("avgPool_test8/avgPool_test8.pdmodel"),
-    std::string("avgPool_test9/avgPool_test9.pdmodel"),
-    std::string("avgAdaptivePool3D_test1/avgAdaptivePool3D_test1.pdmodel"),
-    std::string("avgAdaptivePool3D_test2/avgAdaptivePool3D_test2.pdmodel"),
-    std::string("avgAdaptivePool3D_test3/avgAdaptivePool3D_test3.pdmodel"),
-    std::string("avgAdaptivePool3D_test4/avgAdaptivePool3D_test4.pdmodel"),
-    std::string("avg3dPool_test1/avg3dPool_test1.pdmodel"),
-    std::string("avg3dPool_test2/avg3dPool_test2.pdmodel"),
-    std::string("avg3dPool_test3/avg3dPool_test3.pdmodel"),
-    std::string("avg3dPool_test4/avg3dPool_test4.pdmodel"),
-    std::string("avg3dPool_test5/avg3dPool_test5.pdmodel"),
-    std::string("avg3dPool_test6/avg3dPool_test6.pdmodel"),
-    std::string("avg3dPool_test7/avg3dPool_test7.pdmodel"),
-    std::string("avg3dPool_test8/avg3dPool_test8.pdmodel"),
-    std::string("avg3dPool_test9/avg3dPool_test9.pdmodel"),
-    std::string("avg3dPool_test10/avg3dPool_test10.pdmodel"),
-    std::string("batch_norm_nchw/batch_norm_nchw.pdmodel"),
-    std::string("batch_norm_nhwc/batch_norm_nhwc.pdmodel"),
-    std::string("bicubic_downsample_false_0/bicubic_downsample_false_0.pdmodel"),
-    std::string("bicubic_downsample_false_1/bicubic_downsample_false_1.pdmodel"),
-    std::string("bicubic_downsample_true_0/bicubic_downsample_true_0.pdmodel"),
-    std::string("bicubic_upsample_false_0/bicubic_upsample_false_0.pdmodel"),
-    std::string("bicubic_upsample_false_1/bicubic_upsample_false_1.pdmodel"),
-    std::string("bicubic_upsample_scales/bicubic_upsample_scales.pdmodel"),
-    std::string("bicubic_upsample_scales2/bicubic_upsample_scales2.pdmodel"),
-    std::string("bicubic_upsample_true_0/bicubic_upsample_true_0.pdmodel"),
-    std::string("bicubic_upsample_tensor_size/bicubic_upsample_tensor_size.pdmodel"),
-    std::string("bilinear_downsample_false_0/bilinear_downsample_false_0.pdmodel"),
-    std::string("bilinear_downsample_false_1/bilinear_downsample_false_1.pdmodel"),
-    std::string("bilinear_downsample_true_0/bilinear_downsample_true_0.pdmodel"),
-    std::string("bilinear_upsample_false_0/bilinear_upsample_false_0.pdmodel"),
-    std::string("bilinear_upsample_false_1/bilinear_upsample_false_1.pdmodel"),
-    std::string("bilinear_upsample_scales/bilinear_upsample_scales.pdmodel"),
-    std::string("bilinear_upsample_scales2/bilinear_upsample_scales2.pdmodel"),
-    std::string("bilinear_upsample_true_0/bilinear_upsample_true_0.pdmodel"),
-    std::string("bilinear_upsample_tensor_size/bilinear_upsample_tensor_size.pdmodel"),
-    std::string("bmm/bmm.pdmodel"),
-    std::string("box_coder_1/box_coder_1.pdmodel"),
-    std::string("box_coder_2/box_coder_2.pdmodel"),
-    std::string("box_coder_3/box_coder_3.pdmodel"),
-    std::string("ceil/ceil.pdmodel"),
-    std::string("clip/clip.pdmodel"),
+    std::string("avgPool_test7/avgPool_test7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test8/avgPool_test8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgPool_test9/avgPool_test9" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgAdaptivePool3D_test1/avgAdaptivePool3D_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgAdaptivePool3D_test2/avgAdaptivePool3D_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgAdaptivePool3D_test3/avgAdaptivePool3D_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avgAdaptivePool3D_test4/avgAdaptivePool3D_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test1/avg3dPool_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test2/avg3dPool_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test3/avg3dPool_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test4/avg3dPool_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test5/avg3dPool_test5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test6/avg3dPool_test6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test7/avg3dPool_test7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test8/avg3dPool_test8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test9/avg3dPool_test9" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("avg3dPool_test10/avg3dPool_test10" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("batch_norm_nchw/batch_norm_nchw" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("batch_norm_nhwc/batch_norm_nhwc" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_downsample_false_0/bicubic_downsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_downsample_false_1/bicubic_downsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_downsample_true_0/bicubic_downsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_false_0/bicubic_upsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_false_1/bicubic_upsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_scales/bicubic_upsample_scales" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_scales2/bicubic_upsample_scales2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_true_0/bicubic_upsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bicubic_upsample_tensor_size/bicubic_upsample_tensor_size" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_downsample_false_0/bilinear_downsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_downsample_false_1/bilinear_downsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_downsample_true_0/bilinear_downsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_false_0/bilinear_upsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_false_1/bilinear_upsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_scales/bilinear_upsample_scales" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_scales2/bilinear_upsample_scales2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_true_0/bilinear_upsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bilinear_upsample_tensor_size/bilinear_upsample_tensor_size" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("bmm/bmm" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("box_coder_1/box_coder_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("box_coder_2/box_coder_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("box_coder_3/box_coder_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("ceil/ceil" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("clip/clip" + std::string(TEST_PADDLE_MODEL_EXT)),
     // 95436: sporadic failure
     // could not support dynamic rank
-    // std::string("conditional_block_const/conditional_block_const.pdmodel"),
-    // std::string("conditional_block_const_2outputs/conditional_block_const_2outputs.pdmodel"),
-    std::string("conditional_block_2inputs/conditional_block_2inputs.pdmodel"),
-    std::string("conditional_block_2inputs_2outputs/conditional_block_2inputs_2outputs.pdmodel"),
-    std::string("conditional_block_2inputs_dyn/conditional_block_2inputs_dyn.pdmodel"),
-    std::string("conditional_block_2inputs_dyn_2outputs/conditional_block_2inputs_dyn_2outputs.pdmodel"),
-    std::string("conditional_block_dyn_multiple_consumers/conditional_block_dyn_multiple_consumers.pdmodel"),
-    std::string("conditional_block_dyn_multiple_blocks/conditional_block_dyn_multiple_blocks.pdmodel"),
-    std::string("conditional_block_dyn_multiple_blocks2/conditional_block_dyn_multiple_blocks2.pdmodel"),
+    // std::string("conditional_block_const/conditional_block_const" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_const_2outputs/conditional_block_const_2outputs" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_2inputs/conditional_block_2inputs" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_2inputs_2outputs/conditional_block_2inputs_2outputs" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_2inputs_dyn/conditional_block_2inputs_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_2inputs_dyn_2outputs/conditional_block_2inputs_dyn_2outputs" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_dyn_multiple_consumers/conditional_block_dyn_multiple_consumers" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_dyn_multiple_blocks/conditional_block_dyn_multiple_blocks" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conditional_block_dyn_multiple_blocks2/conditional_block_dyn_multiple_blocks2" + std::string(TEST_PADDLE_MODEL_EXT)),
     //// TensorArray case
     //// Save model failed if update paddlepaddle to 2.4.0
-    // std::string("conditional_block_concat/conditional_block_concat.pdmodel"),
-    // std::string("conditional_block_concat_dyn/conditional_block_concat_dyn.pdmodel"),
-    // std::string("conditional_block_concat_dyn_2axes/conditional_block_concat_dyn_2axes.pdmodel"),
-    // std::string("conditional_block_conditional_block_concat/conditional_block_conditional_block_concat.pdmodel"),
+    // std::string("conditional_block_concat/conditional_block_concat" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_concat_dyn/conditional_block_concat_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_concat_dyn_2axes/conditional_block_concat_dyn_2axes" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_conditional_block_concat/conditional_block_conditional_block_concat" + std::string(TEST_PADDLE_MODEL_EXT)),
     // std::string(
-    //    "conditional_block_conditional_block_concat_dyn/conditional_block_conditional_block_concat_dyn.pdmodel"),
-    // std::string("conditional_block_slice0/conditional_block_slice0.pdmodel"),
-    // std::string("conditional_block_slice0_dyn/conditional_block_slice0_dyn.pdmodel"),
-    // std::string("conditional_block_slice0_else/conditional_block_slice0_else.pdmodel"),
-    // std::string("conditional_block_slice0_scaler/conditional_block_slice0_scaler.pdmodel"),
-    // std::string("conditional_block_slice0_scaler_dyn/conditional_block_slice0_scaler_dyn.pdmodel"),
-    //// std::string("conditional_block_slice0_axis2/conditional_block_slice0_axis2.pdmodel"),   // No such case in
+    //    "conditional_block_conditional_block_concat_dyn/conditional_block_conditional_block_concat_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0/conditional_block_slice0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_dyn/conditional_block_slice0_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_else/conditional_block_slice0_else" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_scaler/conditional_block_slice0_scaler" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_scaler_dyn/conditional_block_slice0_scaler_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    //// std::string("conditional_block_slice0_axis2/conditional_block_slice0_axis2" + std::string(TEST_PADDLE_MODEL_EXT)),   // No such case in
     //// model, as paddle.concat always concat along axis 0.
-    //// std::string("conditional_block_slice0_axis2_dyn/conditional_block_slice0_axis2_dyn.pdmodel"),
-    //// std::string("conditional_block_slice0_axis1_axis2/conditional_block_slice0_axis1_axis2.pdmodel"),
-    //// std::string("conditional_block_slice0_axis1_axis2_dyn/conditional_block_slice0_axis1_axis2_dyn.pdmodel"),
-    // std::string("conditional_block_concat_false/conditional_block_concat_false.pdmodel"),
-    // std::string("conditional_block_concat_false_dyn/conditional_block_concat_false_dyn.pdmodel"),
-    // std::string("conditional_block_slice0_2tensorarrays/conditional_block_slice0_2tensorarrays.pdmodel"),
-    // std::string("conditional_block_slice0_2tensorarrays_dyn/conditional_block_slice0_2tensorarrays_dyn.pdmodel"),
-    // std::string("conditional_block_slice0_2tensorarrays_extra/conditional_block_slice0_2tensorarrays_extra.pdmodel"),
+    //// std::string("conditional_block_slice0_axis2_dyn/conditional_block_slice0_axis2_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    //// std::string("conditional_block_slice0_axis1_axis2/conditional_block_slice0_axis1_axis2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    //// std::string("conditional_block_slice0_axis1_axis2_dyn/conditional_block_slice0_axis1_axis2_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_concat_false/conditional_block_concat_false" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_concat_false_dyn/conditional_block_concat_false_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_2tensorarrays/conditional_block_slice0_2tensorarrays" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_2tensorarrays_dyn/conditional_block_slice0_2tensorarrays_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("conditional_block_slice0_2tensorarrays_extra/conditional_block_slice0_2tensorarrays_extra" + std::string(TEST_PADDLE_MODEL_EXT)),
     // std::string(
-    //    "conditional_block_slice0_2tensorarrays_extra_dyn/conditional_block_slice0_2tensorarrays_extra_dyn.pdmodel"),
-    std::string("conv2d_dilation_assymetric_pads_strides/conv2d_dilation_assymetric_pads_strides.pdmodel"),
-    std::string("conv2d_SAME_padding/conv2d_SAME_padding.pdmodel"),
-    std::string("conv2d_strides_assymetric_padding/conv2d_strides_assymetric_padding.pdmodel"),
-    std::string("conv2d_strides_no_padding/conv2d_strides_no_padding.pdmodel"),
-    std::string("conv2d_strides_padding/conv2d_strides_padding.pdmodel"),
+    //    "conditional_block_slice0_2tensorarrays_extra_dyn/conditional_block_slice0_2tensorarrays_extra_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_dilation_assymetric_pads_strides/conv2d_dilation_assymetric_pads_strides" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_SAME_padding/conv2d_SAME_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_strides_assymetric_padding/conv2d_strides_assymetric_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_strides_no_padding/conv2d_strides_no_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_strides_padding/conv2d_strides_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string(
-        "conv2d_transpose_dilation_assymetric_pads_strides/conv2d_transpose_dilation_assymetric_pads_strides.pdmodel"),
+        "conv2d_transpose_dilation_assymetric_pads_strides/conv2d_transpose_dilation_assymetric_pads_strides" + std::string(TEST_PADDLE_MODEL_EXT)),
     // conv2d_transpose_SAME_padding(Paddle outputs wrong results),
-    std::string("conv2d_transpose_strides_assymetric_padding/conv2d_transpose_strides_assymetric_padding.pdmodel"),
-    std::string("conv2d_transpose_strides_no_padding/conv2d_transpose_strides_no_padding.pdmodel"),
-    std::string("conv2d_transpose_strides_padding/conv2d_transpose_strides_padding.pdmodel"),
-    std::string("conv2d_transpose_VALID_padding/conv2d_transpose_VALID_padding.pdmodel"),
-    std::string("conv2d_VALID_padding/conv2d_VALID_padding.pdmodel"),
-    std::string("cos/cos.pdmodel"),
-    std::string("cumsum/cumsum.pdmodel"),
-    std::string("cumsum_i32/cumsum_i32.pdmodel"),
-    std::string("cumsum_i64/cumsum_i64.pdmodel"),
-    std::string("cumsum_f32/cumsum_f32.pdmodel"),
-    std::string("cumsum_f64/cumsum_f64.pdmodel"),
-    std::string("deformable_conv_default/deformable_conv_default.pdmodel"),
-    std::string("deformable_conv_with_bias/deformable_conv_with_bias.pdmodel"),
-    std::string("deformable_conv_with_deformable_groups/deformable_conv_with_deformable_groups.pdmodel"),
-    std::string("deformable_conv_with_dilation/deformable_conv_with_dilation.pdmodel"),
-    std::string("deformable_conv_with_dilation_list/deformable_conv_with_dilation_list.pdmodel"),
-    std::string("deformable_conv_with_dilation_tuple/deformable_conv_with_dilation_tuple.pdmodel"),
-    std::string("deformable_conv_with_groups/deformable_conv_with_groups.pdmodel"),
+    std::string("conv2d_transpose_strides_assymetric_padding/conv2d_transpose_strides_assymetric_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_transpose_strides_no_padding/conv2d_transpose_strides_no_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_transpose_strides_padding/conv2d_transpose_strides_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_transpose_VALID_padding/conv2d_transpose_VALID_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("conv2d_VALID_padding/conv2d_VALID_padding" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cos/cos" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cumsum/cumsum" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cumsum_i32/cumsum_i32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cumsum_i64/cumsum_i64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cumsum_f32/cumsum_f32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("cumsum_f64/cumsum_f64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_default/deformable_conv_default" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_bias/deformable_conv_with_bias" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_deformable_groups/deformable_conv_with_deformable_groups" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_dilation/deformable_conv_with_dilation" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_dilation_list/deformable_conv_with_dilation_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_dilation_tuple/deformable_conv_with_dilation_tuple" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_groups/deformable_conv_with_groups" + std::string(TEST_PADDLE_MODEL_EXT)),
     // (CVS-86585: about e-5 level accuracy diff)
-    // std::string("deformable_conv_with_mask/deformable_conv_with_mask.pdmodel"),
-    // std::string("deformable_conv_with_mask_bias/deformable_conv_with_mask_bias.pdmodel"),
-    std::string("deformable_conv_with_pad/deformable_conv_with_pad.pdmodel"),
-    std::string("deformable_conv_with_pad_list/deformable_conv_with_pad_list.pdmodel"),
-    std::string("deformable_conv_with_pad_tuple/deformable_conv_with_pad_tuple.pdmodel"),
-    std::string("deformable_conv_with_stride/deformable_conv_with_stride.pdmodel"),
-    std::string("deformable_conv_with_stride_list/deformable_conv_with_stride_list.pdmodel"),
-    std::string("deformable_conv_with_stride_tuple/deformable_conv_with_stride_tuple.pdmodel"),
-    std::string("depthwise_conv2d_convolution/depthwise_conv2d_convolution.pdmodel"),
-    std::string("depthwise_conv2d_transpose_convolution/depthwise_conv2d_transpose_convolution.pdmodel"),
-    std::string("dropout/dropout.pdmodel"),
-    std::string("dropout_upscale_in_train/dropout_upscale_in_train.pdmodel"),
-    std::string("elementwise_add1/elementwise_add1.pdmodel"),
-    std::string("elementwise_div1/elementwise_div1.pdmodel"),
-    std::string("elementwise_max1/elementwise_max1.pdmodel"),
-    std::string("elementwise_min1/elementwise_min1.pdmodel"),
-    std::string("elementwise_mod1/elementwise_mod1.pdmodel"),
-    std::string("elementwise_mul1/elementwise_mul1.pdmodel"),
-    std::string("elementwise_pow1/elementwise_pow1.pdmodel"),
-    std::string("elementwise_sub1/elementwise_sub1.pdmodel"),
-    std::string("elementwise_add2/elementwise_add2.pdmodel"),
-    std::string("elementwise_div2/elementwise_div2.pdmodel"),
-    std::string("elementwise_max2/elementwise_max2.pdmodel"),
-    std::string("elementwise_min2/elementwise_min2.pdmodel"),
-    std::string("elementwise_mod2/elementwise_mod2.pdmodel"),
-    std::string("elementwise_mul2/elementwise_mul2.pdmodel"),
-    std::string("elementwise_pow2/elementwise_pow2.pdmodel"),
-    std::string("elementwise_sub2/elementwise_sub2.pdmodel"),
-    std::string("elementwise_add3/elementwise_add3.pdmodel"),
-    std::string("elementwise_div3/elementwise_div3.pdmodel"),
-    std::string("elementwise_max3/elementwise_max3.pdmodel"),
-    std::string("elementwise_min3/elementwise_min3.pdmodel"),
-    std::string("elementwise_mod3/elementwise_mod3.pdmodel"),
-    std::string("elementwise_mul3/elementwise_mul3.pdmodel"),
-    std::string("elementwise_pow3/elementwise_pow3.pdmodel"),
-    std::string("elementwise_sub3/elementwise_sub3.pdmodel"),
-    std::string("elementwise_add4/elementwise_add4.pdmodel"),
-    std::string("elementwise_div4/elementwise_div4.pdmodel"),
-    std::string("elementwise_max4/elementwise_max4.pdmodel"),
-    std::string("elementwise_min4/elementwise_min4.pdmodel"),
-    std::string("elementwise_mod4/elementwise_mod4.pdmodel"),
-    std::string("elementwise_mul4/elementwise_mul4.pdmodel"),
-    std::string("elementwise_pow4/elementwise_pow4.pdmodel"),
-    std::string("elementwise_sub4/elementwise_sub4.pdmodel"),
-    std::string("elementwise_floordiv_int32_1/elementwise_floordiv_int32_1.pdmodel"),
-    std::string("elementwise_floordiv_int32_2/elementwise_floordiv_int32_2.pdmodel"),
-    std::string("elementwise_floordiv_int32_3/elementwise_floordiv_int32_3.pdmodel"),
-    std::string("elementwise_floordiv_int64_1/elementwise_floordiv_int64_1.pdmodel"),
-    std::string("elementwise_floordiv_int64_2/elementwise_floordiv_int64_2.pdmodel"),
-    std::string("elementwise_floordiv_int64_3/elementwise_floordiv_int64_3.pdmodel"),
-    std::string("elementwise_mul_bool1/elementwise_mul_bool1.pdmodel"),
-    std::string("elu/elu.pdmodel"),
-    std::string("embedding_0/embedding_0.pdmodel"),
-    std::string("embedding_sparse/embedding_sparse.pdmodel"),
-    std::string("embedding_none_weight/embedding_none_weight.pdmodel"),
-    std::string("embedding_paddings/embedding_paddings.pdmodel"),
-    std::string("embedding_paddings_neg1/embedding_paddings_neg1.pdmodel"),
-    std::string("embedding_tensorIds/embedding_tensorIds.pdmodel"),
-    std::string("embedding_tensorIds_paddings/embedding_tensorIds_paddings.pdmodel"),
-    std::string("equal/equal.pdmodel"),
-    std::string("expand_v2/expand_v2.pdmodel"),
-    std::string("expand_v2_tensor/expand_v2_tensor.pdmodel"),
-    std::string("expand_v2_tensor_list/expand_v2_tensor_list.pdmodel"),
-    std::string("expand_v2_tensor_list2/expand_v2_tensor_list2.pdmodel"),
-    std::string("expand_as_v2_1/expand_as_v2_1.pdmodel"),
-    std::string("expand_as_v2_2/expand_as_v2_2.pdmodel"),
-    std::string("exp_test_float32/exp_test_float32.pdmodel"),
-    std::string("eye/eye.pdmodel"),
-    std::string("eye_int32/eye_int32.pdmodel"),
-    std::string("eye_int64/eye_int64.pdmodel"),
-    std::string("flip_1/flip_1.pdmodel"),
-    std::string("flip_2/flip_2.pdmodel"),
-    std::string("flip_3/flip_3.pdmodel"),
-    std::string("flip_4/flip_4.pdmodel"),
-    std::string("flip_5/flip_5.pdmodel"),
-    std::string("flip_dynamic_1/flip_dynamic_1.pdmodel"),
-    std::string("flip_dynamic_2/flip_dynamic_2.pdmodel"),
-    std::string("full_like/full_like.pdmodel"),
-    std::string("full_like_f16/full_like_f16.pdmodel"),
-    std::string("full_like_f32/full_like_f32.pdmodel"),
-    std::string("full_like_f64/full_like_f64.pdmodel"),
-    std::string("full_like_i16/full_like_i16.pdmodel"),
-    std::string("full_like_i32/full_like_i32.pdmodel"),
-    std::string("full_like_i64/full_like_i64.pdmodel"),
-    std::string("full_like_bool/full_like_bool.pdmodel"),
-    std::string("full_like_bool_2/full_like_bool_2.pdmodel"),
-    std::string("full/full.pdmodel"),
-    std::string("full_int32/full_int32.pdmodel"),
-    std::string("full_int64/full_int64.pdmodel"),
-    std::string("full_tensor/full_tensor.pdmodel"),
-    std::string("full_shape_tensor/full_shape_tensor.pdmodel"),
-    std::string("full_shape_tensor_list/full_shape_tensor_list.pdmodel"),
-    std::string("flatten_contiguous_range_test1/flatten_contiguous_range_test1.pdmodel"),
-    std::string("floor_float32/floor_float32.pdmodel"),
-    std::string("floor_mod1/floor_mod1.pdmodel"),
-    std::string("floor_mod2/floor_mod2.pdmodel"),
-    std::string("gather_multi_dimension/gather_multi_dimension.pdmodel"),
-    std::string("gather_one_dimension/gather_one_dimension.pdmodel"),
-    std::string("gather_one_dimension2/gather_one_dimension2.pdmodel"),
+    // std::string("deformable_conv_with_mask/deformable_conv_with_mask" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("deformable_conv_with_mask_bias/deformable_conv_with_mask_bias" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_pad/deformable_conv_with_pad" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_pad_list/deformable_conv_with_pad_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_pad_tuple/deformable_conv_with_pad_tuple" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_stride/deformable_conv_with_stride" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_stride_list/deformable_conv_with_stride_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("deformable_conv_with_stride_tuple/deformable_conv_with_stride_tuple" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("depthwise_conv2d_convolution/depthwise_conv2d_convolution" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("depthwise_conv2d_transpose_convolution/depthwise_conv2d_transpose_convolution" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("dropout/dropout" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("dropout_upscale_in_train/dropout_upscale_in_train" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_add1/elementwise_add1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_div1/elementwise_div1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_max1/elementwise_max1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_min1/elementwise_min1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mod1/elementwise_mod1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mul1/elementwise_mul1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_pow1/elementwise_pow1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_sub1/elementwise_sub1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_add2/elementwise_add2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_div2/elementwise_div2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_max2/elementwise_max2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_min2/elementwise_min2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mod2/elementwise_mod2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mul2/elementwise_mul2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_pow2/elementwise_pow2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_sub2/elementwise_sub2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_add3/elementwise_add3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_div3/elementwise_div3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_max3/elementwise_max3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_min3/elementwise_min3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mod3/elementwise_mod3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mul3/elementwise_mul3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_pow3/elementwise_pow3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_sub3/elementwise_sub3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_add4/elementwise_add4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_div4/elementwise_div4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_max4/elementwise_max4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_min4/elementwise_min4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mod4/elementwise_mod4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mul4/elementwise_mul4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_pow4/elementwise_pow4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_sub4/elementwise_sub4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int32_1/elementwise_floordiv_int32_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int32_2/elementwise_floordiv_int32_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int32_3/elementwise_floordiv_int32_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int64_1/elementwise_floordiv_int64_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int64_2/elementwise_floordiv_int64_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_floordiv_int64_3/elementwise_floordiv_int64_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elementwise_mul_bool1/elementwise_mul_bool1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("elu/elu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_0/embedding_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_sparse/embedding_sparse" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_none_weight/embedding_none_weight" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_paddings/embedding_paddings" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_paddings_neg1/embedding_paddings_neg1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_tensorIds/embedding_tensorIds" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("embedding_tensorIds_paddings/embedding_tensorIds_paddings" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("equal/equal" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_v2/expand_v2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_v2_tensor/expand_v2_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_v2_tensor_list/expand_v2_tensor_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_v2_tensor_list2/expand_v2_tensor_list2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_as_v2_1/expand_as_v2_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("expand_as_v2_2/expand_as_v2_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("exp_test_float32/exp_test_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("eye/eye" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("eye_int32/eye_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("eye_int64/eye_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_1/flip_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_2/flip_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_3/flip_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_4/flip_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_5/flip_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_dynamic_1/flip_dynamic_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flip_dynamic_2/flip_dynamic_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like/full_like" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_f16/full_like_f16" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_f32/full_like_f32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_f64/full_like_f64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_i16/full_like_i16" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_i32/full_like_i32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_i64/full_like_i64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_bool/full_like_bool" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_like_bool_2/full_like_bool_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full/full" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_int32/full_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_int64/full_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_tensor/full_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_shape_tensor/full_shape_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("full_shape_tensor_list/full_shape_tensor_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("flatten_contiguous_range_test1/flatten_contiguous_range_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("floor_float32/floor_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("floor_mod1/floor_mod1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("floor_mod2/floor_mod2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_multi_dimension/gather_multi_dimension" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_one_dimension/gather_one_dimension" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_one_dimension2/gather_one_dimension2" + std::string(TEST_PADDLE_MODEL_EXT)),
     // gather_axis_input
     // (CVS-82724: not support Axis as input),
-    std::string("gather_nd_float32/gather_nd_float32.pdmodel"),
-    std::string("gather_nd_int64/gather_nd_int64.pdmodel"),
-    std::string("gather_nd_int32/gather_nd_int32.pdmodel"),
+    std::string("gather_nd_float32/gather_nd_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_nd_int64/gather_nd_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_nd_int32/gather_nd_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
     // Not support indices zero rank
-    // std::string("gather_nd_empty/gather_nd_empty.pdmodel"),
-    std::string("gather_nd_low_index/gather_nd_low_index.pdmodel"),
-    std::string("gather_nd_high_rank1/gather_nd_high_rank1.pdmodel"),
-    std::string("gather_nd_high_rank2/gather_nd_high_rank2.pdmodel"),
-    std::string("gelu_erf/gelu_erf.pdmodel"),
-    std::string("gelu_tanh/gelu_tanh.pdmodel"),
-    std::string("generate_proposals_v2_0/generate_proposals_v2_0.pdmodel"),
-    std::string("generate_proposals_v2_1/generate_proposals_v2_1.pdmodel"),
-    std::string("generate_proposals_v2_2/generate_proposals_v2_2.pdmodel"),
-    std::string("generate_proposals_v2_3/generate_proposals_v2_3.pdmodel"),
-    std::string("generate_proposals_v2_4/generate_proposals_v2_4.pdmodel"),
+    // std::string("gather_nd_empty/gather_nd_empty" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_nd_low_index/gather_nd_low_index" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_nd_high_rank1/gather_nd_high_rank1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gather_nd_high_rank2/gather_nd_high_rank2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gelu_erf/gelu_erf" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("gelu_tanh/gelu_tanh" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("generate_proposals_v2_0/generate_proposals_v2_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("generate_proposals_v2_1/generate_proposals_v2_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("generate_proposals_v2_2/generate_proposals_v2_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("generate_proposals_v2_3/generate_proposals_v2_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("generate_proposals_v2_4/generate_proposals_v2_4" + std::string(TEST_PADDLE_MODEL_EXT)),
     // ticket 130605: actual res value is not close
-    // std::string("generate_proposals_v2_5/generate_proposals_v2_5.pdmodel"),
-    // std::string("generate_proposals_v2_6/generate_proposals_v2_6.pdmodel"),
+    // std::string("generate_proposals_v2_5/generate_proposals_v2_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("generate_proposals_v2_6/generate_proposals_v2_6" + std::string(TEST_PADDLE_MODEL_EXT)),
     // greater_equal_big_int64(failure due to CPU inference),
-    std::string("greater_equal_big_int64/greater_equal_big_int64.pdmodel"),
-    std::string("greater_equal_float32/greater_equal_float32.pdmodel"),
-    std::string("greater_equal_int32/greater_equal_int32.pdmodel"),
-    std::string("greater_equal_int64/greater_equal_int64.pdmodel"),
-    std::string("greater_than_float32/greater_than_float32.pdmodel"),
-    std::string("greater_than_int32/greater_than_int32.pdmodel"),
-    std::string("greater_than_int64/greater_than_int64.pdmodel"),
-    std::string("grid_sampler_1/grid_sampler_1.pdmodel"),
-    std::string("grid_sampler_2/grid_sampler_2.pdmodel"),
-    // std::string("grid_sampler_3/grid_sampler_3.pdmodel"),
-    // std::string("grid_sampler_dyn/grid_sampler_dyn.pdmodel"),
-    std::string("group_norm_1/group_norm_1.pdmodel"),
-    std::string("group_norm_2/group_norm_2.pdmodel"),
-    std::string("group_norm_3/group_norm_3.pdmodel"),
-    std::string("hard_sigmoid/hard_sigmoid.pdmodel"),
-    std::string("hard_swish/hard_swish.pdmodel"),
-    std::string("index_select_axis_0/index_select_axis_0.pdmodel"),
-    std::string("index_select_axis_1/index_select_axis_1.pdmodel"),
-    std::string("index_select_axis_native_-1/index_select_axis_native_-1.pdmodel"),
-    std::string("index_select_axis_native_-2/index_select_axis_native_-2.pdmodel"),
-    std::string("layer_norm/layer_norm.pdmodel"),
-    std::string("layer_norm_noall/layer_norm_noall.pdmodel"),
-    std::string("layer_norm_noscale/layer_norm_noscale.pdmodel"),
-    std::string("layer_norm_noshift/layer_norm_noshift.pdmodel"),
-    std::string("leaky_relu/leaky_relu.pdmodel"),
-    std::string("less_than_float32/less_than_float32.pdmodel"),
-    std::string("less_than_int32/less_than_int32.pdmodel"),
-    std::string("less_than_int64/less_than_int64.pdmodel"),
-    std::string("less_equal_float32/less_equal_float32.pdmodel"),
-    std::string("less_equal_int32/less_equal_int32.pdmodel"),
-    std::string("less_equal_int64/less_equal_int64.pdmodel"),
-    std::string("linear_downsample_false_0/linear_downsample_false_0.pdmodel"),
-    std::string("linear_downsample_false_1/linear_downsample_false_1.pdmodel"),
-    std::string("linear_downsample_true_0/linear_downsample_true_0.pdmodel"),
-    std::string("linear_upsample_false_0/linear_upsample_false_0.pdmodel"),
-    std::string("linear_upsample_false_1/linear_upsample_false_1.pdmodel"),
-    std::string("linear_upsample_scales/linear_upsample_scales.pdmodel"),
-    std::string("linear_upsample_scales2/linear_upsample_scales2.pdmodel"),
-    std::string("linear_upsample_true_0/linear_upsample_true_0.pdmodel"),
-    std::string("linear_upsample_tensor_size/linear_upsample_tensor_size.pdmodel"),
-    std::string("linspace_1/linspace_1.pdmodel"),
-    std::string("linspace_2/linspace_2.pdmodel"),
-    std::string("linspace_3/linspace_3.pdmodel"),
-    std::string("log/log.pdmodel"),
-    std::string("logical_and/logical_and.pdmodel"),
-    std::string("logical_not/logical_not.pdmodel"),
-    std::string("logical_or/logical_or.pdmodel"),
-    std::string("logical_xor/logical_xor.pdmodel"),
+    std::string("greater_equal_big_int64/greater_equal_big_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_equal_float32/greater_equal_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_equal_int32/greater_equal_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_equal_int64/greater_equal_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_than_float32/greater_than_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_than_int32/greater_than_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("greater_than_int64/greater_than_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("grid_sampler_1/grid_sampler_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("grid_sampler_2/grid_sampler_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("grid_sampler_3/grid_sampler_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("grid_sampler_dyn/grid_sampler_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("group_norm_1/group_norm_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("group_norm_2/group_norm_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("group_norm_3/group_norm_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("hard_sigmoid/hard_sigmoid" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("hard_swish/hard_swish" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("index_select_axis_0/index_select_axis_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("index_select_axis_1/index_select_axis_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("index_select_axis_native_-1/index_select_axis_native_-1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("index_select_axis_native_-2/index_select_axis_native_-2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("layer_norm/layer_norm" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("layer_norm_noall/layer_norm_noall" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("layer_norm_noscale/layer_norm_noscale" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("layer_norm_noshift/layer_norm_noshift" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("leaky_relu/leaky_relu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_than_float32/less_than_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_than_int32/less_than_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_than_int64/less_than_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_equal_float32/less_equal_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_equal_int32/less_equal_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("less_equal_int64/less_equal_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_downsample_false_0/linear_downsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_downsample_false_1/linear_downsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_downsample_true_0/linear_downsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_false_0/linear_upsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_false_1/linear_upsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_scales/linear_upsample_scales" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_scales2/linear_upsample_scales2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_true_0/linear_upsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linear_upsample_tensor_size/linear_upsample_tensor_size" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linspace_1/linspace_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linspace_2/linspace_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("linspace_3/linspace_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("log/log" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("logical_and/logical_and" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("logical_not/logical_not" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("logical_or/logical_or" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("logical_xor/logical_xor" + std::string(TEST_PADDLE_MODEL_EXT)),
     // 95436: sporadic failure
-    std::string("loop/loop.pdmodel"),
-    std::string("loop_dyn/loop_dyn.pdmodel"),
-    std::string("loop_dyn_x/loop_dyn_x.pdmodel"),
-    // std::string("loop_if/loop_if.pdmodel"),
-    // std::string("loop_if_loop/loop_if_loop.pdmodel"),
-    // std::string("loop_if_loop_if/loop_if_loop_if.pdmodel"),
-    // std::string("loop_if_loop_complex/loop_if_loop_complex.pdmodel"),
+    std::string("loop/loop" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("loop_dyn/loop_dyn" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("loop_dyn_x/loop_dyn_x" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("loop_if/loop_if" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("loop_if_loop/loop_if_loop" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("loop_if_loop_if/loop_if_loop_if" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("loop_if_loop_complex/loop_if_loop_complex" + std::string(TEST_PADDLE_MODEL_EXT)),
     // disabed due to slice could not produce full dynamic shape
-    // std::string("loop_if_tensor_array/loop_if_tensor_array.pdmodel"),
-    // std::string("loop_x/loop_x.pdmodel"),
-    std::string("loop_t/loop_t.pdmodel"),
-    std::string("loop_tensor_array/loop_tensor_array.pdmodel"),
-    std::string("matmul_xt/matmul_xt.pdmodel"),
-    std::string("matmul_xt_yt/matmul_xt_yt.pdmodel"),
-    std::string("matmul_yt/matmul_yt.pdmodel"),
-    std::string("matmul_v2_1dx1d/matmul_v2_1dx1d.pdmodel"),
-    std::string("matmul_v2_1dx2d/matmul_v2_1dx2d.pdmodel"),
-    std::string("matmul_v2_2dx1d/matmul_v2_2dx1d.pdmodel"),
-    std::string("matmul_v2_ndxmd/matmul_v2_ndxmd.pdmodel"),
-    std::string("matmul_v2_xt/matmul_v2_xt.pdmodel"),
-    std::string("matmul_v2_xt_yt/matmul_v2_xt_yt.pdmodel"),
-    std::string("matmul_v2_yt/matmul_v2_yt.pdmodel"),
-    std::string("matrix_nms_by_background/matrix_nms_by_background.pdmodel"),
-    std::string("matrix_nms_by_keep_top_k/matrix_nms_by_keep_top_k.pdmodel"),
-    std::string("matrix_nms_by_nms_top_k/matrix_nms_by_nms_top_k.pdmodel"),
-    std::string("matrix_nms_by_post_threshold/matrix_nms_by_post_threshold.pdmodel"),
-    std::string("matrix_nms_flipped_coordinates/matrix_nms_flipped_coordinates.pdmodel"),
-    std::string("matrix_nms_gaussian/matrix_nms_gaussian.pdmodel"),
-    std::string("matrix_nms_gaussian_sigma/matrix_nms_gaussian_sigma.pdmodel"),
-    std::string("matrix_nms_identical_boxes/matrix_nms_identical_boxes.pdmodel"),
-    std::string("matrix_nms_not_normalized/matrix_nms_not_normalized.pdmodel"),
-    std::string("matrix_nms_not_return_indexed/matrix_nms_not_return_indexed.pdmodel"),
-    std::string("matrix_nms_not_return_rois_num/matrix_nms_not_return_rois_num.pdmodel"),
-    std::string("matrix_nms_not_return_rois_num_neither_index/matrix_nms_not_return_rois_num_neither_index.pdmodel"),
-    std::string("matrix_nms_one_batch/matrix_nms_one_batch.pdmodel"),
-    std::string("matrix_nms_single_box/matrix_nms_single_box.pdmodel"),
-    std::string("matrix_nms_two_batches_two_classes/matrix_nms_two_batches_two_classes.pdmodel"),
-    // std::string("matrix_nms_normalized_random/matrix_nms_normalized_random.pdmodel"),
-    // std::string("matrix_nms_not_normalized_random/matrix_nms_not_normalized_random.pdmodel"),
-    std::string("maxAdaptivePool2D_test1/maxAdaptivePool2D_test1.pdmodel"),
-    std::string("maxPool_test1/maxPool_test1.pdmodel"),
-    std::string("maxPool_test10/maxPool_test10.pdmodel"),
-    std::string("maxPool_test11/maxPool_test11.pdmodel"),
-    std::string("maxPool_test2/maxPool_test2.pdmodel"),
-    std::string("maxPool_test3/maxPool_test3.pdmodel"),
-    std::string("maxPool_test4/maxPool_test4.pdmodel"),
-    std::string("maxPool_test5/maxPool_test5.pdmodel"),
+    // std::string("loop_if_tensor_array/loop_if_tensor_array" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("loop_x/loop_x" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("loop_t/loop_t" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("loop_tensor_array/loop_tensor_array" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_xt/matmul_xt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_xt_yt/matmul_xt_yt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_yt/matmul_yt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_1dx1d/matmul_v2_1dx1d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_1dx2d/matmul_v2_1dx2d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_2dx1d/matmul_v2_2dx1d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_ndxmd/matmul_v2_ndxmd" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_xt/matmul_v2_xt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_xt_yt/matmul_v2_xt_yt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matmul_v2_yt/matmul_v2_yt" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_by_background/matrix_nms_by_background" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_by_keep_top_k/matrix_nms_by_keep_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_by_nms_top_k/matrix_nms_by_nms_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_by_post_threshold/matrix_nms_by_post_threshold" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_flipped_coordinates/matrix_nms_flipped_coordinates" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_gaussian/matrix_nms_gaussian" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_gaussian_sigma/matrix_nms_gaussian_sigma" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_identical_boxes/matrix_nms_identical_boxes" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_not_normalized/matrix_nms_not_normalized" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_not_return_indexed/matrix_nms_not_return_indexed" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_not_return_rois_num/matrix_nms_not_return_rois_num" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_not_return_rois_num_neither_index/matrix_nms_not_return_rois_num_neither_index" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_one_batch/matrix_nms_one_batch" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_single_box/matrix_nms_single_box" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("matrix_nms_two_batches_two_classes/matrix_nms_two_batches_two_classes" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("matrix_nms_normalized_random/matrix_nms_normalized_random" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("matrix_nms_not_normalized_random/matrix_nms_not_normalized_random" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxAdaptivePool2D_test1/maxAdaptivePool2D_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test1/maxPool_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test10/maxPool_test10" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test11/maxPool_test11" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test2/maxPool_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test3/maxPool_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test4/maxPool_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test5/maxPool_test5" + std::string(TEST_PADDLE_MODEL_EXT)),
     // maxPool_test6(nchw support is disabled now),
-    std::string("maxPool_test7/maxPool_test7.pdmodel"),
-    std::string("maxPool_test8/maxPool_test8.pdmodel"),
-    std::string("maxPool_test9/maxPool_test9.pdmodel"),
-    std::string("maxAdaptivePool3D_test1/maxAdaptivePool3D_test1.pdmodel"),
-    std::string("maxAdaptivePool3D_test2/maxAdaptivePool3D_test2.pdmodel"),
-    std::string("maxAdaptivePool3D_test3/maxAdaptivePool3D_test3.pdmodel"),
-    std::string("maxAdaptivePool3D_test4/maxAdaptivePool3D_test4.pdmodel"),
-    std::string("max3dPool_test1/max3dPool_test1.pdmodel"),
-    std::string("max3dPool_test2/max3dPool_test2.pdmodel"),
-    std::string("max3dPool_test3/max3dPool_test3.pdmodel"),
-    std::string("max3dPool_test4/max3dPool_test4.pdmodel"),
-    std::string("max3dPool_test5/max3dPool_test5.pdmodel"),
-    std::string("max3dPool_test6/max3dPool_test6.pdmodel"),
-    std::string("max3dPool_test7/max3dPool_test7.pdmodel"),
-    std::string("max3dPool_test8/max3dPool_test8.pdmodel"),
-    std::string("max3dPool_test9/max3dPool_test9.pdmodel"),
-    std::string("max3dPool_test10/max3dPool_test10.pdmodel"),
-    std::string("max3dRetureMask/max3dRetureMask.pdmodel"),
-    std::string("meshgrid/meshgrid.pdmodel"),
-    std::string("multiclass_nms_by_background/multiclass_nms_by_background.pdmodel"),
-    std::string("multiclass_nms_by_class_id/multiclass_nms_by_class_id.pdmodel"),
-    std::string("multiclass_nms_by_IOU/multiclass_nms_by_IOU.pdmodel"),
-    std::string("multiclass_nms_by_IOU_and_scores/multiclass_nms_by_IOU_and_scores.pdmodel"),
-    std::string("multiclass_nms_by_keep_top_k/multiclass_nms_by_keep_top_k.pdmodel"),
-    std::string("multiclass_nms_by_nms_eta/multiclass_nms_by_nms_eta.pdmodel"),
-    std::string("multiclass_nms_by_nms_top_k/multiclass_nms_by_nms_top_k.pdmodel"),
-    std::string("multiclass_nms_flipped_coordinates/multiclass_nms_flipped_coordinates.pdmodel"),
-    std::string("multiclass_nms_identical_boxes/multiclass_nms_identical_boxes.pdmodel"),
+    std::string("maxPool_test7/maxPool_test7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test8/maxPool_test8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxPool_test9/maxPool_test9" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxAdaptivePool3D_test1/maxAdaptivePool3D_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxAdaptivePool3D_test2/maxAdaptivePool3D_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxAdaptivePool3D_test3/maxAdaptivePool3D_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("maxAdaptivePool3D_test4/maxAdaptivePool3D_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test1/max3dPool_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test2/max3dPool_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test3/max3dPool_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test4/max3dPool_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test5/max3dPool_test5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test6/max3dPool_test6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test7/max3dPool_test7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test8/max3dPool_test8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test9/max3dPool_test9" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dPool_test10/max3dPool_test10" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("max3dRetureMask/max3dRetureMask" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("meshgrid/meshgrid" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_background/multiclass_nms_by_background" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_class_id/multiclass_nms_by_class_id" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_IOU/multiclass_nms_by_IOU" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_IOU_and_scores/multiclass_nms_by_IOU_and_scores" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_keep_top_k/multiclass_nms_by_keep_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_nms_eta/multiclass_nms_by_nms_eta" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_by_nms_top_k/multiclass_nms_by_nms_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_flipped_coordinates/multiclass_nms_flipped_coordinates" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_identical_boxes/multiclass_nms_identical_boxes" + std::string(TEST_PADDLE_MODEL_EXT)),
     // default
     std::string("multiclass_nms_lod_roisnum_multiple_images_default/"
-                "multiclass_nms_lod_roisnum_multiple_images_default.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_default" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_default/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_default.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_default" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string(
-        "multiclass_nms_lod_roisnum_single_image_default/multiclass_nms_lod_roisnum_single_image_default.pdmodel"),
+        "multiclass_nms_lod_roisnum_single_image_default/multiclass_nms_lod_roisnum_single_image_default" + std::string(TEST_PADDLE_MODEL_EXT)),
     // background
     std::string("multiclass_nms_lod_roisnum_multiple_images_background/"
-                "multiclass_nms_lod_roisnum_multiple_images_background.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_background" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_background/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_background.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_background" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_single_image_background/"
-                "multiclass_nms_lod_roisnum_single_image_background.pdmodel"),
+                "multiclass_nms_lod_roisnum_single_image_background" + std::string(TEST_PADDLE_MODEL_EXT)),
     // score_threshold
     std::string("multiclass_nms_lod_roisnum_multiple_images_score_threshold/"
-                "multiclass_nms_lod_roisnum_multiple_images_score_threshold.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_score_threshold" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_score_threshold/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_score_threshold.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_score_threshold" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_single_image_score_threshold/"
-                "multiclass_nms_lod_roisnum_single_image_score_threshold.pdmodel"),
+                "multiclass_nms_lod_roisnum_single_image_score_threshold" + std::string(TEST_PADDLE_MODEL_EXT)),
     // nms_top_k
     std::string("multiclass_nms_lod_roisnum_multiple_images_nms_top_k/"
-                "multiclass_nms_lod_roisnum_multiple_images_nms_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_nms_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_nms_top_k/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_nms_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_nms_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_single_image_nms_top_k/"
-                "multiclass_nms_lod_roisnum_single_image_nms_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_single_image_nms_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     // keep_top_k
     std::string("multiclass_nms_lod_roisnum_multiple_images_keep_top_k/"
-                "multiclass_nms_lod_roisnum_multiple_images_keep_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_keep_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_keep_top_k/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_keep_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_keep_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_single_image_keep_top_k/"
-                "multiclass_nms_lod_roisnum_single_image_keep_top_k.pdmodel"),
+                "multiclass_nms_lod_roisnum_single_image_keep_top_k" + std::string(TEST_PADDLE_MODEL_EXT)),
     // normalized
     std::string("multiclass_nms_lod_roisnum_multiple_images_normalized/"
-                "multiclass_nms_lod_roisnum_multiple_images_normalized.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_normalized" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_multiple_images_0_normalized/"
-                "multiclass_nms_lod_roisnum_multiple_images_0_normalized.pdmodel"),
+                "multiclass_nms_lod_roisnum_multiple_images_0_normalized" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_lod_roisnum_single_image_normalized/"
-                "multiclass_nms_lod_roisnum_single_image_normalized.pdmodel"),
-    std::string("multiclass_nms_not_normalized/multiclass_nms_not_normalized.pdmodel"),
-    std::string("multiclass_nms_not_return_indexed/multiclass_nms_not_return_indexed.pdmodel"),
-    std::string("multiclass_nms_single_box/multiclass_nms_single_box.pdmodel"),
+                "multiclass_nms_lod_roisnum_single_image_normalized" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_not_normalized/multiclass_nms_not_normalized" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_not_return_indexed/multiclass_nms_not_return_indexed" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("multiclass_nms_single_box/multiclass_nms_single_box" + std::string(TEST_PADDLE_MODEL_EXT)),
     std::string("multiclass_nms_two_batches_two_classes_by_class_id/"
-                "multiclass_nms_two_batches_two_classes_by_class_id.pdmodel"),
-    // std::string("multiclass_nms_normalized_random/multiclass_nms_normalized_random.pdmodel"),
-    // std::string("multiclass_nms_not_normalized_random/multiclass_nms_not_normalized_random.pdmodel"),
-    std::string("nearest_downsample_false_0/nearest_downsample_false_0.pdmodel"),
-    std::string("nearest_downsample_false_1/nearest_downsample_false_1.pdmodel"),
-    std::string("nearest_upsample_false_0/nearest_upsample_false_0.pdmodel"),
-    std::string("nearest_upsample_false_1/nearest_upsample_false_1.pdmodel"),
-    std::string("nearest_upsample_tensor_size/nearest_upsample_tensor_size.pdmodel"),
-    std::string("not_equal_float32/not_equal_float32.pdmodel"),
-    std::string("not_equal_int32/not_equal_int32.pdmodel"),
-    std::string("not_equal_int64/not_equal_int64.pdmodel"),
-    std::string("one_hot_v2_1/one_hot_v2_1.pdmodel"),
-    std::string("one_hot_v2_2/one_hot_v2_2.pdmodel"),
-    std::string("one_hot_v2_3/one_hot_v2_3.pdmodel"),
-    std::string("p_norm1/p_norm1.pdmodel"),
-    std::string("p_norm2/p_norm2.pdmodel"),
-    std::string("p_norm3/p_norm3.pdmodel"),
-    std::string("p_norm4/p_norm4.pdmodel"),
-    std::string("p_norm5/p_norm5.pdmodel"),
-    std::string("p_norm6/p_norm6.pdmodel"),
-    std::string("p_norm7/p_norm7.pdmodel"),
-    std::string("p_norm8/p_norm8.pdmodel"),
-    std::string("pad3d_test1/pad3d_test1.pdmodel"),
-    std::string("pad3d_test2/pad3d_test2.pdmodel"),
-    std::string("pad3d_test3/pad3d_test3.pdmodel"),
-    std::string("partial_concat_1/partial_concat_1.pdmodel"),
-    std::string("partial_concat_2/partial_concat_2.pdmodel"),
-    std::string("partial_concat_3/partial_concat_3.pdmodel"),
-    std::string("partial_sum_1/partial_sum_1.pdmodel"),
-    std::string("partial_sum_2/partial_sum_2.pdmodel"),
-    std::string("partial_sum_3/partial_sum_3.pdmodel"),
+                "multiclass_nms_two_batches_two_classes_by_class_id" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("multiclass_nms_normalized_random/multiclass_nms_normalized_random" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("multiclass_nms_not_normalized_random/multiclass_nms_not_normalized_random" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("nearest_downsample_false_0/nearest_downsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("nearest_downsample_false_1/nearest_downsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("nearest_upsample_false_0/nearest_upsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("nearest_upsample_false_1/nearest_upsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("nearest_upsample_tensor_size/nearest_upsample_tensor_size" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("not_equal_float32/not_equal_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("not_equal_int32/not_equal_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("not_equal_int64/not_equal_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("one_hot_v2_1/one_hot_v2_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("one_hot_v2_2/one_hot_v2_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("one_hot_v2_3/one_hot_v2_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm1/p_norm1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm2/p_norm2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm3/p_norm3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm4/p_norm4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm5/p_norm5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm6/p_norm6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm7/p_norm7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("p_norm8/p_norm8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pad3d_test1/pad3d_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pad3d_test2/pad3d_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pad3d_test3/pad3d_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_concat_1/partial_concat_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_concat_2/partial_concat_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_concat_3/partial_concat_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_sum_1/partial_sum_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_sum_2/partial_sum_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("partial_sum_3/partial_sum_3" + std::string(TEST_PADDLE_MODEL_EXT)),
     // pad3d_test4,
-    std::string("pow_float32/pow_float32.pdmodel"),
-    std::string("pow_int32/pow_int32.pdmodel"),
-    std::string("pow_int64/pow_int64.pdmodel"),
+    std::string("pow_float32/pow_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pow_int32/pow_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("pow_int64/pow_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
     // pow_int64_out_of_range(out of range of OV int64),
-    std::string("pow_y_tensor/pow_y_tensor.pdmodel"),
-    std::string("prior_box_attrs_mmar_order_true/prior_box_attrs_mmar_order_true.pdmodel"),
-    std::string("prior_box_default/prior_box_default.pdmodel"),
-    std::string("prior_box_flip_clip_false/prior_box_flip_clip_false.pdmodel"),
-    std::string("prior_box_max_sizes_none/prior_box_max_sizes_none.pdmodel"),
-    std::string("range0/range0.pdmodel"),
-    std::string("range1/range1.pdmodel"),
-    std::string("range2/range2.pdmodel"),
-    std::string("reduce_all_test_0/reduce_all_test_0.pdmodel"),
-    std::string("reduce_all_test_1/reduce_all_test_1.pdmodel"),
-    std::string("reduce_all_test_2/reduce_all_test_2.pdmodel"),
-    std::string("reduce_all_test_3/reduce_all_test_3.pdmodel"),
-    std::string("reduce_all_test_4/reduce_all_test_4.pdmodel"),
-    std::string("reduce_all_test_5/reduce_all_test_5.pdmodel"),
-    std::string("reduce_max_test_0/reduce_max_test_0.pdmodel"),
-    std::string("reduce_max_test_1/reduce_max_test_1.pdmodel"),
-    std::string("reduce_max_test_2/reduce_max_test_2.pdmodel"),
-    std::string("reduce_max_test_3/reduce_max_test_3.pdmodel"),
-    std::string("reduce_max_test_4/reduce_max_test_4.pdmodel"),
-    std::string("reduce_max_test_5/reduce_max_test_5.pdmodel"),
-    std::string("reduce_mean_test_0/reduce_mean_test_0.pdmodel"),
-    std::string("reduce_mean_test_1/reduce_mean_test_1.pdmodel"),
-    std::string("reduce_mean_test_2/reduce_mean_test_2.pdmodel"),
-    std::string("reduce_mean_test_3/reduce_mean_test_3.pdmodel"),
-    std::string("reduce_mean_test_4/reduce_mean_test_4.pdmodel"),
-    std::string("reduce_mean_test_5/reduce_mean_test_5.pdmodel"),
-    std::string("reduce_min_test_0/reduce_min_test_0.pdmodel"),
-    std::string("reduce_min_test_1/reduce_min_test_1.pdmodel"),
-    std::string("reduce_min_test_2/reduce_min_test_2.pdmodel"),
-    std::string("reduce_min_test_3/reduce_min_test_3.pdmodel"),
-    std::string("reduce_min_test_4/reduce_min_test_4.pdmodel"),
-    std::string("reduce_min_test_5/reduce_min_test_5.pdmodel"),
-    std::string("reduce_prod_test_0/reduce_prod_test_0.pdmodel"),
-    std::string("reduce_prod_test_1/reduce_prod_test_1.pdmodel"),
-    std::string("reduce_prod_test_2/reduce_prod_test_2.pdmodel"),
-    std::string("reduce_prod_test_3/reduce_prod_test_3.pdmodel"),
-    std::string("reduce_prod_test_4/reduce_prod_test_4.pdmodel"),
-    std::string("reduce_prod_test_5/reduce_prod_test_5.pdmodel"),
-    std::string("reduce_sum_test_0/reduce_sum_test_0.pdmodel"),
-    std::string("reduce_sum_test_1/reduce_sum_test_1.pdmodel"),
-    std::string("reduce_sum_test_2/reduce_sum_test_2.pdmodel"),
-    std::string("reduce_sum_test_3/reduce_sum_test_3.pdmodel"),
-    std::string("reduce_sum_test_4/reduce_sum_test_4.pdmodel"),
-    std::string("reduce_sum_test_5/reduce_sum_test_5.pdmodel"),
-    std::string("relu/relu.pdmodel"),
-    std::string("relu6/relu6.pdmodel"),
-    std::string("relu6_1/relu6_1.pdmodel"),
-    std::string("reshape/reshape.pdmodel"),
-    std::string("reshape_tensor/reshape_tensor.pdmodel"),
-    std::string("reshape_tensor_list/reshape_tensor_list.pdmodel"),
-    std::string("rnn_lstm_layer_1_bidirectional/rnn_lstm_layer_1_bidirectional.pdmodel"),
-    std::string("rnn_lstm_layer_1_forward/rnn_lstm_layer_1_forward.pdmodel"),
-    std::string("rnn_lstm_layer_2_bidirectional/rnn_lstm_layer_2_bidirectional.pdmodel"),
-    std::string("rnn_lstm_layer_2_forward/rnn_lstm_layer_2_forward.pdmodel"),
-    std::string("rnn_lstm_layer_1_forward_seq_len_4/rnn_lstm_layer_1_forward_seq_len_4.pdmodel"),
-    std::string("rnn_lstm_layer_2_bidirectional_seq_len_4/rnn_lstm_layer_2_bidirectional_seq_len_4.pdmodel"),
-    std::string("roi_align_test/roi_align_test.pdmodel"),
-    std::string("roi_align_test2/roi_align_test2.pdmodel"),
-    std::string("roll_test_0/roll_test_0.pdmodel"),
-    std::string("roll_test_1/roll_test_1.pdmodel"),
-    std::string("roll_test_2/roll_test_2.pdmodel"),
-    std::string("roll_test_3/roll_test_3.pdmodel"),
-    std::string("roll_test_4/roll_test_4.pdmodel"),
-    std::string("round/round.pdmodel"),
-    std::string("scale_bias_after_float32/scale_bias_after_float32.pdmodel"),
-    std::string("scale_bias_after_int32/scale_bias_after_int32.pdmodel"),
-    std::string("scale_bias_after_int64/scale_bias_after_int64.pdmodel"),
-    std::string("scale_bias_before_float32/scale_bias_before_float32.pdmodel"),
-    std::string("scale_bias_before_int32/scale_bias_before_int32.pdmodel"),
-    std::string("scale_bias_before_int64/scale_bias_before_int64.pdmodel"),
-    std::string("scale_tensor_bias_after/scale_tensor_bias_after.pdmodel"),
-    std::string("scale_tensor_bias_before/scale_tensor_bias_before.pdmodel"),
-    std::string("set_value1/set_value1.pdmodel"),
-    std::string("set_value2/set_value2.pdmodel"),
-    std::string("set_value3/set_value3.pdmodel"),
-    std::string("set_value4/set_value4.pdmodel"),
-    std::string("set_value5/set_value5.pdmodel"),
-    // std::string("set_value6/set_value6.pdmodel"),
-    // std::string("set_value7/set_value7.pdmodel"),
-    // std::string("set_value8/set_value8.pdmodel"),
-    // std::string("set_value_dynamic1/set_value_dynamic1.pdmodel"),
-    std::string("set_value_dynamic2/set_value_dynamic2.pdmodel"),
-    std::string("shape/shape.pdmodel"),
-    std::string("share_data_test_0/share_data_test_0.pdmodel"),
-    std::string("sigmoid/sigmoid.pdmodel"),
-    std::string("silu_static_test1/silu_static_test1.pdmodel"),
-    std::string("silu_static_test2/silu_static_test2.pdmodel"),
-    std::string("silu_static_test3/silu_static_test3.pdmodel"),
-    std::string("silu_static_test4/silu_static_test4.pdmodel"),
-    std::string("silu_dynamic_test1/silu_dynamic_test1.pdmodel"),
-    std::string("silu_dynamic_test2/silu_dynamic_test2.pdmodel"),
-    std::string("silu_dynamic_test3/silu_dynamic_test3.pdmodel"),
-    std::string("silu_dynamic_test4/silu_dynamic_test4.pdmodel"),
-    std::string("sin/sin.pdmodel"),
-    std::string("slice/slice.pdmodel"),
-    std::string("slice_1d/slice_1d.pdmodel"),
-    std::string("slice_decrease_axis/slice_decrease_axis.pdmodel"),
-    std::string("slice_decrease_axis_all/slice_decrease_axis_all.pdmodel"),
-    std::string("slice_reshape/slice_reshape.pdmodel"),
-    std::string("softmax/softmax.pdmodel"),
-    std::string("softmax_minus/softmax_minus.pdmodel"),
-    std::string("softplus_default_params/softplus_default_params.pdmodel"),
-    std::string("softshrink_default_params/softshrink_default_params.pdmodel"),
-    std::string("softshrink_threshold_0.6/softshrink_threshold_0.6.pdmodel"),
-    std::string("split_test1/split_test1.pdmodel"),
-    std::string("split_test2/split_test2.pdmodel"),
-    std::string("split_test3/split_test3.pdmodel"),
-    std::string("split_test4/split_test4.pdmodel"),
-    std::string("split_test5/split_test5.pdmodel"),
-    std::string("split_test6/split_test6.pdmodel"),
-    std::string("split_test_dim_int32/split_test_dim_int32.pdmodel"),
-    std::string("split_test_dim_int64/split_test_dim_int64.pdmodel"),
-    std::string("split_test_list/split_test_list.pdmodel"),
-    std::string("split_test_list_tensor/split_test_list_tensor.pdmodel"),
-    std::string("sqrt_float32/sqrt_float32.pdmodel"),
-    std::string("squeeze/squeeze.pdmodel"),
-    std::string("squeeze_null_axes/squeeze_null_axes.pdmodel"),
-    std::string("stack_test_float32/stack_test_float32.pdmodel"),
-    std::string("stack_test_int32/stack_test_int32.pdmodel"),
-    std::string("stack_test_neg_axis/stack_test_neg_axis.pdmodel"),
-    std::string("stack_test_none_axis/stack_test_none_axis.pdmodel"),
-    std::string("strided_slice_input1_1/strided_slice_input1_1.pdmodel"),
-    std::string("strided_slice_input1_2/strided_slice_input1_2.pdmodel"),
-    std::string("strided_slice_input1_3/strided_slice_input1_3.pdmodel"),
-    std::string("strided_slice_input1_4/strided_slice_input1_4.pdmodel"),
-    std::string("strided_slice_input2_1/strided_slice_input2_1.pdmodel"),
-    std::string("strided_slice_input2_2/strided_slice_input2_2.pdmodel"),
-    std::string("strided_slice_input2_3/strided_slice_input2_3.pdmodel"),
-    std::string("strided_slice_input3_1/strided_slice_input3_1.pdmodel"),
-    std::string("strided_slice_input3_2/strided_slice_input3_2.pdmodel"),
-    std::string("sum_1/sum_1.pdmodel"),
-    std::string("sum_2/sum_2.pdmodel"),
-    std::string("sum_3/sum_3.pdmodel"),
-    std::string("sum_4/sum_4.pdmodel"),
-    std::string("swish_default_params/swish_default_params.pdmodel"),
-    std::string("swish_beta/swish_beta.pdmodel"),
-    std::string("tanh/tanh.pdmodel"),
-    std::string("tanh_shrink_1/tanh_shrink_1.pdmodel"),
-    std::string("tanh_shrink_2/tanh_shrink_2.pdmodel"),
-    std::string("tile_repeat_times_tensor/tile_repeat_times_tensor.pdmodel"),
-    std::string("tile_list_float32/tile_list_float32.pdmodel"),
-    std::string("tile_list_int32/tile_list_int32.pdmodel"),
-    std::string("tile_list_int64/tile_list_int64.pdmodel"),
-    std::string("tile_list_bool/tile_list_bool.pdmodel"),
-    std::string("tile_tuple_float32/tile_tuple_float32.pdmodel"),
-    std::string("tile_tuple_int32/tile_tuple_int32.pdmodel"),
-    std::string("tile_tuple_int64/tile_tuple_int64.pdmodel"),
-    std::string("tile_tuple_bool/tile_tuple_bool.pdmodel"),
-    std::string("tile_tensor_list/tile_tensor_list.pdmodel"),
-    std::string("tile_repeat_gt_x/tile_repeat_gt_x.pdmodel"),
-    std::string("tile_repeat_lt_x/tile_repeat_lt_x.pdmodel"),
-    std::string("top_k_v2_test_1/top_k_v2_test_1.pdmodel"),
-    std::string("top_k_v2_test_2/top_k_v2_test_2.pdmodel"),
-    std::string("top_k_v2_test_3/top_k_v2_test_3.pdmodel"),
-    std::string("top_k_v2_test_4/top_k_v2_test_4.pdmodel"),
-    std::string("top_k_v2_test_5/top_k_v2_test_5.pdmodel"),
-    std::string("top_k_v2_test_6/top_k_v2_test_6.pdmodel"),
-    std::string("triu/triu.pdmodel"),
-    std::string("triu_1/triu_1.pdmodel"),
-    std::string("triu_2/triu_2.pdmodel"),
-    std::string("triu_3/triu_3.pdmodel"),
-    std::string("triu_4/triu_4.pdmodel"),
-    std::string("triu_int32/triu_int32.pdmodel"),
-    std::string("triu_int64/triu_int64.pdmodel"),
-    std::string("trilinear_downsample_false_0/trilinear_downsample_false_0.pdmodel"),
-    std::string("trilinear_downsample_false_1/trilinear_downsample_false_1.pdmodel"),
-    std::string("trilinear_downsample_true_0/trilinear_downsample_true_0.pdmodel"),
-    std::string("trilinear_upsample_false_0/trilinear_upsample_false_0.pdmodel"),
-    std::string("trilinear_upsample_false_1/trilinear_upsample_false_1.pdmodel"),
-    std::string("trilinear_upsample_scales/trilinear_upsample_scales.pdmodel"),
-    std::string("trilinear_upsample_scales2/trilinear_upsample_scales2.pdmodel"),
-    std::string("trilinear_upsample_true_0/trilinear_upsample_true_0.pdmodel"),
-    std::string("trilinear_upsample_tensor_size/trilinear_upsample_tensor_size.pdmodel"),
-    std::string("tril/tril.pdmodel"),
-    std::string("tril_1/tril_1.pdmodel"),
-    std::string("tril_2/tril_2.pdmodel"),
-    std::string("tril_3/tril_3.pdmodel"),
-    std::string("tril_4/tril_4.pdmodel"),
-    std::string("tril_int32/tril_int32.pdmodel"),
-    std::string("tril_int64/tril_int64.pdmodel"),
-    std::string("unsqueeze/unsqueeze.pdmodel"),
-    std::string("unique/unique.pdmodel"),
-    std::string("unique_ret_index/unique_ret_index.pdmodel"),
-    std::string("unique_ret_inverse/unique_ret_inverse.pdmodel"),
-    std::string("unique_ret_counts/unique_ret_counts.pdmodel"),
-    std::string("unique_ret_index_inverse/unique_ret_index_inverse.pdmodel"),
-    std::string("unique_ret_index_counts/unique_ret_index_counts.pdmodel"),
-    std::string("unique_ret_inverse_counts/unique_ret_inverse_counts.pdmodel"),
-    std::string("unique_ret_index_inverse_counts/unique_ret_index_inverse_counts.pdmodel"),
-    std::string("unique_ret_index_axis/unique_ret_index_axis.pdmodel"),
-    std::string("unique_ret_index_i32/unique_ret_index_i32.pdmodel"),
-    std::string("unstack_1/unstack_1.pdmodel"),
-    std::string("unstack_2/unstack_2.pdmodel"),
-    std::string("unstack_3/unstack_3.pdmodel"),
-    std::string("unstack_4/unstack_4.pdmodel"),
-    std::string("unstack_5/unstack_5.pdmodel"),
-    std::string("where_1/where_1.pdmodel"),
-    std::string("where_2/where_2.pdmodel"),
-    std::string("where_3/where_3.pdmodel"),
-    std::string("where_index_1/where_index_1.pdmodel"),
-    std::string("where_index_2/where_index_2.pdmodel"),
-    std::string("where_index_3/where_index_3.pdmodel"),
-    std::string("where_index_4/where_index_4.pdmodel"),
-    std::string("where_index_5/where_index_5.pdmodel"),
-    std::string("where_index_6/where_index_6.pdmodel"),
-    std::string("abs_float32/abs_float32.pdmodel"),
-    std::string("atan2/atan2.pdmodel"),
-    std::string("reduce_any_test_0/reduce_any_test_0.pdmodel"),
-    std::string("reduce_any_test_1/reduce_any_test_1.pdmodel"),
-    std::string("reduce_any_test_2/reduce_any_test_2.pdmodel"),
-    std::string("reduce_any_test_3/reduce_any_test_3.pdmodel"),
-    std::string("reduce_any_test_4/reduce_any_test_4.pdmodel"),
-    std::string("scatter_test_1/scatter_test_1.pdmodel"),
-    std::string("scatter_test_2/scatter_test_2.pdmodel"),
-    std::string("scatter_nd_add_test_1/scatter_nd_add_test_1.pdmodel"),
+    std::string("pow_y_tensor/pow_y_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("prior_box_attrs_mmar_order_true/prior_box_attrs_mmar_order_true" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("prior_box_default/prior_box_default" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("prior_box_flip_clip_false/prior_box_flip_clip_false" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("prior_box_max_sizes_none/prior_box_max_sizes_none" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("range0/range0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("range1/range1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("range2/range2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_0/reduce_all_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_1/reduce_all_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_2/reduce_all_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_3/reduce_all_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_4/reduce_all_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_all_test_5/reduce_all_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_0/reduce_max_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_1/reduce_max_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_2/reduce_max_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_3/reduce_max_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_4/reduce_max_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_max_test_5/reduce_max_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_0/reduce_mean_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_1/reduce_mean_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_2/reduce_mean_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_3/reduce_mean_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_4/reduce_mean_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_mean_test_5/reduce_mean_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_0/reduce_min_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_1/reduce_min_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_2/reduce_min_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_3/reduce_min_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_4/reduce_min_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_min_test_5/reduce_min_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_0/reduce_prod_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_1/reduce_prod_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_2/reduce_prod_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_3/reduce_prod_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_4/reduce_prod_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_prod_test_5/reduce_prod_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_0/reduce_sum_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_1/reduce_sum_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_2/reduce_sum_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_3/reduce_sum_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_4/reduce_sum_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_sum_test_5/reduce_sum_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("relu/relu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("relu6/relu6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("relu6_1/relu6_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reshape/reshape" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reshape_tensor/reshape_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reshape_tensor_list/reshape_tensor_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_1_bidirectional/rnn_lstm_layer_1_bidirectional" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_1_forward/rnn_lstm_layer_1_forward" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_2_bidirectional/rnn_lstm_layer_2_bidirectional" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_2_forward/rnn_lstm_layer_2_forward" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_1_forward_seq_len_4/rnn_lstm_layer_1_forward_seq_len_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("rnn_lstm_layer_2_bidirectional_seq_len_4/rnn_lstm_layer_2_bidirectional_seq_len_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roi_align_test/roi_align_test" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roi_align_test2/roi_align_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roll_test_0/roll_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roll_test_1/roll_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roll_test_2/roll_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roll_test_3/roll_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("roll_test_4/roll_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("round/round" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_after_float32/scale_bias_after_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_after_int32/scale_bias_after_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_after_int64/scale_bias_after_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_before_float32/scale_bias_before_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_before_int32/scale_bias_before_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_bias_before_int64/scale_bias_before_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_tensor_bias_after/scale_tensor_bias_after" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scale_tensor_bias_before/scale_tensor_bias_before" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value1/set_value1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value2/set_value2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value3/set_value3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value4/set_value4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value5/set_value5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("set_value6/set_value6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("set_value7/set_value7" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("set_value8/set_value8" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("set_value_dynamic1/set_value_dynamic1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("set_value_dynamic2/set_value_dynamic2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("shape/shape" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("share_data_test_0/share_data_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sigmoid/sigmoid" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_static_test1/silu_static_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_static_test2/silu_static_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_static_test3/silu_static_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_static_test4/silu_static_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_dynamic_test1/silu_dynamic_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_dynamic_test2/silu_dynamic_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_dynamic_test3/silu_dynamic_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("silu_dynamic_test4/silu_dynamic_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sin/sin" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("slice/slice" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("slice_1d/slice_1d" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("slice_decrease_axis/slice_decrease_axis" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("slice_decrease_axis_all/slice_decrease_axis_all" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("slice_reshape/slice_reshape" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("softmax/softmax" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("softmax_minus/softmax_minus" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("softplus_default_params/softplus_default_params" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("softshrink_default_params/softshrink_default_params" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("softshrink_threshold_0.6/softshrink_threshold_0.6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test1/split_test1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test2/split_test2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test3/split_test3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test4/split_test4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test5/split_test5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test6/split_test6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test_dim_int32/split_test_dim_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test_dim_int64/split_test_dim_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test_list/split_test_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("split_test_list_tensor/split_test_list_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sqrt_float32/sqrt_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("squeeze/squeeze" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("squeeze_null_axes/squeeze_null_axes" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("stack_test_float32/stack_test_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("stack_test_int32/stack_test_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("stack_test_neg_axis/stack_test_neg_axis" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("stack_test_none_axis/stack_test_none_axis" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input1_1/strided_slice_input1_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input1_2/strided_slice_input1_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input1_3/strided_slice_input1_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input1_4/strided_slice_input1_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input2_1/strided_slice_input2_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input2_2/strided_slice_input2_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input2_3/strided_slice_input2_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input3_1/strided_slice_input3_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("strided_slice_input3_2/strided_slice_input3_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sum_1/sum_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sum_2/sum_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sum_3/sum_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("sum_4/sum_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("swish_default_params/swish_default_params" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("swish_beta/swish_beta" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tanh/tanh" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tanh_shrink_1/tanh_shrink_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tanh_shrink_2/tanh_shrink_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_repeat_times_tensor/tile_repeat_times_tensor" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_list_float32/tile_list_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_list_int32/tile_list_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_list_int64/tile_list_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_list_bool/tile_list_bool" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_tuple_float32/tile_tuple_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_tuple_int32/tile_tuple_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_tuple_int64/tile_tuple_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_tuple_bool/tile_tuple_bool" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_tensor_list/tile_tensor_list" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_repeat_gt_x/tile_repeat_gt_x" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tile_repeat_lt_x/tile_repeat_lt_x" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_1/top_k_v2_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_2/top_k_v2_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_3/top_k_v2_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_4/top_k_v2_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_5/top_k_v2_test_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("top_k_v2_test_6/top_k_v2_test_6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu/triu" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_1/triu_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_2/triu_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_3/triu_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_4/triu_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_int32/triu_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("triu_int64/triu_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_downsample_false_0/trilinear_downsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_downsample_false_1/trilinear_downsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_downsample_true_0/trilinear_downsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_false_0/trilinear_upsample_false_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_false_1/trilinear_upsample_false_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_scales/trilinear_upsample_scales" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_scales2/trilinear_upsample_scales2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_true_0/trilinear_upsample_true_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("trilinear_upsample_tensor_size/trilinear_upsample_tensor_size" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril/tril" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_1/tril_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_2/tril_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_3/tril_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_4/tril_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_int32/tril_int32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("tril_int64/tril_int64" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unsqueeze/unsqueeze" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique/unique" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index/unique_ret_index" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_inverse/unique_ret_inverse" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_counts/unique_ret_counts" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index_inverse/unique_ret_index_inverse" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index_counts/unique_ret_index_counts" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_inverse_counts/unique_ret_inverse_counts" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index_inverse_counts/unique_ret_index_inverse_counts" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index_axis/unique_ret_index_axis" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unique_ret_index_i32/unique_ret_index_i32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unstack_1/unstack_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unstack_2/unstack_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unstack_3/unstack_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unstack_4/unstack_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("unstack_5/unstack_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_1/where_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_2/where_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_3/where_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_1/where_index_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_2/where_index_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_3/where_index_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_4/where_index_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_5/where_index_5" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("where_index_6/where_index_6" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("abs_float32/abs_float32" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("atan2/atan2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_any_test_0/reduce_any_test_0" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_any_test_1/reduce_any_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_any_test_2/reduce_any_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_any_test_3/reduce_any_test_3" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("reduce_any_test_4/reduce_any_test_4" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scatter_test_1/scatter_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scatter_test_2/scatter_test_2" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("scatter_nd_add_test_1/scatter_nd_add_test_1" + std::string(TEST_PADDLE_MODEL_EXT)),
     // Temporily disable them until root caused to secure CI stable.
     // CVS-66703 to track this.
-    // std::string("yolo_box_clip_box/yolo_box_clip_box.pdmodel"),
-    // std::string("yolo_box_default/yolo_box_default.pdmodel"),
-    // std::string("yolo_box_scale_xy/yolo_box_scale_xy.pdmodel"),
-    std::string("yolo_box_uneven_wh/yolo_box_uneven_wh.pdmodel")};
+    // std::string("yolo_box_clip_box/yolo_box_clip_box" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("yolo_box_default/yolo_box_default" + std::string(TEST_PADDLE_MODEL_EXT)),
+    // std::string("yolo_box_scale_xy/yolo_box_scale_xy" + std::string(TEST_PADDLE_MODEL_EXT)),
+    std::string("yolo_box_uneven_wh/yolo_box_uneven_wh" + std::string(TEST_PADDLE_MODEL_EXT))};
 
 INSTANTIATE_TEST_SUITE_P(PaddleFuzzyOpTest,
                          FrontEndFuzzyOpTest,
diff --git a/src/frontends/paddle/tests/partial_shape.cpp b/src/frontends/paddle/tests/partial_shape.cpp
index 33146b2849..dc76f21457 100644
--- a/src/frontends/paddle/tests/partial_shape.cpp
+++ b/src/frontends/paddle/tests/partial_shape.cpp
@@ -13,7 +13,7 @@ using PaddlePartialShapeTest = FrontEndPartialShapeTest;
 
 static PartShape getTestShape_2in_2out() {
     PartShape res;
-    res.m_modelName = "2in_2out/2in_2out.pdmodel";
+    res.m_modelName = "2in_2out/2in_2out" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_tensorName = "inputX1";
     res.m_oldPartialShape = PartialShape{1, 1, 3, 3};
     res.m_newPartialShape = PartialShape{2, 1, 3, 3};
@@ -22,7 +22,7 @@ static PartShape getTestShape_2in_2out() {
 
 static PartShape getTestShape_2in_2out_dynbatch() {
     PartShape res;
-    res.m_modelName = "2in_2out_dynbatch/2in_2out_dynbatch.pdmodel";
+    res.m_modelName = "2in_2out_dynbatch/2in_2out_dynbatch" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_tensorName = "inputX1";
     res.m_oldPartialShape = PartialShape{Dimension::dynamic(), 1, 3, 3};
     res.m_newPartialShape = PartialShape{2, 1, 3, 3};
@@ -31,7 +31,7 @@ static PartShape getTestShape_2in_2out_dynbatch() {
 
 static PartShape getTestShape_conv2d() {
     PartShape res;
-    res.m_modelName = "conv2d/conv2d.pdmodel";
+    res.m_modelName = "conv2d/conv2d" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_tensorName = "x";
     res.m_oldPartialShape = PartialShape{1, 3, 4, 4};
     res.m_newPartialShape = PartialShape{1, 3, 8, 8};
@@ -40,7 +40,7 @@ static PartShape getTestShape_conv2d() {
 
 static PartShape getTestShape_conv2d_setDynamicBatch() {
     PartShape res;
-    res.m_modelName = "conv2d/conv2d.pdmodel";
+    res.m_modelName = "conv2d/conv2d" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_tensorName = "x";
     res.m_oldPartialShape = PartialShape{1, 3, 4, 4};
     res.m_newPartialShape = PartialShape{Dimension::dynamic(), 3, 8, 8};
@@ -49,7 +49,7 @@ static PartShape getTestShape_conv2d_setDynamicBatch() {
 
 static PartShape getTestShape_conv2d_relu() {
     PartShape res;
-    res.m_modelName = "conv2d_relu/conv2d_relu.pdmodel";
+    res.m_modelName = "conv2d_relu/conv2d_relu" + std::string(TEST_PADDLE_MODEL_EXT);
     res.m_tensorName = "xxx";
     res.m_oldPartialShape = PartialShape{1, 3, 4, 4};
     res.m_newPartialShape = PartialShape{5, 3, 5, 5};
diff --git a/src/frontends/paddle/tests/places.cpp b/src/frontends/paddle/tests/places.cpp
index acacd42105..cbb509ff98 100644
--- a/src/frontends/paddle/tests/places.cpp
+++ b/src/frontends/paddle/tests/places.cpp
@@ -9,11 +9,12 @@
 
 #include "gtest/gtest.h"
 #include "paddle_utils.hpp"
+#include "functional_test_utils/skip_tests_config.hpp"
 
 using namespace ov::frontend;
 
 const std::string model_file = FrontEndTestUtils::make_model_path(std::string(TEST_PADDLE_MODELS_DIRNAME) +
-                                                                  "place_test_model/place_test_model.pdmodel");
+                                                                  "place_test_model/place_test_model" + std::string(TEST_PADDLE_MODEL_EXT));
 const std::string vars_name_file =
     FrontEndTestUtils::make_model_path(std::string(TEST_PADDLE_MODELS_DIRNAME) + "place_test_model/vars_name.txt");
 const std::string outputs_name_file =
@@ -22,6 +23,7 @@ const std::string outputs_name_file =
 class Paddle_Places : public ::testing::Test {
 protected:
     void SetUp() override {
+        SKIP_IF_CURRENT_TEST_IS_DISABLED();
         std::fstream name_file;
         name_file.open(vars_name_file, std::ios::in);
         if (name_file.is_open()) {
diff --git a/src/frontends/paddle/tests/read_paddle_model_test.cpp b/src/frontends/paddle/tests/read_paddle_model_test.cpp
index dac4a739c1..9b63d09c82 100644
--- a/src/frontends/paddle/tests/read_paddle_model_test.cpp
+++ b/src/frontends/paddle/tests/read_paddle_model_test.cpp
@@ -48,8 +48,13 @@ TEST(Paddle_Reader_Tests, LoadModelMemoryToCore) {
     ASSERT_TRUE(bin_ptr != nullptr) << "can't open " << param;
     ov::Tensor weight_tensor = ov::Tensor(ov::element::u8, {1, bin_size}, bin_ptr);
     std::string model_str = std::string((char*)xml_ptr, xml_size);
+    if (std::string(TEST_ENABLE_PIR) == "1") {
+        ASSERT_NO_THROW(core.read_model(model_str, weight_tensor));
+        free(xml_ptr);
+        free(bin_ptr);
+        return;
+    }
     auto function = core.read_model(model_str, weight_tensor);
-
     const auto inputType = ov::element::f32;
     const auto inputShape = ov::Shape{1, 3, 4, 4};
     const auto data = std::make_shared<ov::opset1::Parameter>(inputType, inputShape);
@@ -99,6 +104,10 @@ TEST(Paddle_Reader_Tests, ImportBasicModelToCore) {
                                                     std::string(TEST_PADDLE_MODEL_EXT));
 
     ov::Core core;
+    if (std::string(TEST_ENABLE_PIR) == "1") {
+        ASSERT_NO_THROW(core.read_model(FrontEndTestUtils::make_model_path(model)));
+        return;
+    }
     auto function = core.read_model(FrontEndTestUtils::make_model_path(model));
 
     const auto inputType = ov::element::f32;
diff --git a/src/frontends/paddle/tests/set_element_type.cpp b/src/frontends/paddle/tests/set_element_type.cpp
index ecd7f52ec3..27817b5ff3 100644
--- a/src/frontends/paddle/tests/set_element_type.cpp
+++ b/src/frontends/paddle/tests/set_element_type.cpp
@@ -14,7 +14,7 @@ static SetTypeFEParam getTestData_relu() {
     SetTypeFEParam res;
     res.m_frontEndName = PADDLE_FE;
     res.m_modelsPath = std::string(TEST_PADDLE_MODELS_DIRNAME);
-    res.m_modelName = "relu/relu.pdmodel";
+    res.m_modelName = "relu/relu" + std::string(TEST_PADDLE_MODEL_EXT);
     return res;
 }
 
diff --git a/src/frontends/paddle/tests/skip_tests_config.cpp b/src/frontends/paddle/tests/skip_tests_config.cpp
index 7ecc5c0788..0694e8489c 100644
--- a/src/frontends/paddle/tests/skip_tests_config.cpp
+++ b/src/frontends/paddle/tests/skip_tests_config.cpp
@@ -8,17 +8,19 @@
 #include <vector>
 
 std::vector<std::string> disabledTestPatterns() {
-    return {
+    std::vector<std::string> result =
+    {
 #ifdef OPENVINO_STATIC_LIBRARY
         // Disable tests for static libraries
         ".*FrontendLibCloseTest.*",
 #endif
         ".*testUnloadLibBeforeDeletingDependentObject.*",
-        // CVS-130605, CVS-170348
-        ".*paddle_yolo_box_uneven_wh_yolo_box_uneven_wh_pdmodel.*",
-        ".*paddle_loop_dyn_loop_dyn_pdmodel.*",
-        ".*paddle_scatter_test_1_scatter_test_1_pdmodel.*",
-        ".*paddle_top_k_.*",
-        ".*generate_proposals.*",
     };
+    if (TEST_PADDLE_MODEL_EXT == ".json") {
+        result.insert(result.end(),
+        {
+            "Paddle_Places.*",
+        });
+    }
+    return result;
 }
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out.py
index 377a03fb07..4e38c2a06e 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out.py
@@ -3,9 +3,55 @@
 
 import paddle
 import numpy as np
-import os
 import sys
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
+
+if is_pir_enabled():
+    class TwoInputAndTwoOutput(paddle.nn.Layer):
+        def __init__(self):
+            super(TwoInputAndTwoOutput, self).__init__()
+            self.conv_layer1 = paddle.nn.Conv2D(
+                in_channels=1,      # Number of input channels (e.g., RGB image has 3)
+                out_channels=1,    # Number of output channels (filters)
+                kernel_size=1,      # Size of the convolution kernel
+                stride=1,           # Stride of the convolution
+                padding=0,           # Padding added to both sides of the input
+                dilation=1,
+                groups=1,
+                bias_attr=None
+            )
+            self.conv_layer2 = paddle.nn.Conv2D(
+                in_channels=2,      # Number of input channels (e.g., RGB image has 3)
+                out_channels=1,    # Number of output channels (filters)
+                kernel_size=1,      # Size of the convolution kernel
+                stride=1,           # Stride of the convolution
+                padding=0,           # Padding added to both sides of the input
+                dilation=1,
+                groups=1,
+                bias_attr=None
+            )
+            self.relu2a = paddle.nn.ReLU()
+            self.relu2b = paddle.nn.ReLU()
+            self.relu3a = paddle.nn.ReLU()
+            self.relu3b = paddle.nn.ReLU()
+        def forward(self, x, y):
+            conv1_res = self.conv_layer1(x)
+            conv2_res = self.conv_layer2(y)
+            add1_res = paddle.add(conv1_res, conv2_res)
+            relu2a_res = self.relu2a(add1_res)
+            relu2b_res = self.relu2b(add1_res)
+            add2_res = paddle.add(relu2a_res, relu2b_res)
+            relu3a_res = self.relu3a(add2_res)
+            relu3b_res = self.relu3b(add2_res)
+            return relu3a_res, relu3b_res
+    model = TwoInputAndTwoOutput()
+    net = paddle.jit.to_static(model, full_graph=True)
+    net.eval()
+    x = np.random.rand(1, 1, 3, 3).astype('float32');
+    y = np.random.rand(1, 2, 3, 3).astype('float32');
+    name = "2in_2out"
+    saveModel_v3(name, model, [x, y], sys.argv[1])
+    sys.exit(0)
 
 if paddle.__version__ >= '2.6.0':
     import paddle.base as fluid
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out_dynbatch.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out_dynbatch.py
index 454771985c..55aec86cc0 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out_dynbatch.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_2in_2out_dynbatch.py
@@ -3,9 +3,56 @@
 
 import paddle
 import numpy as np
-import os
 import sys
-from save_model import saveModel 
+from save_model import saveModel, is_pir_enabled, saveModel_v3
+
+if is_pir_enabled():
+    class TwoInputAndTwoOutput(paddle.nn.Layer):
+        def __init__(self):
+            super(TwoInputAndTwoOutput, self).__init__()
+            self.conv_layer1 = paddle.nn.Conv2D(
+                in_channels=1,      # Number of input channels (e.g., RGB image has 3)
+                out_channels=1,    # Number of output channels (filters)
+                kernel_size=1,      # Size of the convolution kernel
+                stride=1,           # Stride of the convolution
+                padding=0,           # Padding added to both sides of the input
+                dilation=1,
+                groups=1,
+                bias_attr=None
+            )
+            self.conv_layer2 = paddle.nn.Conv2D(
+                in_channels=2,      # Number of input channels (e.g., RGB image has 3)
+                out_channels=1,    # Number of output channels (filters)
+                kernel_size=1,      # Size of the convolution kernel
+                stride=1,           # Stride of the convolution
+                padding=0,           # Padding added to both sides of the input
+                dilation=1,
+                groups=1,
+                bias_attr=None
+            )
+            self.relu2a = paddle.nn.ReLU()
+            self.relu2b = paddle.nn.ReLU()
+            self.relu3a = paddle.nn.ReLU()
+            self.relu3b = paddle.nn.ReLU()
+        def forward(self, x, y):
+            conv1_res = self.conv_layer1(x)
+            conv2_res = self.conv_layer2(y)
+            add1_res = paddle.add(conv1_res, conv2_res)
+            relu2a_res = self.relu2a(add1_res)
+            relu2b_res = self.relu2b(add1_res)
+            add2_res = paddle.add(relu2a_res, relu2b_res)
+            relu3a_res = self.relu3a(add2_res)
+            relu3b_res = self.relu3b(add2_res)
+            return relu3a_res, relu3b_res
+    model = TwoInputAndTwoOutput()
+    net = paddle.jit.to_static(model, full_graph=True)
+    net.eval()
+    x = np.random.rand(1, 1, 3, 3).astype('float32');
+    y = np.random.rand(1, 2, 3, 3).astype('float32');
+    name = "2in_2out_dynbatch"
+    saveModel_v3(name, model, [x, y], sys.argv[1], [[-1,1,3,3], [-1,2,3,3]])
+    sys.exit(0)
+
 
 if paddle.__version__ >= '2.6.0':
     import paddle.base as fluid
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign.py
index 779bb0e5f6..64226d8113 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign.py
@@ -1,13 +1,39 @@
 # Copyright (C) 2018-2025 Intel Corporation
 # SPDX-License-Identifier: Apache-2.0
- 
-import os
+
 import sys
 
 import numpy as np
 import paddle
 
-from save_model import exportModel
+from save_model import exportModel, saveModel_v3, is_pir_enabled
+if not is_pir_enabled() :
+    class Assign(paddle.nn.Layer):
+        def __init__(self):
+            super(Assign, self).__init__()
+        def forward(self, array):
+            result1 = paddle.zeros(shape=[3, 2], dtype='float32')
+            paddle.assign(array, result1) # result1 = [[1, 1], [3 4], [1, 3]]
+            return result1
+    model = Assign()
+    name = "assign"
+    x = np.array([[1, 1],
+                [3, 4],
+                [1, 3]]).astype(np.int64)
+    output = saveModel_v3(name, model, [x], sys.argv[1])
+
+if is_pir_enabled():
+    class Assign_none(paddle.nn.Layer):
+        def __init__(self):
+            super(Assign_none, self).__init__()
+        def forward(self, data):
+            result2 = paddle.assign(data)  # result2 = [[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]]
+            return result2
+    model = Assign_none()
+    name = "assign_none"
+    x = paddle.full(shape=[3, 2], fill_value=2.5, dtype='float32') # [[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]]
+    output = saveModel_v3(name, model, [x], sys.argv[1])
+    sys.exit(0)
 
 '''
 assign w/ output
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign_value.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign_value.py
index 688decd5b2..d27e2602e3 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign_value.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_assign_value.py
@@ -2,12 +2,11 @@
 # SPDX-License-Identifier: Apache-2.0
 
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
-
+import paddle
 
 def paddle_assign_value(name, test_x):
-    import paddle
     paddle.enable_static()
     main_program = paddle.static.Program()
     startup_program = paddle.static.Program()
@@ -28,6 +27,18 @@ def paddle_assign_value(name, test_x):
 
         saveModel(name, exe, feed_vars=[node_x], fetchlist=[result], inputs=[test_x], outputs=[outs[0]], target_dir=sys.argv[1])
 
+def paddle_assign_value_v3(name, test_x):
+    import paddle
+    class Assign(paddle.nn.Layer):
+        def __init__(self):
+            super(Assign, self).__init__()
+        def forward(self, test_x):
+            node_x = paddle.cast(test_x, dtype=test_x.dtype)
+            const_value = paddle.assign(test_x, output=None)
+            result = paddle.cast(paddle.concat([node_x, const_value], 0), dtype=np.float32)
+            return result
+    model = Assign()
+    saveModel_v3(name, model, [test_x], sys.argv[1])
 
 def compare():
 
@@ -49,8 +60,12 @@ def compare():
             "input": np.array([False, True, False])
         }
     ]
+
     for test in test_cases:
-        paddle_assign_value(test['name'], test['input'])
+        if is_pir_enabled() :
+            paddle_assign_value_v3(test['name'], test['input'])
+        else:
+            paddle_assign_value(test['name'], test['input'])
 
 
 if __name__ == "__main__":
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_batch_norm.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_batch_norm.py
index 9d3d40195d..3f2699691a 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_batch_norm.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_batch_norm.py
@@ -5,12 +5,12 @@
 # pool2d paddle model generator
 #
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
+import paddle
 
 
 def batch_norm1(name : str, x, scale, bias, mean, var, data_layout):
-    import paddle
     paddle.enable_static()
 
     node_x = paddle.static.data(name='x', shape=x.shape, dtype='float32')
@@ -40,6 +40,23 @@ def batch_norm1(name : str, x, scale, bias, mean, var, data_layout):
 
     return outs[0]
 
+def batch_norm1_v3(name : str, x, scale, bias, mean, var, data_layout):
+    scale_attr = paddle.ParamAttr(name="scale1", initializer=paddle.nn.initializer.Assign(scale))
+    bias_attr = paddle.ParamAttr(name="bias1", initializer=paddle.nn.initializer.Assign(bias))
+    batch_norm = paddle.nn.BatchNorm(num_channels = scale.shape[0],
+                                     epsilon=1e-5,
+                                     param_attr=scale_attr,
+                                     bias_attr=bias_attr,
+                                     moving_mean_name="bn_mean1",
+                                     moving_variance_name="bn_variance1",
+                                     use_global_stats=True,
+                                     data_layout=data_layout)
+    batch_norm._parameters["_mean"].set_value(mean)
+    batch_norm._parameters["_variance"].set_value(var)
+
+    output = saveModel_v3(name, batch_norm, [x], sys.argv[1])
+    return output.numpy()
+
 def batch_norm2(name : str, x, scale, bias, mean, var, data_layout):
     import paddle
     paddle.enable_static()
@@ -71,22 +88,45 @@ def batch_norm2(name : str, x, scale, bias, mean, var, data_layout):
 
     return outs[0]
 
+def batch_norm2_v3(name : str, x, scale, bias, mean, var, data_layout):
+    scale_attr = paddle.ParamAttr(name="scale2", initializer=paddle.nn.initializer.Assign(scale))
+    bias_attr = paddle.ParamAttr(name="bias2", initializer=paddle.nn.initializer.Assign(bias))
+    batch_norm = paddle.nn.BatchNorm(num_channels = scale.shape[0],
+                                     epsilon=1e-5,
+                                     param_attr=scale_attr,
+                                     bias_attr=bias_attr,
+                                     moving_mean_name="bn_mean2",
+                                     moving_variance_name="bn_variance2",
+                                     use_global_stats=True,
+                                     data_layout=data_layout)
+
+    batch_norm._parameters["_mean"].set_value(mean)
+    batch_norm._parameters["_variance"].set_value(var)
+
+    output = saveModel_v3(name, batch_norm, [x], sys.argv[1])
+    return output.numpy()
+
 def main():
-    import paddle
     data = np.array([[[[-1, 0, 1]], [[2, 3, 4]]]]).astype(np.float32)
     # data layout is NCHW
     scale = np.array([1.0, 1.5]).astype(np.float32)
     bias = np.array([0, 1]).astype(np.float32)
     mean = np.array([0, 3]).astype(np.float32)
     var = np.array([1, 1.5]).astype(np.float32)
-    batch_norm1("batch_norm_nchw", data, scale, bias, mean, var, "NCHW")
+    if is_pir_enabled():
+        batch_norm1_v3("batch_norm_nchw", data, scale, bias, mean, var, "NCHW")
+    else:
+        batch_norm1("batch_norm_nchw", data, scale, bias, mean, var, "NCHW")
 
     # data layout is NHWC
     scale = np.array([1.0, 1.5, 2.0]).astype(np.float32)
     bias = np.array([0, 1, 2]).astype(np.float32)
     mean = np.array([0.5, 1.5, 1.5]).astype(np.float32)
     var = np.array([1, 1.5, 2]).astype(np.float32)
-    batch_norm2("batch_norm_nhwc", data, scale, bias, mean, var, "NHWC")
+    if is_pir_enabled():
+        batch_norm2_v3("batch_norm_nhwc", data, scale, bias, mean, var, "NHWC")
+    else:
+        batch_norm2("batch_norm_nhwc", data, scale, bias, mean, var, "NHWC")
 
 if __name__ == "__main__":
     main()
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_bmm.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_bmm.py
index 6e5d2c6c33..4262ea12ed 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_bmm.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_bmm.py
@@ -2,13 +2,11 @@
 # SPDX-License-Identifier: Apache-2.0
 
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
-
+import paddle
 
 def paddle_bmm(x1, x2):
-    import paddle
-
     paddle.enable_static()
     node_x1 = paddle.static.data(name='x1', shape=x1.shape, dtype=x1.dtype)
     node_x2 = paddle.static.data(name='x2', shape=x2.shape, dtype=x2.dtype)
@@ -28,6 +26,18 @@ def paddle_bmm(x1, x2):
 
     return outs[0]
 
+def paddle_bmm_v3(x1, x2):
+    class BMM(paddle.nn.Layer):
+        def __init__(self):
+            super(BMM, self).__init__()
+            self.batch_norm = paddle.nn.BatchNorm(7, use_global_stats=True)
+        def forward(self, x1, x2):
+            bmm_res = paddle.bmm(x1, x2)
+            result1 = self.batch_norm(bmm_res)
+            return result1
+    model = BMM()
+    name = "bmm"
+    saveModel_v3(name, model, [x1, x2], sys.argv[1])
 
 if __name__ == "__main__":
     input1 = np.array([[[0., 1., 2., 3., 4.],  # (1, 1, 7, 5) input tensor
@@ -39,4 +49,8 @@ if __name__ == "__main__":
                         [30., 31., 32., 33., 34.,]]]).astype(np.float32)
 
     input2 = np.ones([1, 5, 7]).astype('float32')
-    paddle_result = paddle_bmm(input1, input2)
+
+    if is_pir_enabled():
+        paddle_result = paddle_bmm_v3(input1, input2)
+    else:
+        paddle_result = paddle_bmm(input1, input2)
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_box_coder.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_box_coder.py
index 3d0d736d40..6d0ec33f68 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_box_coder.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_box_coder.py
@@ -2,7 +2,12 @@ import sys
 
 import numpy as np
 import paddle
-from ops import box_coder
+
+from save_model import is_pir_enabled
+if is_pir_enabled():
+    from paddle.vision.ops import box_coder
+else:
+    from ops import box_coder
 
 from save_model import exportModel, saveModel
 
@@ -44,7 +49,7 @@ def test_box_coder(name: str, prior_box, prior_box_var, target_box, code_type, b
         outs = exe.run(
             feed=feed_dict,
             fetch_list=[out])
-        
+
         feed_vars = [prior_box_decode, target_box_decode]
         if is_tensor:
             feed_vars.append(prior_box_var_decode)
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_combinations.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_combinations.py
index fe46d72fa9..676b579533 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_combinations.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_combinations.py
@@ -1,12 +1,10 @@
 # Copyright (C) 2018-2025 Intel Corporation
 # SPDX-License-Identifier: Apache-2.0
 
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import numpy as np
 import paddle
 import sys
-paddle.enable_static()
-
 
 def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
     cpu = paddle.static.cpu_places(1)
@@ -23,6 +21,7 @@ def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
 
 
 def paddle_conv2d(input_x, name, input_shape, kernel, dilation, padding, stride, groups=1, use_cudnn=True):
+    paddle.enable_static()
     main_program = paddle.static.Program()
     startup_program = paddle.static.Program()
     with paddle.static.program_guard(main_program, startup_program):
@@ -32,6 +31,21 @@ def paddle_conv2d(input_x, name, input_shape, kernel, dilation, padding, stride,
                                        padding=padding, param_attr=weight_attr, dilation=dilation, stride=stride, groups=groups, use_cudnn=use_cudnn)
     run_and_save_model(input_x, name, data, conv2d, main_program, startup_program)
 
+def paddle_conv2d_v3(input_x, name, input_shape, kernel, dilation, padding, stride, groups=1, use_cudnn=True):
+    conv2d_weight_param_name = "conv2d_weight" + name
+    weight_attr = paddle.ParamAttr(name=conv2d_weight_param_name, initializer=paddle.nn.initializer.Assign(kernel))
+    conv_layer = paddle.nn.Conv2D(
+        in_channels=3,      # Number of input channels (e.g., RGB image has 3)
+        out_channels=kernel.shape[0],    # Number of output channels (filters)
+        kernel_size=kernel.shape[2:4],      # Size of the convolution kernel
+        stride=stride,           # Stride of the convolution
+        padding=padding,           # Padding added to both sides of the input
+        dilation=dilation,
+        groups=groups,
+        weight_attr=weight_attr,
+        bias_attr=None
+    )
+    saveModel_v3(name, conv_layer, [input_x], sys.argv[1])
 
 if __name__ == "__main__":
 
@@ -136,13 +150,21 @@ if __name__ == "__main__":
             "use_cudnn": False
         }
     ]
-    for test in test_cases:
 
-        paddle_conv2d(test['input_x'], test['name'], test["input_shape"],
-                    test['kernel'], test['dilation'],
-                    test['padding'],
-                    test['stride'],
-                    1 if "groups" not in test else test['groups'],
-                    True if "use_cudnn" not in test else test['use_cudnn'])
+    for test in test_cases:
+        if is_pir_enabled():
+            paddle_conv2d_v3(test['input_x'], test['name'], test["input_shape"],
+                             test['kernel'], test['dilation'],
+                             test['padding'],
+                             test['stride'],
+                             1 if "groups" not in test else test['groups'],
+                             True if "use_cudnn" not in test else test['use_cudnn'])
+        else:
+            paddle_conv2d(test['input_x'], test['name'], test["input_shape"],
+                        test['kernel'], test['dilation'],
+                        test['padding'],
+                        test['stride'],
+                        1 if "groups" not in test else test['groups'],
+                        True if "use_cudnn" not in test else test['use_cudnn'])
 
 
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_relu.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_relu.py
index e65fee1918..a9875b88e4 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_relu.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_relu.py
@@ -4,8 +4,24 @@
 import paddle
 import numpy as np
 import sys
+import os
+from save_model import saveModel, saveModel_v3, is_pir_enabled
+if is_pir_enabled():
+    conv_layer = paddle.nn.Conv2D(
+        in_channels=3,      # Number of input channels (e.g., RGB image has 3)
+        out_channels=5,    # Number of output channels (filters)
+        kernel_size=1,      # Size of the convolution kernel
+        stride=1,           # Stride of the convolution
+        padding=1,           # Padding added to both sides of the input
+        dilation=1,
+        groups=1,
+        bias_attr=None
+    )
+    relu = paddle.nn.ReLU(conv_layer)
+    x = np.random.randn(1, 3, 4, 4).astype(np.float32)
+    saveModel_v3("conv2d_relu", relu, [x], sys.argv[1])
+    sys.exit(0)
 
-from save_model import saveModel
 
 if paddle.__version__ >= '2.6.0':
     import paddle.base as fluid
@@ -29,6 +45,7 @@ if paddle.__version__ >= '2.0.0':
 else:
     relu = fluid.layers.relu(test_layer)
 
+
 exe = fluid.Executor(fluid.CPUPlace())
 exe.run(fluid.default_startup_program())
 inp_dict = {'xxx': inp_blob}
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_s.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_s.py
index 079c58ce50..ba4423d261 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_s.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_s.py
@@ -6,7 +6,24 @@ import numpy as np
 import os
 import sys
 
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
+
+if is_pir_enabled():
+    conv_layer = paddle.nn.Conv2D(
+        in_channels=3,      # Number of input channels (e.g., RGB image has 3)
+        out_channels=5,    # Number of output channels (filters)
+        kernel_size=1,      # Size of the convolution kernel
+        stride=1,           # Stride of the convolution
+        padding=1,           # Padding added to both sides of the input
+        dilation=1,
+        groups=1,
+        bias_attr=None
+    )
+    x = np.random.rand(1, 3, 4, 4).astype('float32');
+    name = "conv2d_s"
+    saveModel_v3(name, conv_layer, [x], sys.argv[1])
+    sys.exit(0)
+
 
 if paddle.__version__ >= '2.6.0':
     import paddle.base as fluid
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_transpose.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_transpose.py
index f9cef60be5..71271a3c46 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_transpose.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_conv2d_transpose.py
@@ -3,11 +3,9 @@
 
 import numpy as np
 import paddle
-paddle.enable_static()
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
 
-
 def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
     cpu = paddle.static.cpu_places(1)
     exe = paddle.static.Executor(cpu[0])
@@ -22,6 +20,7 @@ def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
 
 
 def paddle_conv2d_transpose(input_x, name, input_shape, kernel, dilation, padding, stride, groups=1, use_cudnn=True):
+    paddle.enable_static()
     main_program = paddle.static.Program()
     startup_program = paddle.static.Program()
     with paddle.static.program_guard(main_program, startup_program):
@@ -31,6 +30,21 @@ def paddle_conv2d_transpose(input_x, name, input_shape, kernel, dilation, paddin
                                        padding=padding, param_attr=weight_attr, dilation=dilation, stride=stride, groups=groups, use_cudnn=use_cudnn)
     run_and_save_model(input_x, name, data, conv2d, main_program, startup_program)
 
+def paddle_conv2d_transpose_v3(input_x, name, input_shape, kernel, dilation, padding, stride, groups=1, use_cudnn=True):
+    conv2d_weight_param_name = "conv2d_weight" + name
+    weight_attr = paddle.ParamAttr(name=conv2d_weight_param_name, initializer=paddle.nn.initializer.Assign(kernel))
+    conv_layer = paddle.nn.Conv2DTranspose(
+        in_channels=3,      # Number of input channels (e.g., RGB image has 3)
+        out_channels=kernel.shape[0],    # Number of output channels (filters)
+        kernel_size=kernel.shape[2:4],      # Size of the convolution kernel
+        stride=stride,           # Stride of the convolution
+        padding=padding,           # Padding added to both sides of the input
+        dilation=dilation,
+        groups=groups,
+        weight_attr=weight_attr,
+        bias_attr=None
+    )
+    saveModel_v3(name, conv_layer, [input_x], sys.argv[1])
 
 if __name__ == "__main__":
 
@@ -135,13 +149,22 @@ if __name__ == "__main__":
             "use_cudnn": False
         }
     ]
+
     for test in test_cases:
+        if is_pir_enabled():
+            paddle_conv2d_transpose_v3(test['input_x'], test['name'], test["input_shape"],
+                        test['kernel'], test['dilation'],
+                        test['padding'],
+                        test['stride'],
+                        1 if "groups" not in test else test['groups'],
+                        True if "use_cudnn" not in test else test['use_cudnn'])
+        else:
+            paddle_conv2d_transpose(test['input_x'], test['name'], test["input_shape"],
+                        test['kernel'], test['dilation'],
+                        test['padding'],
+                        test['stride'],
+                        1 if "groups" not in test else test['groups'],
+                        True if "use_cudnn" not in test else test['use_cudnn'])
 
-        paddle_conv2d_transpose(test['input_x'], test['name'], test["input_shape"],
-                    test['kernel'], test['dilation'],
-                    test['padding'],
-                    test['stride'],
-                    1 if "groups" not in test else test['groups'],
-                    True if "use_cudnn" not in test else test['use_cudnn'])
 
 
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_elu.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_elu.py
index 6f164de002..4b24b2d651 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_elu.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_elu.py
@@ -10,12 +10,12 @@ import paddle
 import sys
 
 
-def elu(name: str, x, alpha=None, data_type='float32'):
+def elu(name: str, x, alpha=1.0, data_type='float32'):
     paddle.enable_static()
 
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
         node_x = paddle.static.data(name='x', shape=x.shape, dtype=data_type)
-        
+
         if paddle.__version__ >= '2.0.0':
             out = paddle.nn.functional.elu(node_x, alpha, name='elu')
         else:
@@ -27,7 +27,7 @@ def elu(name: str, x, alpha=None, data_type='float32'):
 
         outs = exe.run(
             feed={'x': x},
-            fetch_list=[out])             
+            fetch_list=[out])
 
         saveModel(name, exe, feed_vars=[node_x], fetchlist=[out],
                   inputs=[x], outputs=[outs[0]], target_dir=sys.argv[1])
@@ -38,7 +38,7 @@ def elu(name: str, x, alpha=None, data_type='float32'):
 def main():
     data_type = 'float32'
     data = np.random.randn(2, 3, 4).astype('float32')
-    elu("elu", data)
+    elu("elu", data, 0)
 
 if __name__ == "__main__":
     main()
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_embedding.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_embedding.py
index 95da33e4c3..fdb5245322 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_embedding.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_embedding.py
@@ -9,9 +9,10 @@
 #
 import numpy as np
 import sys
+import os
 import paddle
 
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 
 def ov_embedding(ids, vocab_embeddings, vocab_size, embedding_dim, padding_idx, sparse):
     """
@@ -50,7 +51,7 @@ def ov_embedding(ids, vocab_embeddings, vocab_size, embedding_dim, padding_idx,
     parameters = [node_ids, node_w]
     inputs_dict = {'ids': ids, "w": vocab_embeddings}
 
-    # 
+    #
     ov_model = ov.Model(graph, parameters, "embedding")
     core = Core()
     compiled_model = core.compile_model(ov_model, 'CPU')
@@ -61,7 +62,7 @@ def ov_embedding(ids, vocab_embeddings, vocab_size, embedding_dim, padding_idx,
 
 def embedding(name: str, ids, vocab_size, embedding_dim, padding_idx=None, sparse=False, vocab_embeddings=None, compare=False):
     """
-    padding_idx (int|long|None) 
+    padding_idx (int|long|None)
     """
     paddle.enable_static()
 
@@ -125,6 +126,21 @@ def embedding(name: str, ids, vocab_size, embedding_dim, padding_idx=None, spars
 
     return outputs
 
+def embedding_v3(name: str, ids, vocab_size, embedding_dim, padding_idx=None, sparse=False, vocab_embeddings=None, compare=False):
+    """
+    padding_idx (int|long|None)
+    """
+    wei_name = name + "W"
+    pretrained_attr = paddle.ParamAttr(name=wei_name,
+                                       initializer=paddle.nn.initializer.Assign(
+                                           vocab_embeddings),
+                                       trainable=False) if vocab_embeddings is not None else None
+
+    node_embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim,
+                                         padding_idx=padding_idx, sparse=sparse, weight_attr=pretrained_attr, name=name)
+    net = paddle.jit.to_static(node_embedding, full_graph=True)
+    output = saveModel_v3(name, node_embedding, [ids], sys.argv[1])
+    return output
 
 if __name__ == "__main__":
     vocab_size = 17
@@ -134,26 +150,43 @@ if __name__ == "__main__":
 
     #
     ids = np.random.randint(0, vocab_size, 4).astype("int32")
-    embedding("embedding_0", ids, vocab_size, embedding_dim,
-              vocab_embeddings=table, compare=False)
+
+    if is_pir_enabled():
+        embedding_v3("embedding_0", ids, vocab_size, embedding_dim,
+                     vocab_embeddings=table, compare=False)
+    else:
+        embedding("embedding_0", ids, vocab_size, embedding_dim,
+                  vocab_embeddings=table, compare=False)
 
     #
     ids = np.random.randint(0, vocab_size, 4).astype("int32")
-    embedding("embedding_sparse", ids, vocab_size, embedding_dim,
-              sparse=True, vocab_embeddings=table, compare=False)
+    if is_pir_enabled():
+        embedding_v3("embedding_sparse", ids, vocab_size, embedding_dim,
+                     sparse=True, vocab_embeddings=table, compare=False)
+    else:
+        embedding("embedding_sparse", ids, vocab_size, embedding_dim,
+                  sparse=True, vocab_embeddings=table, compare=False)
 
     # # compare fail
     ids = np.random.randint(0, vocab_size, 4).astype("int32")
-    embedding("embedding_none_weight", ids,
-              vocab_size, embedding_dim, compare=False)
+    if is_pir_enabled():
+        embedding_v3("embedding_none_weight", ids,
+                     vocab_size, embedding_dim, compare=False)
+    else:
+        embedding("embedding_none_weight", ids,
+                  vocab_size, embedding_dim, compare=False)
 
     #
     ids = np.random.randint(0, vocab_size, 4).astype("int32")
     ids = np.squeeze(ids)
     padding_idx = np.random.choice(ids, 1)[0]
     # print('padding_idx {}, ids {}'.format(padding_idx, ids))
-    outputs = embedding("embedding_paddings", ids, vocab_size, embedding_dim, padding_idx=int(
-        padding_idx), vocab_embeddings=table, compare=False)
+    if is_pir_enabled():
+        outputs = embedding_v3("embedding_paddings", ids, vocab_size, embedding_dim, padding_idx=int(
+            padding_idx), vocab_embeddings=table, compare=False)
+    else:
+        outputs = embedding("embedding_paddings", ids, vocab_size, embedding_dim, padding_idx=int(
+            padding_idx), vocab_embeddings=table, compare=False)
     # print('outputs {}'.format(outputs))
 
     # corner case
@@ -162,15 +195,24 @@ if __name__ == "__main__":
     ids[pick] = vocab_size - 1
     padding_idx = -1
     # print('padding_idx {}, ids {}'.format(padding_idx, ids))
-    outputs = embedding("embedding_paddings_neg1", ids, vocab_size, embedding_dim,
+    if is_pir_enabled():
+        outputs = embedding_v3("embedding_paddings_neg1", ids, vocab_size, embedding_dim,
+                        padding_idx=int(padding_idx), vocab_embeddings=table, compare=False)
+    else:
+        outputs = embedding("embedding_paddings_neg1", ids, vocab_size, embedding_dim,
                         padding_idx=int(padding_idx), vocab_embeddings=table, compare=False)
     # print('outputs {}'.format(outputs))
 
     #
     ids = np.random.randint(low=0, high=vocab_size,
                             size=(2, 4, 5)).astype("int32")
-    embedding("embedding_tensorIds", ids, vocab_size,
-              embedding_dim, vocab_embeddings=table, compare=False)
+
+    if is_pir_enabled():
+        embedding_v3("embedding_tensorIds", ids, vocab_size,
+                     embedding_dim, vocab_embeddings=table, compare=False)
+    else:
+        embedding("embedding_tensorIds", ids, vocab_size,
+                   embedding_dim, vocab_embeddings=table, compare=False)
 
     #
     ids = np.random.randint(low=0, high=vocab_size,
@@ -179,7 +221,10 @@ if __name__ == "__main__":
     padding_idx = np.random.choice(flatten_idx, 1)[0]
     # print('padding_idx {}'.format(padding_idx))
 
-    if paddle.__version__ >= '2.0.0':
+    if is_pir_enabled():
+        outputs = embedding_v3("embedding_tensorIds_paddings", ids, vocab_size, embedding_dim,
+                            padding_idx=np.int64(padding_idx), vocab_embeddings=table, compare=False)
+    elif paddle.__version__ >= '2.0.0':
         outputs = embedding("embedding_tensorIds_paddings", ids, vocab_size, embedding_dim,
                             padding_idx=np.int64(padding_idx), vocab_embeddings=table, compare=False)
     else:
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_generate_proposal_v2.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_generate_proposal_v2.py
index 1c0d291abd..a94969950c 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_generate_proposal_v2.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_generate_proposal_v2.py
@@ -2,7 +2,7 @@
 # pool2d paddle model generator
 #
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, is_pir_enabled
 import sys
 
 def generate_proposals_v2(name: str, input_data: dict, attr: dict):
@@ -17,9 +17,11 @@ def generate_proposals_v2(name: str, input_data: dict, attr: dict):
     nms_thresh = attr["nms_thresh"]
     min_size = attr["min_size"]
     pixel_offset = attr["pixel_offset"]
-
     import paddle
-    from ops import generate_proposals
+    if is_pir_enabled() :
+        from paddle.vision.ops import generate_proposals
+    else:
+        from ops import generate_proposals
 
     paddle.enable_static()
 
@@ -62,8 +64,8 @@ def generate_proposals_v2(name: str, input_data: dict, attr: dict):
             },
             fetch_list=[rois, roi_probs, rois_num])
 
-        # Save inputs in order of OpenVINO model, to facilitate Fuzzy test, 
-        # which accepts inputs and outputs in this order as well. 
+        # Save inputs in order of OpenVINO model, to facilitate Fuzzy test,
+        # which accepts inputs and outputs in this order as well.
         saveModel(name, exe, feed_vars=[scores, bbox_deltas, im_shape, anchors, variances],
                   fetchlist=[rois, roi_probs, rois_num],
                   inputs=[scores_np, bbox_deltas_np, im_shape_np, anchors_np, variances_np],
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_group_norm.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_group_norm.py
index 50c7293186..c7110f0f14 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_group_norm.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_group_norm.py
@@ -5,9 +5,10 @@
 # group norm paddle model generator
 #
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import paddle
 import sys
+import os
 
 data_type = "float32"
 
@@ -42,15 +43,39 @@ def group_norm(name: str, x, groups, epsilon, scale, bias, data_layout):
 
     return outs[0]
 
+def group_norm_v3(name: str, x, groups, epsilon, scale, bias, data_layout):
+    if scale is False:
+        scale_attr = scale
+    else:
+        scale_name = name + "_scale1"
+        scale_attr = paddle.ParamAttr(name=scale_name, initializer=paddle.nn.initializer.Assign(scale))
+    if bias is False:
+        bias_attr = bias
+    else:
+        bias_name = name + "_bias1"
+        bias_attr = paddle.ParamAttr(name=bias_name, initializer=paddle.nn.initializer.Assign(bias))
+
+    group_norm_layer = paddle.nn.GroupNorm(num_channels = 4, num_groups=groups,
+                                      epsilon=epsilon,
+                                      weight_attr=scale_attr,
+                                      bias_attr=bias_attr,
+                                      data_format=data_layout)
+    output = saveModel_v3(name, group_norm_layer, [x], sys.argv[1])
+    return output.numpy()
 
 def main():
+    enable_pir = False;
+
     # data layout is NCHW
     data = np.random.random((2, 4, 3, 4)).astype(np.float32)
     groups = 2
     epsilon = 1e-05
     scale = np.random.random(4).astype(np.float32)
     bias = np.random.random(4).astype(np.float32)
-    group_norm("group_norm_1", data, groups, epsilon, scale, bias, "NCHW")
+    if is_pir_enabled():
+        group_norm_v3("group_norm_1", data, groups, epsilon, scale, bias, "NCHW")
+    else:
+        group_norm("group_norm_1", data, groups, epsilon, scale, bias, "NCHW")
 
     # data layout is NHWC
     data = np.random.random((2, 4, 3, 4)).astype(np.float32)
@@ -58,12 +83,18 @@ def main():
     epsilon = 1e-05
     scale = np.random.random(4).astype(np.float32)
     bias = np.random.random(4).astype(np.float32)
-    group_norm("group_norm_2", data, groups, epsilon, scale, bias, "NHWC")
+    if is_pir_enabled():
+        group_norm_v3("group_norm_2", data, groups, epsilon, scale, bias, "NHWC")
+    else:
+        group_norm("group_norm_2", data, groups, epsilon, scale, bias, "NHWC")
 
     # scale and bias are None
     scale = False
     bias = False
-    group_norm("group_norm_3", data, groups, epsilon, scale, bias, "NHWC")
+    if is_pir_enabled():
+        group_norm_v3("group_norm_3", data, groups, epsilon, scale, bias, "NHWC")
+    else:
+        group_norm("group_norm_3", data, groups, epsilon, scale, bias, "NHWC")
 
 
 if __name__ == "__main__":
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_interpolate.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_interpolate.py
index e87e8e0585..6d01117398 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_interpolate.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_interpolate.py
@@ -4,7 +4,7 @@
 import numpy as np
 import paddle
 from paddle.nn.functional import interpolate
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
 
 def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
@@ -25,6 +25,29 @@ def run_and_save_model(input_x, name, feed, fetch_list, main_prog, start_prog):
 
 def paddle_interpolate(x, sizes=None, scale_factor=None, mode='nearest', align_corners=True,
                      align_mode=0, data_format='NCHW', name=None):
+
+    if is_pir_enabled() :
+        class PaddleInterpolate(paddle.nn.Layer):
+            def __init__(self, x):
+                super(PaddleInterpolate, self).__init__()
+                self.batch_norm = paddle.nn.BatchNorm(num_channels=x.shape[1], use_global_stats=True, epsilon=0)
+                self.scale_factor = scale_factor
+                self.sizes = sizes
+                self.mode = mode
+                self.align_corners = align_corners
+                self.align_mode = align_mode
+                self.data_format = data_format
+                self.name = name
+            def forward(self, x):
+                interp = interpolate(x, size=self.sizes, scale_factor=self.scale_factor,
+                                     mode=self.mode, align_corners=self.align_corners, align_mode=self.align_mode,
+                                     data_format=self.data_format, name=self.name)
+                result1 = self.batch_norm(interp)
+                return result1
+        model = PaddleInterpolate(x)
+        output = saveModel_v3(name, model, [x], sys.argv[1])
+        return output.numpy();
+
     paddle.enable_static()
     main_program = paddle.static.Program()
     startup_program = paddle.static.Program()
@@ -102,6 +125,21 @@ def resize_downsample_nearest():
                                        align_mode=test['align_mode'], data_format='NCHW', name=test['name'])
 
 def paddle_interpolate_tensor_size(data, sizes, mode='nearest', align_corners=True, align_mode=0, data_format='NCHW', name=None):
+    if is_pir_enabled():
+        class PaddleInterpolate(paddle.nn.Layer):
+            def __init__(self, x):
+                super(PaddleInterpolate, self).__init__()
+                self.batch_norm = paddle.nn.BatchNorm(num_channels=x.shape[1], use_global_stats=True, epsilon=0)
+            def forward(self, x, sizes):
+                interp = interpolate(x, size=sizes, scale_factor=None,
+                                     mode=mode, align_corners=align_corners, align_mode=align_mode,
+                                     data_format=data_format, name=name)
+                result1 = self.batch_norm(interp)
+                return result1
+        model = PaddleInterpolate(data)
+        output = saveModel_v3(name, model, [data, sizes], sys.argv[1])
+        return output.numpy();
+
     paddle.enable_static()
     main_program = paddle.static.Program()
     startup_program = paddle.static.Program()
@@ -334,8 +372,24 @@ def linear_upsample_tensor_size():
     sizes = np.array([8,], dtype="int32")
 
     test_case = [{'name': 'linear_upsample_tensor_size', 'align_corners': False, 'align_mode': 1}]
-
     for test in test_case:
+        if is_pir_enabled():
+            class PaddleInterpolate(paddle.nn.Layer):
+                def __init__(self, x, align_corners, align_mode):
+                    super(PaddleInterpolate, self).__init__()
+                    self.batch_norm = paddle.nn.BatchNorm(num_channels=x.shape[1], use_global_stats=True, epsilon=0)
+                    self.align_corners = align_corners
+                    self.align_mode = align_mode
+                def forward(self, x, sizes):
+                    interp = interpolate(x, size=sizes, scale_factor=None,
+                                 mode='linear', align_corners=self.align_corners, align_mode=self.align_mode,
+                                 data_format='NCW', name=test['name'])
+                    result1 = self.batch_norm(interp)
+                    return result1
+            model = PaddleInterpolate(data, test['align_corners'], test['align_mode'])
+            saveModel_v3(test["name"], model,[data, sizes], sys.argv[1])
+            continue
+
         main_program = paddle.static.Program()
         startup_program = paddle.static.Program()
         with paddle.static.program_guard(main_program, startup_program):
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_layer_norm.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_layer_norm.py
index 1222920955..c3b972713b 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_layer_norm.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_layer_norm.py
@@ -5,9 +5,10 @@
 # layer_norm paddle model generator
 #
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import paddle
 import sys
+import os
 
 if paddle.__version__ >= '2.6.0':
     from paddle.base import param_attr
@@ -17,8 +18,19 @@ else:
 data_type = 'float32'
 
 def layer_norm(name:str, x, begin_norm_axis, scale=True, shift=True, param_attr=None, bias_attr=None):
+    if is_pir_enabled():
+        if scale == False:
+            weight_attr = False;
+        else:
+            weight_attr = param_attr;
+        if shift == False:
+            bias_attr = False;
+        layer_norm_layer = paddle.nn.LayerNorm(normalized_shape=x.shape[begin_norm_axis:], weight_attr=weight_attr, bias_attr=bias_attr)
+        output = saveModel_v3(name, layer_norm_layer, [x], sys.argv[1])
+        return output.numpy()
+
     paddle.enable_static()
-    
+
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
         data = paddle.static.data(name='x', shape=x.shape, dtype = data_type)
         out = paddle.static.nn.layer_norm(input=data, scale=scale, shift=shift,\
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_lower_version.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_lower_version.py
index 448069f123..74239016fb 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_lower_version.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_lower_version.py
@@ -5,8 +5,10 @@ import paddle
 import numpy as np
 import os
 import sys
-
-if paddle.__version__ >= '2.6.0':
+from save_model import is_pir_enabled
+if is_pir_enabled():
+    sys.exit(0)
+elif paddle.__version__ >= '2.6.0':
     from paddle.base.proto import framework_pb2
 else:
     from paddle.fluid.proto import framework_pb2
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_mul.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_mul.py
index 6c4ee50ccb..0f2bebca95 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_mul.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_mul.py
@@ -2,11 +2,27 @@
 # SPDX-License-Identifier: Apache-2.0
 
 import numpy as np
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 import sys
 
 def paddle_matmul(name, x1, x2, x_transpose=False, y_transpose=False):
     import paddle
+    if is_pir_enabled():
+        class PaddleMul(paddle.nn.Layer):
+            def __init__(self, num_channels):
+                super(PaddleMul, self).__init__()
+                self.batch_norm = paddle.nn.BatchNorm(num_channels, use_global_stats=True)
+            def forward(self, x1, x2):
+                mul_node = paddle.matmul(x1, x2, x_transpose, y_transpose)
+                result = self.batch_norm(mul_node)
+                return result
+        if y_transpose:
+            num_channels = x2.shape[0]
+        else:
+            num_channels = x2.shape[1]
+        model = PaddleMul(num_channels)
+        output = saveModel_v3(name, model, [x1, x2], sys.argv[1])
+        return output.numpy()
 
     paddle.enable_static()
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_p_norm.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_p_norm.py
index 22d1f56715..285de8168a 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_p_norm.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_p_norm.py
@@ -9,15 +9,15 @@ import sys
 
 import numpy as np
 import paddle
+import os
 
 if paddle.__version__ >= '2.6.0':
     from paddle.base.layer_helper import LayerHelper
 else:
     from paddle.fluid.layer_helper import LayerHelper
 
-from save_model import saveModel
+from save_model import saveModel, saveModel_v3, is_pir_enabled
 
-paddle.enable_static()
 
 
 def p_norm_ref(x, p=None, axis=None, epsilon=1e-12, keepdim=None, name=None):
@@ -37,6 +37,20 @@ def p_norm_ref(x, p=None, axis=None, epsilon=1e-12, keepdim=None, name=None):
 
 
 def p_norm(name: str, x, axis, p, keepdim):
+    if is_pir_enabled():
+        class PNormLayer(paddle.nn.Layer):
+            def __init__(self, p=2.0, axis=1, keepdim=True):
+                super().__init__()
+                self.p = p
+                self.axis = axis
+                self.keepdim = keepdim
+            def forward(self, x):
+                return paddle.norm(x, p=self.p, axis=self.axis, keepdim=self.keepdim)
+        model = PNormLayer(p = p, axis =axis, keepdim = keepdim)
+        output = saveModel_v3(name, model, [x], sys.argv[1])
+        return output.numpy()
+
+    paddle.enable_static()
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
         node_x = paddle.static.data(name='x', shape=x.shape, dtype=x.dtype)
 
@@ -74,7 +88,7 @@ def main():
     input_shape = (3, 5, 6)
     input_data = np.random.rand(*input_shape).astype(np.float32)
     paddle_result = p_norm('p_norm5', input_data, axis=1, p=float('-inf'), keepdim=True)
-    
+
     input_shape = (3, 6, 7)
     input_data = np.zeros(input_shape).astype(np.float32)
     paddle_result = p_norm('p_norm6', input_data, axis=0, p=0.0, keepdim=None)
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_concat.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_concat.py
index f3ea3b2258..2017761b7f 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_concat.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_concat.py
@@ -6,12 +6,31 @@
 #
 import numpy as np
 from save_model import saveModel
+from save_model import saveModel_v3
+from save_model import is_pir_enabled
 import paddle
 import sys
+import os
 
 def partial_concat(name: str, x, y, start_index=0, length=-1):
-    paddle.enable_static()
+    if is_pir_enabled():
+        class PartialConcat(paddle.nn.Layer):
+            def __init__(self, start_index, length):
+                super().__init__()
+                self.start_index = start_index
+                self.length = length
+            def forward(self, x, y):
+                sliced = []
+                axis = 0
+                for item in [x , y]:
+                    end_index = x.shape[axis] if length == -1 else start_index + length
+                    sliced.append(item.slice([axis], [start_index], [end_index]))
+                return paddle.concat(sliced, axis=axis)
+        model = PartialConcat(start_index=start_index, length=length)
+        output = saveModel_v3(name, model, [x, y], sys.argv[1])
+        return output.numpy()
 
+    paddle.enable_static()
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
         x_data = paddle.static.data(name="x", shape=x.shape, dtype=x.dtype)
         y_data = paddle.static.data(name="y", shape=x.shape, dtype=y.dtype)
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_sum.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_sum.py
index a04715dade..c14573d158 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_sum.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_partial_sum.py
@@ -6,11 +6,39 @@
 #
 import numpy as np
 from save_model import saveModel
+from save_model import saveModel_v3
+from save_model import is_pir_enabled
 import paddle
 import sys
+import os
 
 
 def partial_sum(name: str, x, y, start_index=0, length=-1):
+    enable_pir = False;
+    if os.getenv('FLAGS_enable_pir_api') == '1':
+        enable_pir = True
+    elif os.getenv('FLAGS_enable_pir_api') == '0':
+        enable_pir = False
+    else:
+        enable_pir = False
+
+    if is_pir_enabled():
+        class PartialSum(paddle.nn.Layer):
+            def __init__(self, start_index, length):
+                super().__init__()
+                self.start_index = start_index
+                self.length = length
+            def forward(self, x, y):
+                sliced = []
+                axis = 0
+                for item in [x , y]:
+                    end_index = x.shape[axis] if length == -1 else start_index + length
+                    sliced.append(item.slice([axis], [start_index], [end_index]))
+                return sliced[0] + sliced[1]
+        model = PartialSum(start_index=start_index, length=length)
+        output = saveModel_v3(name, model, [x, y], sys.argv[1])
+        return output.numpy()
+
     paddle.enable_static()
 
     with paddle.static.program_guard(paddle.static.Program(), paddle.static.Program()):
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/generate_unsupported_relu.py b/src/frontends/paddle/tests/test_models/gen_scripts/generate_unsupported_relu.py
index a013d5934d..4e55844dba 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/generate_unsupported_relu.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/generate_unsupported_relu.py
@@ -12,6 +12,7 @@ import os
 import numpy as np
 import paddle
 from save_model import saveModel
+from save_model import is_pir_enabled
 
 
 # print numpy array like C structure
@@ -71,13 +72,15 @@ def main():
     data = np.array([-2, 0, 1]).astype('float32')
 
     relu("relu_unsupported", data)
-
-    with open(os.path.join(sys.argv[1], "relu_unsupported", "relu_unsupported.pdmodel"), mode='rb') as file:
+    extention = ".pdmodel"
+    if is_pir_enabled():
+        extention = ".json"
+    with open(os.path.join(sys.argv[1], "relu_unsupported", "relu_unsupported" + extention), mode='rb') as file:
         modelContent = file.read()
 
     modelContent = modelContent.replace(b"relu", b"rxyz")
 
-    with open(os.path.join(sys.argv[1], "relu_unsupported", "relu_unsupported.pdmodel"), mode='wb') as file:
+    with open(os.path.join(sys.argv[1], "relu_unsupported", "relu_unsupported" + extention), mode='wb') as file:
         file.write(modelContent)
 
 
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/nms.py b/src/frontends/paddle/tests/test_models/gen_scripts/nms.py
index ea810b633c..0cd72b3e4e 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/nms.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/nms.py
@@ -3,13 +3,12 @@
 #
 # helper for multiclass/matrix_nms paddle model generator
 #
-import os
 import numpy as np
 import copy  # deepcopy
 import sys
 import paddle
 
-from save_model import saveModel, exportModel, print_alike
+from save_model import saveModel, exportModel, print_alike, saveModel_v3, is_pir_enabled
 
 if paddle.__version__ >= '2.6.0':
     from paddle.base import data_feeder
@@ -21,8 +20,11 @@ else:
 # scores shape (N, C, M) if shared else (M, C)
 def NMS(name: str, bboxes, scores, attrs: dict, rois_num=None, verbose=False):
     import paddle
+    if is_pir_enabled():
+        from paddle.vision.ops import matrix_nms as matrix_nms
+    else:
+        from ops import matrix_nms as matrix_nms
     from ops import multiclass_nms as multiclass_nms
-    from ops import matrix_nms as matrix_nms
     paddle.enable_static()
 
     with paddle.static.program_guard(paddle.static.Program(),
@@ -105,7 +107,7 @@ def NMS(name: str, bboxes, scores, attrs: dict, rois_num=None, verbose=False):
         else:
             index = np.array(output_lod.pop(0)).astype(data_feeder.convert_dtype(
                 output[2].dtype)) if output[2] is not None else None
-            
+
         feed_vars = [node_boxes, node_scores]
         if node_rois_num is not None:
             feed_vars.append(node_rois_num)
diff --git a/src/frontends/paddle/tests/test_models/gen_scripts/save_model.py b/src/frontends/paddle/tests/test_models/gen_scripts/save_model.py
index f842b75178..85bc1a8765 100644
--- a/src/frontends/paddle/tests/test_models/gen_scripts/save_model.py
+++ b/src/frontends/paddle/tests/test_models/gen_scripts/save_model.py
@@ -2,11 +2,23 @@
 # SPDX-License-Identifier: Apache-2.0
 
 import os
-import sys
 import numpy as np
 import paddle
 
-#print numpy array like C structure       
+#print numpy array like C structure
+def is_pir_enabled():
+    enable_pir_flag = False;
+    if os.getenv('FLAGS_enable_pir_api') == '1':
+        enable_pir_flag = True
+    elif os.getenv('FLAGS_enable_pir_api') == '0':
+        enable_pir_flag = False
+    else:
+        enable_pir_flag = False
+    if paddle.__version__ >= '3.0.0' and enable_pir_flag:
+        return True
+    else:
+        return False
+
 def print_alike(arr, seperator_begin='{', seperator_end='}', verbose=False):
     shape = arr.shape
     rank = len(shape)
@@ -38,7 +50,7 @@ def print_alike(arr, seperator_begin='{', seperator_end='}', verbose=False):
             return line
 
     if verbose:
-        print(print_array(arr, seperator_end))        
+        print(print_array(arr, seperator_end))
 
 def saveModel(name, exe, feed_vars:list, fetchlist:list, inputs:list, outputs:list, target_dir:str):
     model_dir = os.path.join(target_dir, name)
@@ -47,11 +59,12 @@ def saveModel(name, exe, feed_vars:list, fetchlist:list, inputs:list, outputs:li
 
     # print("\n\n------------- %s -----------\n" % (name))
     for i, input in enumerate(inputs):
-        feedkey = feed_vars[i].name
         # print("INPUT %s :" % (feedkey), input.shape, input.dtype, "\n")
         # print_alike(input)
         np.save(os.path.join(model_dir, "input{}".format(i)), input)
-        np.save(os.path.join(model_dir, "input{}.{}.{}".format(i, feedkey, input.dtype)), input)
+        if not is_pir_enabled():
+            feedkey = feed_vars[i].name
+            np.save(os.path.join(model_dir, "input{}.{}.{}".format(i, feedkey, input.dtype)), input)
     # print("\n")
 
     for i, output in enumerate(outputs):
@@ -63,6 +76,39 @@ def saveModel(name, exe, feed_vars:list, fetchlist:list, inputs:list, outputs:li
     model_name = os.path.join(model_dir, name)
     paddle.static.io.save_inference_model(model_name, feed_vars, fetchlist, exe)
 
+def saveModel_v3(name, model, inputs:list, target_dir:str, dyn_shapes:list=[]):
+    net = paddle.jit.to_static(model, full_graph=True)
+    net.eval()
+    return exportModel(name, net, inputs, target_dir, dyn_shapes)
+    #model_dir = os.path.join(target_dir, name)
+    #model_path = os.path.join(model_dir, name)
+    #if not os.path.exists(model_dir):
+    #    os.makedirs(model_dir)
+    #input_tensor_list = []
+    #input_specs = []
+
+    #if len(dyn_shapes)>0:
+    #    assert(len(dyn_shapes) == len(inputs))
+
+    #for idx, data in enumerate(inputs):
+    #    input_name = 'input{}'.format(idx)
+    #    input_shape = dyn_shapes[idx] if len(dyn_shapes)>0 and dyn_shapes[idx] is not None else data.shape
+    #    input_specs.append(
+    #        paddle.static.InputSpec(shape=input_shape, dtype=data.dtype, name=input_name)
+    #    )
+    #    # dump input
+    #    np.save(os.path.join(model_dir, "input{}".format(idx)), data)
+    #    input_tensor_list.append(paddle.to_tensor(data))
+
+    #paddle.jit.save(net, model_path, input_specs)
+    #output = net(*input_tensor_list)
+    #if isinstance(output, (tuple, list)):
+    #    for idx, out in enumerate(output):
+    #        np.save(os.path.join(model_dir, "output{}".format(idx)), out.numpy())
+    #else:
+    #    np.save(os.path.join(model_dir, "output{}".format(0)), output.numpy())
+
+    #return output
 
 '''
 export dyn model, along with input and output for reference.
@@ -87,7 +133,7 @@ def exportModel(name, dyn_func, input_data:list, target_dir:str, dyn_shapes:list
         )
 
         # dump input
-        np.save(os.path.join(model_dir, "input{}".format(idx)), data)        
+        np.save(os.path.join(model_dir, "input{}".format(idx)), data)
 
     paddle.jit.save(dyn_func, save_path, input_specs)
     print('saved exported model to {}'.format(save_path))
@@ -96,15 +142,15 @@ def exportModel(name, dyn_func, input_data:list, target_dir:str, dyn_shapes:list
     model = paddle.jit.load(save_path)
 
     result = model(*[input[:] for input in input_data])
-   
+
     # dump output for reference
     if isinstance(result, (tuple, list)):
         for idx, out in enumerate(result):
             np.save(os.path.join(model_dir, "output{}".format(idx)), out.numpy())
-    else:       
+    else:
         np.save(os.path.join(model_dir, "output{}".format(0)), result.numpy())
-    
-    if paddle.__version__ < "2.6.0": 
+
+    if paddle.__version__ < "2.6.0":
         paddle.fluid.core.clear_executor_cache()
     else:
         paddle.base.core.clear_executor_cache()
@@ -113,7 +159,7 @@ def exportModel(name, dyn_func, input_data:list, target_dir:str, dyn_shapes:list
 
 if __name__ == "__main__":
     np.set_printoptions(precision=2)
-    np.set_printoptions(suppress=True)  
+    np.set_printoptions(suppress=True)
 
     #x = np.random.randn(2,3).astype(np.float32)
     x = np.array([[[
@@ -123,7 +169,7 @@ if __name__ == "__main__":
     [
         [1, 2, 3],
         [4, 5, 6]
-    ]], 
+    ]],
     [[
         [1, 2, 3],
         [4, 5, 6]
diff --git a/src/frontends/paddle/tests/test_models/gen_wrapper.py b/src/frontends/paddle/tests/test_models/gen_wrapper.py
index 1516075546..5db5945c86 100644
--- a/src/frontends/paddle/tests/test_models/gen_wrapper.py
+++ b/src/frontends/paddle/tests/test_models/gen_wrapper.py
@@ -4,6 +4,7 @@
 import glob
 import os
 import subprocess
+import paddle
 import sys
 
 print(sys.argv)
@@ -16,9 +17,24 @@ out_folder = sys.argv[2]
 mark_file = os.path.join(out_folder, "generate_done.txt")
 
 gen_files = glob.glob(os.path.join(gen_folder, '**/generate_*.py'), recursive=True)
+failed_filelist = []
+
+enable_pir = os.getenv('FLAGS_enable_pir_api')
+if  enable_pir == "1":
+    failed_filelist = [
+    "generate_assign.py",
+    "generate_place_test_model.py"
+    ]
 
 for gen_script in gen_files:
     print("Processing: {} ".format(gen_script))
+    torun = True;
+    for file in failed_filelist:
+        if file in gen_script:
+            torun = False
+            break
+    if torun == False:
+        continue
     status = subprocess.run([sys.executable, gen_script, out_folder], env=os.environ)
     if status.returncode != 0:
         print("ERROR: PaddlePaddle model gen script FAILED: {}".format(gen_script))
diff --git a/src/frontends/paddle/tests/throw_in_conversion.cpp b/src/frontends/paddle/tests/throw_in_conversion.cpp
index 30cb1d64a1..c554d2359b 100644
--- a/src/frontends/paddle/tests/throw_in_conversion.cpp
+++ b/src/frontends/paddle/tests/throw_in_conversion.cpp
@@ -18,7 +18,7 @@ TEST(FrontEndConvertModelTest, throw_in_conversion) {
     OV_ASSERT_NO_THROW(frontEnd = fem.load_by_framework(PADDLE_FE));
     ASSERT_NE(frontEnd, nullptr);
     auto model_filename = FrontEndTestUtils::make_model_path(
-        std::string(TEST_PADDLE_MODELS_DIRNAME) + std::string("throw_in_conversion/throw_in_conversion.pdmodel"));
+        std::string(TEST_PADDLE_MODELS_DIRNAME) + std::string("throw_in_conversion/throw_in_conversion" + std::string(TEST_PADDLE_MODEL_EXT)));
     OV_ASSERT_NO_THROW(inputModel = frontEnd->load(model_filename));
     ASSERT_NE(inputModel, nullptr);
     std::shared_ptr<ov::Model> model;
@@ -32,7 +32,7 @@ TEST(FrontEndConvertModelTest, unsupported_version) {
     OV_ASSERT_NO_THROW(frontEnd = fem.load_by_framework(PADDLE_FE));
     ASSERT_NE(frontEnd, nullptr);
     auto model_filename = FrontEndTestUtils::make_model_path(std::string(TEST_PADDLE_MODELS_DIRNAME) +
-                                                             std::string("lower_version/lower_version.pdmodel"));
+                                                             std::string("lower_version/lower_version" + std::string(TEST_PADDLE_MODEL_EXT)));
 
     ASSERT_THROW(inputModel = frontEnd->load(model_filename), GeneralFailure);
 }
