#!/usr/bin/env python

#
# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache 2.0
#

from collections import defaultdict
from collections.abc import Mapping
import copy
import enum
import hashlib
import json
import os

BatchedImage = list


def splitOnBatch(files_list) -> BatchedImage:
    return files_list.split("|")


def if_file(file_path):
    return os.path.isfile(file_path)


def is_directory(directory_path):
    return os.path.isdir(directory_path)


def get_not_files(file_paths: list):
    return [f for f in file_paths if not if_file(f)]

def shape_to_list(shape):
    ret = shape
    if isinstance(shape, (str, bytes)):
        ret = json.loads(shape)
    return ret

def layout_to_str(layout):
    ret = layout
    if isinstance(layout, list):
        ret = "".join(layout)
    return ret

class FilesStorage:
    class SourceType(enum.IntEnum):
        image = 0
        bin = 1

    source_file_types = [ name for name, _ in SourceType.__members__.items()]
    source_description = f'''It is possible to specify three types of inputs:
two from the list {source_file_types} and a path to a JSON file contained inputs description."
Please look at the required JSON formats:\n
1) If you want to specify files as "{SourceType.image.name}", please use the following format:''' + '''
"{
    \\"my_model_input\\": {
        \\"files\\": [
            image_path_0,
            image_path_1,
            ...
        ],
        \\"type\\":\\"''' + SourceType.image.name + '''\\"
    }
}"
\n2) If you want to specify files as "''' + SourceType.image.name + '''", and convert them to different shape or data type please add a "convert" section and use the following format:''' + '''
"{
    \\"my_model_input\\": {
        \\"files\\": [
            image_path_0,
            image_path_1,
            ...
        ],
        \\"type\\":\\"''' + SourceType.image.name + '''\\",
        \\"convert\\":{\\"shape\\":\\"[1,3,372,500]\\", \\"element_type\\":\\"float32\\"}
    }
}"

\n3) If you want to specify files as "''' + SourceType.bin.name + '''", please use the following format:''' + '''
"{
    \\"my_model_input.1\\": {
        \\"files\\": [
            blob_path_0,
            blob_path_1,
            ...
        ],
        \\"type\\": \\"''' + SourceType.bin.name + '''\\",
        \\"shape\\": [1, 3, 299, 299],
        \\"layout\\": \\"NCHW\\",
        \\"element_type\": numpy.dtype.name
    }
}"
\n4) Or just specify a path to input descriptions in JSON files, which
are automatically generated by the tool every time it finishes inference.
Location of these files usually described by the paths:

    "<provider_name>/<model_name>/input_img.json"
or
    "<provider_name>/<model_name>/input_dump_data.json"

'''
    source_json_schema_allowable = {SourceType.image.name: {"files"},
                                    SourceType.bin.name  : {"files", "element_type", "shape"}}
    source_json_schema_forbidden = {SourceType.image.name: {"shape", "element_type"},
                                    SourceType.bin.name  : {"convert"}}
    def __init__(self):
        self.files_per_input_json = {}

    def __validate_image__(data):
        if "convert" in data.keys():
            if "shape" in data["convert"].keys():
                data["convert"]["shape"] = shape_to_list(data["convert"]["shape"])
            if "layout" in data["convert"].keys():
                data["convert"]["layout"] = layout_to_str(data["convert"]["layout"])

    def __validate_bin__(data):
        if "shape" in data.keys():
            data["shape"] = shape_to_list(data["shape"])
        if "layout" in data.keys():
            data["layout"] = layout_to_str(data["layout"])

    def __validate__(files_per_input_json) :
        for i,d in files_per_input_json.items():
            if "type" not in d.keys():
                raise RuntimeError(f"FilesStorage validity failed: important field \"type\" is absent for input source: \"{i}\": {d}\n. Please specify a one from: {FilesStorage.source_file_types}")
            if d["type"] not in FilesStorage.source_file_types:
                raise RuntimeError(f"FilesStorage validity failed: unsupported \"type\" value: \"{d['type']}\" for input source: \"{i}\": {d}\n. Please specify a correct one from: {FilesStorage.source_file_types}")
            if not FilesStorage.source_json_schema_allowable[d["type"]].issubset(set(d.keys())):
                raise RuntimeError(f"FilesStorage validity failed: some important fields: {FilesStorage.source_json_schema_allowable[d['type']]} are absent in the description of input source: \"{i}\": {d}\n. Please specify them all")
            if len(FilesStorage.source_json_schema_forbidden[d["type"]] & (set(d.keys()))):
                raise RuntimeError(f"FilesStorage validity failed: These fields: {FilesStorage.source_json_schema_forbidden[d['type']]} are not allowed in the description of input source: \"{i}\": {d}\n. Please remove them all.")

            if d["type"] == FilesStorage.source_file_types[int(FilesStorage.SourceType.image)]:
                FilesStorage.__validate_image__(d)
            if d["type"] == FilesStorage.source_file_types[int(FilesStorage.SourceType.bin)]:
                FilesStorage.__validate_bin__(d)

    @staticmethod
    def __parse_json_obj__(json_data):
        return_json_data = {}
        for io_name, io_data in json_data.items():
            if isinstance(io_data, Mapping):
                return_json_data[io_name] = io_data
                if "type" not in io_data.keys():
                    return_json_data[io_name]["type"] = FilesStorage.SourceType.bin.name
            else:
                return_json_data[io_name] = {"files": io_data, "type" :  FilesStorage.SourceType.image.name}
        FilesStorage.__validate__(return_json_data)
        return return_json_data

    def parse_inputs(self, console_input_files_list_per_model: str):
        if if_file(console_input_files_list_per_model):
            with open(console_input_files_list_per_model) as file:
                json_input = json.load(file)
        else:
            try:
                json_input = json.loads(console_input_files_list_per_model)
            except json.JSONDecodeError as ex:
                raise RuntimeError(f"Cannot parse JSON input: {repr(console_input_files_list_per_model)[1:-1]}. Error: {ex}") from None

        self.files_per_input_json = FilesStorage.__parse_json_obj__(json_input)

    def inputs(self):
        return self.files_per_input_json

class FilesPerModel:
    input_separator = ","

    def __init__(self):
        self.files_per_input = defaultdict(BatchedImage)
        self.files_per_output = defaultdict(BatchedImage)
        self.unnamed_files = []

    @staticmethod
    def __validate__(named, unnamed):
        # validate
        if len(named) != 0 and len(unnamed) != 0:
            raise RuntimeError(
                f'Each input file either must be preceded by INPUT/OUTPUT name in the format "INPUT<<PATH" / "OUTPUT>>PATH"  or not specified by INPUT/OUTPUT at all. Named files: {named} , unnamed files: {unnamed}'
            )

    @staticmethod
    def __parse_io_impl__(input_files_list_per_io: str, sep: str):
        input_package = input_files_list_per_io.split(sep)
        files_per_named_io = defaultdict(BatchedImage)
        files_per_unnamed_io = []

        if len(input_package) != 1:
            files_per_named_io[input_package[0]] = splitOnBatch(input_package[1])
        else:
            files_per_unnamed_io.extend(splitOnBatch(input_files_list_per_io))

        return files_per_named_io, files_per_unnamed_io

    def parse_inputs(self, console_input_files_list_per_model: str):
        for input_files_list_per_io in console_input_files_list_per_model.split(
            FilesPerModel.input_separator
        ):
            named, unnamed = FilesPerModel.__parse_io_impl__(
                input_files_list_per_io, "<<"
            )
            self.files_per_input.update(named)
            if len(unnamed) != 0:
                self.unnamed_files.append(unnamed)

        FilesPerModel.__validate__(self.files_per_input, self.unnamed_files)

    def parse_outputs(self, console_output_files_list_per_model: str):
        for output_files_list_per_io in console_output_files_list_per_model.split(
            FilesPerModel.input_separator
        ):
            named, unnamed = FilesPerModel.__parse_io_impl__(
                output_files_list_per_io, ">>"
            )
            self.files_per_output.update(named)
            if len(unnamed) != 0:
                self.unnamed_files.append(unnamed)

        FilesPerModel.__validate__(self.files_per_output, self.unnamed_files)

    def parse(self, console_input_files_list_per_model: str):
        for input_files_list_per_io in console_input_files_list_per_model.split(
            FilesPerModel.input_separator
        ):
            input_named_candidates, input_unnamed_cand = (
                FilesPerModel.__parse_io_impl__(input_files_list_per_io, "<<")
            )
            output_named_candidates, output_unnamed_cand = (
                FilesPerModel.__parse_io_impl__(input_files_list_per_io, ">>")
            )

            FilesPerModel.__validate__(input_named_candidates, input_unnamed_cand)
            FilesPerModel.__validate__(output_named_candidates, output_unnamed_cand)

            if len(input_unnamed_cand) == len(output_named_candidates):
                self.files_per_output.update(output_named_candidates)

            if len(output_unnamed_cand) == len(input_named_candidates):
                self.files_per_input.update(input_named_candidates)

            if len(output_unnamed_cand) == len(input_unnamed_cand):
                self.unnamed_files.append(output_unnamed_cand)

        # validate
        if (len(self.files_per_input) != 0 or len(self.files_per_output) != 0) and len(
            self.unnamed_files
        ) != 0:
            raise RuntimeError(
                f'Each input file either must be preceded by INPUT/OUTPUT name in the format "INPUT<<PATH" / "OUTPUT>>PATH"  or not specified by INPUT/OUTPUT at all. Names files: {self.files_per_input} and {self.files_per_output}, unnamed files: {self.unnamed_files}'
            )

    def inputs(self):
        non_files = []
        if len(self.files_per_input) != 0:
            for files in self.files_per_input.values():
                non_files.extend(get_not_files(files))
            if len(non_files) != 0:
                raise RuntimeError(
                    f"Cannot find input files: {non_files}. Please make sure they are exist and paths are valid"
                )
            return self.files_per_input.copy()
        if len(self.unnamed_files) != 0:
            for file_list in self.unnamed_files:
                non_files.extend(get_not_files(file_list))
            if len(non_files) != 0:
                raise RuntimeError(
                    f"Cannot find input files: {non_files}. Please make sure they are exist and paths are valid"
                )
            return self.unnamed_files.copy()
        raise RuntimeError("No input files")

    def outputs(self):
        if len(self.files_per_output) != 0:
            return self.files_per_output.copy()
        if len(self.unnamed_files) != 0:
            return self.unnamed_files.copy()
        raise RuntimeError("No output files")


class UseCaseFiles:
    use_case_separator = ";"

    def __init__(self):
        self.files_per_case = []

    def parse_all(self, console_io_files_list: str):
        if console_io_files_list is None:
            return

        file_paths_per_case = console_io_files_list.split(
            UseCaseFiles.use_case_separator
        )
        for case_files in file_paths_per_case:
            files_aggregator = FilesStorage()
            files_aggregator.parse(case_files)
            self.files_per_case.append(files_aggregator)

    def parse_inputs(self, console_input_files_list: str):
        if console_input_files_list is None:
            return

        file_paths_per_case = console_input_files_list.split(
            UseCaseFiles.use_case_separator
        )
        for case_files in file_paths_per_case:
            files_aggregator = FilesStorage()
            files_aggregator.parse_inputs(case_files)
            self.files_per_case.append(files_aggregator)


class Config:
    config_description = '''Expects information in JSON format implementing the schema:
"{
    \\"attr_name_0\\": \\"value_0\\",
    \\"attr_name_1\\": \\"value_1\\",
    ...
    \\"attr_name_N\\": \\"value_N\\",
}"
'''
    def __init__(self, cmd_argument: str):
        self.cfg_dict = {}
        if cmd_argument and len(cmd_argument) != 0:
            if not if_file(cmd_argument):
                self.cfg_dict = json.loads(cmd_argument)
            else:
                with open(cmd_argument, "r") as file:
                    isPlainText = False
                    try:
                        self.cfg_dict = json.load(file)
                    except json.JSONDecodeError as ex:
                        # not json file, probably just a plain file
                        isPlainText = True
                        pass

                    if isPlainText:
                        file.seek(0)
                        lines = file.readlines()
                        for line in lines:
                            line = line.strip()
                            if len(line) == 0  or line[0] == '#':
                                continue

                            kv_pairs = line.split()
                            if len(kv_pairs) != 2:
                                raise RuntimeError(f"Cannot parse config file: {cmd_argument}. It must be either JSON or a list of 'KEY\\tVALUE' pairs, encountered the failed line: \"{line}\"")
                            self.cfg_dict[kv_pairs[0]] = kv_pairs[1]


class ModelInfo:
    model_description = '''Expects information in JSON format implementing the schema:
"{
    \\"input_0\\": {
        \\"layout\\":\\"NCHW\\",
        \\"element_type\\":\\"float32\\",
        \\"shape\\": [1,2,3,4]
    },
    \\"input_1\\": {
        \\"shape\\": [2,3,4]
    }
}"
'''
    @staticmethod
    def __validate__(preproc_per_io):
        for i, d in preproc_per_io.items():
            if "shape" in d.keys():
                d["shape"] = shape_to_list(d["shape"])
            if "layout" in d.keys():
                d["layout"] = layout_to_str(d["layout"])

    def __init__(self, command_line_ppm=""):
        self.preproc_per_io = {}
        self.model_name = None
        if command_line_ppm and len(command_line_ppm) != 0:
            if if_file(command_line_ppm):
                with open(command_line_ppm) as file:
                    self.preproc_per_io = json.load(file)
            else:
                try:
                    self.preproc_per_io = json.loads(command_line_ppm)
                except Exception as ex:
                    raise RuntimeError(
                        ModelInfo.model_description + f"\nGot:\n{command_line_ppm}"
                    )
        ModelInfo.__validate__(self.preproc_per_io)

    def set_model_name(self, model_name : str):
        self.model_name = model_name

    def insert_info(self, io_name: str, info: {}):
        self.preproc_per_io[io_name] = info
        ModelInfo.__validate__(self.preproc_per_io)

    def update_info(self, io_name: str, additional_info: {}):
        for k,v in additional_info.items():
            self.preproc_per_io[io_name][k] = v
        ModelInfo.__validate__(self.preproc_per_io)

    def get_model_io_names(self):
        return self.preproc_per_io.keys()

    def get_model_io_info(self, io_name: str):
        if io_name not in self.get_model_io_names():
            raise RuntimeError(f"Cannot find input/output: {io_name} for a model among: {self.get_model_io_names()}")
        return self.preproc_per_io[io_name]

class ModelInfoPrinter:
    def __init__(self):
        pass

    def serialize_model_info(self, base_directory : str, model_path : str, orig_model_info : ModelInfo):
        model_info = copy.deepcopy(orig_model_info)
        base_directory = os.path.join(*base_directory.split("/"))
        os.makedirs(base_directory, exist_ok=True)

        utter_model_info = {}
        for minput_name in model_info.get_model_io_names():
            utter_model_info[minput_name] = model_info.get_model_io_info(minput_name)

        if not model_info.model_name or len(model_info.model_name) == 0:
            model_info.model_name = os.path.basename(model_path).split('.')[0]

        model_info_json_path = os.path.join(base_directory,  model_info.model_name + "_info.json")

        with open(model_info_json_path, "w") as outfile:
            json.dump(utter_model_info, outfile)

        # add meta information
        for node_info in utter_model_info.values():
            if "shape" in node_info.keys():
                node_info["shape"] = node_info["shape"]
        model_meta_info = {}
        model_meta_info["model_path"] = model_path
        model_meta_info["model_info_path"] = model_info_json_path
        sha256 = hashlib.sha256()
        sha256.update( os.path.abspath(__file__).encode('utf-8'))
        utter_model_info["_meta_" + sha256.hexdigest()] = model_meta_info
        return json.dumps(utter_model_info,indent=4)


class TensorInfo:
    necessary_attrs = {"data", "bytes_size", "model"}
    ext_attrs = ["element_type", "shape"]

    types={"input", "output"}
    def __init__(self):
        self.info = {}

    def set_type(self, ttype:str):
        if ttype not in TensorInfo.types:
            raise RuntimeError(f"Cannot specify type: {ttype} for TensorInfo, available types: {TensorInfo.types}")
        self.info["type"] = ttype

    def get_type(self) -> str:
        return self.info["type"]

    def validate(self):
        if not TensorInfo.necessary_attrs.issubset(self.info.keys()):
            raise RuntimeError(f"Fields: {TensorInfo.necessary_attrs} are required in {self.info}")

        if "shape" in self.info.keys():
            self.info["shape"] = shape_to_list(self.info["shape"])
        if "layout" in self.info.keys():
            self.info["layout"] = layout_to_str(self.info["layout"])

class TensorsInfoPrinter:
    canonization_table = {
        "<": "%%_lt_%%",
        ">": "%%_gt_%%",
        ":": "%%_colon_%%",
        "\"": "%%_dquote_%%",
        "/": "%%_fslash_%%",
        "\\": "%%_bslash_%%",
        "|": "%%_pipe_%%",
        "?": "%%_quest_%%",
        "*": "%%_asterix_%%"
    }
    decanonization_table = {v: k for k, v in canonization_table.items()}

    def __init__(self):
        pass

    @staticmethod
    def canonize_io_name(io_name):
        return "".join([TensorsInfoPrinter.canonization_table.get(c, c) for c in io_name])

    @staticmethod
    def decanonize_io_name(io_name):
        return "".join([TensorsInfoPrinter.decanonization_table.get(c, c) for c in io_name])

    @staticmethod
    def get_file_name_to_dump_model_source(source: str):
        return source + "s_dump_data.json"

    @staticmethod
    def canonize_file_name(file_name : str):
        file_name = "".join(str(file_name).split())   # remove spaces
        file_name = "_".join(str(file_name).split(","))   # remove spaces
        return file_name

    @staticmethod
    def get_printable_tensor_name(info):
        file_name = ""
        if info["type"] == "input":
            file_name = "idata_"
            for attr in [attr for attr in info.keys() if attr in TensorInfo.ext_attrs]:
                file_name += TensorsInfoPrinter.canonize_file_name(info[attr])
                file_name += "_"
        elif info["type"] == "output":
            file_name = "odata_"
            for attr in [attr for attr in info.keys() if attr in TensorInfo.ext_attrs]:
                file_name += TensorsInfoPrinter.canonize_file_name(info[attr])
                file_name += "_"
        if len(file_name) == 0:
            raise RuntimeError(f"Cannot compose tensor name from the info: {info}")
        file_name = file_name[0:-1] + ".blob"
        return file_name

    def get_printable_input_tensor_info(self, input_tensors_dict:list):
        printable_tensor_info = {}
        for info in input_tensors_dict:
            if info["type"] != "input":
                continue
            input_files = info['input_files']['files']
            tensor_source = {}
            tensor_source["files"] = info['input_files']['files']
            tensor_source["type"] = info['input_files']['type']
            if tensor_source["type"] == FilesStorage.source_file_types[int(FilesStorage.SourceType.image)]:
                tensor_source["convert"] = {}
                tensor_source["convert"]["shape"] = info['shape']
                tensor_source["convert"]["layout"] = layout_to_str(info['layout'])
                tensor_source["convert"]["element_type"] = info['element_type']
            else:
                tensor_source["shape"] = info['shape']
                tensor_source["layout"] = layout_to_str(info['layout'])
                tensor_source["element_type"] = info['element_type']

            printable_tensor_info[info['source']] = tensor_source
        return printable_tensor_info

    def get_printable_output_tensor_info(self, input_tensors_dict:list):
        printable_tensor_info = {}
        for info in input_tensors_dict:
            if info["type"] != "output":
                continue

            tensor_source = {}
            if "input_files" in info.keys():
                tensor_source["files"] = info['input_files']['files']
                tensor_source["type"] = info['input_files']['type']

            if "layout" in info.keys():
                tensor_source["layout"] = layout_to_str(info['layout'])

            tensor_source["shape"] = info['shape']
            tensor_source["element_type"] = info['element_type']
            tensor_source["type"] = 'bin'
            printable_tensor_info[info['source']] = tensor_source
        return printable_tensor_info

    def get_printable_tensor_info(self, input_tensors_dict:list, ttype):
        return  self.get_printable_input_tensor_info(input_tensors_dict) if  ttype == "input" else self.get_printable_output_tensor_info(input_tensors_dict)


    def serialize_tensors_by_type(self, base_directory : str, input_tensors_dict:list, ttype):
        base_directory = os.path.join(*base_directory.split("/"))
        printable_input_tensor_info = self.get_printable_tensor_info(input_tensors_dict, ttype)
        printable_tensor_dump_info = copy.deepcopy(printable_input_tensor_info)

        # make sure the base directory exists
        serialzied_file_paths = []
        main_model_dir = ""
        try:
            os.makedirs(base_directory, exist_ok=True)
            for info in input_tensors_dict:
                if ttype and len(ttype) !=0 and info["type"] != ttype:
                    continue

                # create model directory
                # we will dump aggregated input/output tensors info in JSON into it
                main_model_dir = os.path.join(base_directory, info["model"])

                # create model input/output directory
                main_model_source_dir = os.path.join(main_model_dir, info["type"])
                os.makedirs(main_model_source_dir, exist_ok=True)

                # well known filesystems forbid special symbols in string paths
                # apply canonization
                canonized_fs_input_name = TensorsInfoPrinter.canonize_io_name(info["source"])

                # dump input tensor info per input/output in JSON
                if ttype == "input":
                    if printable_input_tensor_info[info["source"]]["type"] != FilesStorage.SourceType.bin.name:
                        with open(os.path.join(main_model_source_dir, canonized_fs_input_name + "_img.json"), "w") as outfile:
                            json.dump({info["source"] : printable_input_tensor_info[info["source"]]}, outfile)

                file_directory = os.path.join(main_model_source_dir, canonized_fs_input_name)
                os.makedirs(file_directory, exist_ok=True)

                file_path = os.path.join(file_directory, TensorsInfoPrinter.get_printable_tensor_name(info))
                with open(file_path, "wb") as input_tensor_file:
                    input_tensor_file.write(info["data"])
                serialzied_file_paths.append(file_path)

                # dump binary tensor info per input/output in JSON
                printable_tensor_dump_info[info["source"]]["files"] = [file_path]   # print as list
                printable_tensor_dump_info[info["source"]]["type"] =  FilesStorage.SourceType.bin.name
                if "convert" in printable_tensor_dump_info[info["source"]].keys():
                    printable_tensor_dump_info[info["source"]].update(printable_tensor_dump_info[info["source"]]["convert"])
                    del printable_tensor_dump_info[info["source"]]["convert"]
                with open(os.path.join(main_model_source_dir, canonized_fs_input_name + "_dump.json"), "w") as outfile:
                    json.dump({info["source"] : printable_tensor_dump_info[info["source"]]}, outfile)
        except Exception as ex:
            raise RuntimeError(f"Cannot serialize tensor of type: {ttype} into a file, error: {ex}")

        # store aggregated model I/O info as JSON
        input_info_file_path = os.path.join(main_model_dir, ttype + "s_img.json")
        input_info_dumps_file_path = os.path.join(main_model_dir, TensorsInfoPrinter.get_file_name_to_dump_model_source(ttype))
        if len(main_model_dir) != 0:
            with open(input_info_file_path, "w") as outfile:
                json.dump(printable_input_tensor_info, outfile)
        if len(main_model_dir) != 0:
            with open(input_info_dumps_file_path, "w") as outfile:
                json.dump(printable_tensor_dump_info, outfile)

        return serialzied_file_paths,input_info_file_path, input_info_dumps_file_path

    def deserialize_output_tensor_descriptions(self, base_directory : str, model_name : str):
        ttype = "output"
        if ttype not in TensorInfo.types:
            raise RuntimeError(f"Incorrect tensor type to deserialize: {ttype}. Expected: {TensorInfo.types}")

        base_directory = os.path.join(*base_directory.split("/"))
        if not is_directory(base_directory):
            raise RuntimeError(f"Cannot deserialize tensors as the provider directory doesn't exist: {base_directory}")

        main_model_dir = os.path.join(base_directory, model_name)
        if not is_directory(base_directory):
            raise RuntimeError(f"Cannot deserialize tensors as the model info directory doesn't exist: {main_model_dir}")

        model_sources_info_file_path = os.path.join(main_model_dir, TensorsInfoPrinter.get_file_name_to_dump_model_source(ttype))
        if not if_file(model_sources_info_file_path):
            raise RuntimeError(f"Cannot deserialize tensors as the model info file doesn't exist: {model_sources_info_file_path}")

        model_sources_info = {}
        try:
            with open(model_sources_info_file_path, "r") as file:
                model_sources_info = json.load(file)
        except json.JSONDecodeError as ex:
            raise RuntimeError(f"The file: {model_sources_info_file_path} contains no JSON data. Error: {ex}") from None

        for io, data in model_sources_info.items():
            necessary_fields = FilesStorage.source_json_schema_allowable[FilesStorage.SourceType.bin.name]
            if not necessary_fields.issubset(data.keys()):
                raise RuntimeError(f"Cannot deserialize tensor info from file: {model_sources_info_file_path}, as one from required fields are missing: {necessary_fields}")
            if "layout" in data.keys():
                data["layout"] = layout_to_str(data['layout'])
        return model_sources_info
