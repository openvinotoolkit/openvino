
/*******************************************************************************
 * Copyright (c) 2022-2025 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

namespace KERNEL_NAME {
#include "cm_pa_common.hpp"

#ifdef CM_HAS_LSC_UNTYPED_2D
#define USE_LSC 1
#else
#define USE_LSC 0
#endif

extern "C" _GENX_MAIN_ void KERNEL_NAME(
    //query [q_len, num_heads, S]
    half* query [[type("svmptr_t")]],
#if CMPA_KVCACHE_U8
    int8_t* k_cache [[type("svmptr_t")]],
    int8_t* v_cache [[type("svmptr_t")]],
#else
    half* k_cache [[type("svmptr_t")]],
    half* v_cache [[type("svmptr_t")]],
#endif
    int32_t* past_lens [[type("svmptr_t")]],
    int32_t* block_indices [[type("svmptr_t")]],
    int32_t* block_indices_begins [[type("svmptr_t")]],
    int32_t* subsequence_begins [[type("svmptr_t")]],
    half* output [[type("svmptr_t")]],
#if SPARSE_BLOCK_SIZE > 1
    bool* sparse_block_mask [[type("svmptr_t")]],
    bool* sparse_block_mask_wg [[type("svmptr_t")]],
    int q_len,
    int num_q_blocks,
    int num_k_blocks,
    // validate sparse atten process
    bool validate) {
#else
    int q_len) {
#endif
    constexpr int is_causal = CMFLA_IS_CAUSAL;
    constexpr int num_heads = CMFLA_NUM_HEADS;
    constexpr int head_size = CMFLA_HEAD_SIZE;
    constexpr int num_kv_heads = CMFLA_NUM_KV_HEADS;
    constexpr int pa_block_sz = CMPA_BLOCK_SZ;
    //# query [q_len, num_heads, S]
    //# k_cache [kv_len, num_heads, S]
    //# v_cache [kv_len, num_heads, S]
#if CMPA_KVCACHE_U8
    constexpr uint K_SLM_SIZE = (4*kv_step * head_size * sizeof(half));
    constexpr uint V_SLM_SIZE = (4*kv_step * head_size * sizeof(half));
    constexpr uint Q_SLM_SIZE = 0;

    cm_slm_init(K_SLM_SIZE + V_SLM_SIZE + Q_SLM_SIZE);

    auto slm_K = cm_slm_alloc(K_SLM_SIZE);
    auto slm_V = cm_slm_alloc(V_SLM_SIZE);

#endif
    auto batch = cm_group_id(0);
    auto h = cm_group_id(1);
    auto hkv = h / (num_heads/num_kv_heads);
    auto wg_id = cm_group_id(2); // each work-group handles a sequence
    auto wg_local_id = cm_local_id(2);
    int local_size = cm_local_size(2);

    int q_start_sg, kv_start, kv_seq_len, q_len_sg;

    // multiple work-groups are required to split a sequence,
    // need to figure out which part of query-tokens to process
    int wg_seq_len = local_size * q_step;
    int past_q_lens = past_lens[0];
    kv_start = 0;
    kv_seq_len = q_len + past_q_lens;
    q_start_sg = (wg_id * local_size + wg_local_id) * q_step;
    q_len_sg = q_step;
    if (q_start_sg + q_len_sg > q_len) {
        q_len_sg = q_len - q_start_sg;
    }

    // qkv is fused
    int kv_stop = kv_seq_len;
    if constexpr (is_causal) {
        /*
        --------------------------------
        |       |       |       |       |
        |  00   |       |       |       |
        |       |       |       |       |
         --------------------------------
        |       |       |       |       |
        |  10   |  11   |       |       |
        |       |       |       |       |
        ---------------------------------
        |       |       |       |       |
        |  20   |  21   |  22   |       |
        |       |       |       |       |
         ---------------------------------
        |       |       |       |       |
        |  30   |  31   |   32  |   33  |
        |       |       |       |       |
        ---------------------------------
        each grid can be [q_len_per_trunk, q_len_per_trunk].
        For each trunk, [q_len_per_trunk, past_q_lens] must be calculated. Such as: `20`,`21`. but for the 22,
        casual mask optimization can be applied. differnt wgs would has different kv stop.
        //todo:kv_stop is wg level, should we change to sg level?
               sglevel would cause sgs in one wg diverge. so leave for now. also one wg has same kvstop makes eaiser for kv copying/loading into SLM/cache.
    */
        kv_stop = (wg_id + 1) * wg_seq_len + past_q_lens;
        if (kv_stop > kv_seq_len) kv_stop = kv_seq_len;
    }

    //Q/O[B, L, H, S]
    uint q_offset = (q_start_sg*num_heads + h)*head_size;

#if SPARSE_BLOCK_SIZE > 1
    bool *block_mask_base, *wg_block_mask_base;
    if (validate) {
        //# sparse_block_mask [num_heads, num_q_blocks, num_k_blocks]
        //# sparse_block_mask_wg [num_heads, wg_count_along_query, num_k_blocks]
        auto q_start_block = q_start_sg/ SPARSE_BLOCK_SIZE;
        block_mask_base = sparse_block_mask + (h * num_q_blocks + q_start_block) * num_k_blocks;
        wg_block_mask_base = sparse_block_mask_wg + (h * cm_group_count(2) + wg_id) * num_k_blocks;
    }
 #endif

#if CMPA_KVCACHE_U8
    uint kv_offset = hkv*(head_size+4)*pa_block_sz;
    pa_lsc_u8<is_causal, num_heads, num_kv_heads, head_size, 0>(
                            slm_K,
                            slm_V,
                            wg_local_id,
                            local_size,
                            q_start_sg, //q_start for SG,
                            kv_stop,
                            q_len_sg, //q_step,
                            kv_seq_len, //kv_len,
                            reinterpret_cast<svmptr_t>(query + q_offset),
                            reinterpret_cast<svmptr_t>(k_cache + kv_offset),
                            reinterpret_cast<svmptr_t>(v_cache + kv_offset),
#if SPARSE_BLOCK_SIZE > 1
                            reinterpret_cast<svmptr_t>(block_mask_base),
                            reinterpret_cast<svmptr_t>(wg_block_mask_base),
                            validate,
#endif
                            reinterpret_cast<svmptr_t>(output + q_offset),
                            past_q_lens,
                            block_indices);
#else
    uint kv_offset = hkv*head_size*pa_block_sz;
    pa_kernel_lsc_prefetch_f16<is_causal, num_heads, num_kv_heads, head_size, 0, 16>(
                            wg_local_id,
                            q_start_sg, //q_start for SG,
                            kv_stop,
                            q_len_sg, //q_step,
                            kv_seq_len, //kv_len,
                            reinterpret_cast<svmptr_t>(query + q_offset),
                            reinterpret_cast<svmptr_t>(k_cache + kv_offset),
                            reinterpret_cast<svmptr_t>(v_cache + kv_offset),
#if SPARSE_BLOCK_SIZE > 1
                            reinterpret_cast<svmptr_t>(block_mask_base),
                            reinterpret_cast<svmptr_t>(wg_block_mask_base),
                            validate,
#endif
                            reinterpret_cast<svmptr_t>(output + q_offset),
                            past_q_lens,
                            block_indices);
#endif
}
} // namespace KERNEL_NAME