/*******************************************************************************
 * Copyright (c) 2018-2026 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

// xe-1:8, xe-2:16
#if XE_ARCH==1
#define REG_N 8
#define USE_LSC_BLOCK_2D_DESC 0
#else
#define REG_N 16
#define USE_LSC_BLOCK_2D_DESC 1
#endif

#define SystolicDepth 8
#define RepeatCount 1
#define VNNI_WIDTH 2
#define REG_K (SystolicDepth * VNNI_WIDTH)
#define REG_M RepeatCount
#define KV_PARTITION_STEP_NUM  (KV_PARTITION_SIZE / KV_STEP)

#if KV_CACHE_COMPRESSION
    // scale/zp is half-precision, so size = 2 * 2 = 4 bytes
    #define KV_SCALE_ZP_SIZE 4 // scale/zp bytes
    #define KV_ELEMENT_TYPE uint8_t
#else
    #define KV_SCALE_ZP_SIZE 0 // no scale/zp
    #define KV_ELEMENT_TYPE half
#endif

//prepack [K, N] to [K/2, N, 2] layout.
template <typename T1, typename T2, int K, int N>
inline void prepack_to_VNNI_W2(matrix_ref<T1, K, N> input, matrix_ref<T2, K/2, N*2> out) {
    #pragma unroll
    for (int r = 0; r < K/2; r++) {
        out.row(r).select<N, 2>(0) = input.row(r*2);
        out.row(r).select<N, 2>(1) = input.row(r*2+1);
    }
}

template <typename T1, typename T2>
CM_INLINE void Transpose_8x8(matrix_ref<T1, 8, 8> in, matrix_ref<T2, 8, 8> out) {
    matrix<T2, 8, 8> temp;
    #pragma unroll
    for (int i = 0; i < 4; ++i) {
        temp.row(i)     = in.template select<2, 1, 4, 2>(i * 2, 0);
        temp.row(i + 4) = in.template select<2, 1, 4, 2>(i * 2, 1);
    }

    #pragma unroll
    for (int i = 0; i < 4; ++i) {
        out.row(i * 2)     = temp.template select<4, 1, 2, 4>(0, i);
        out.row(i * 2 + 1) = temp.template select<4, 1, 2, 4>(4, i);
    }
}

extern "C" _GENX_MAIN_ void KERNEL_NAME(
    half* query [[type("svmptr_t")]],
    KV_ELEMENT_TYPE* key [[type("svmptr_t")]],
    KV_ELEMENT_TYPE* value [[type("svmptr_t")]],
    int* past_lens [[type("svmptr_t")]],
    int* block_indices [[type("svmptr_t")]],
    int* block_indices_begins [[type("svmptr_t")]],
    int* subsequence_begins [[type("svmptr_t")]],
    float* output [[type("svmptr_t")]],
    float* lse [[type("svmptr_t")]],
    int q_len// 1
    ) {
    //# batch=1, seq_num=1 or >1
    //# query [seq_idx, seq_num, head_num, head_size]
    //# output[seq_idx, seq_num, head_num, head_size]
    //#   key [block_num, kv_head_num, block_size, head_size] + [block_num, kv_head_num, block_size, 4] (scale/zp)
    //# value [block_num, kv_head_num, block_size, head_size] + [block_num, kv_head_num, block_size, 4] (scale/zp)

    //# KV_PARTITION_SIZE should be multiple of kv_block_size(KV_BLOCK_SIZE)
    //# kv_len dimision will be split into multiple partitions, each WG process a partition
    //# total_partitions_num = kv_len // KV_PARTITION_SIZE
    //
    //# MaxRepeatCount=8
    //# q_heads_per_kv_head = num_heads // num_kv_heads
    //# q_head_chunks_per_kv_head = (q_heads_per_kv_head + (MaxRepeatCount - 1)) // MaxRepeatCount
    //# q_head_chunk_size = num_heads // (num_kv_heads * q_head_chunks_per_kv_head)
    //# GWS=[seq_num, num_kv_heads * q_head_chunks_per_kv_head, total_partitions_num]
    //# LWS=[1, 1, 1]

    //# Each WG processes a partition, which is KV_PARTITION_SIZE long and multiple of KV_BLOCK_SIZE.
    //# KV_BLOCK_SIZE can be 32/64/128/256, etc.
    const uint seq_idx = cm_global_id(0);
    const uint kv_head_num_idx = cm_global_id(1) / Q_head_chunks_per_kv_head;
    const uint head_num_idx = cm_global_id(1) * Q_head_chunk_size;
    //# KV_PARTITION_SIZE --> EU thread
    const uint wg_thread_id = cm_global_id(2);
    const uint kv_partition_num = cm_group_count(2);
    const uint kv_partition_idx = cm_group_id(2);

    const uint kv_len = past_lens[seq_idx] + 1;
    // The code here requires KV_PARTITION_SIZE to be an integer multiple of KV_BLOCK_SIZE.
    const uint start_block_idx = block_indices_begins[seq_idx] + kv_partition_idx * (KV_PARTITION_SIZE / KV_BLOCK_SIZE);

    if(kv_partition_idx * KV_PARTITION_SIZE > kv_len) {
        return;
    }
    const uint total_blocks_num = (kv_len + KV_BLOCK_SIZE - 1) / KV_BLOCK_SIZE;
    constexpr uint kv_pitch = HEAD_SIZE * sizeof(KV_ELEMENT_TYPE);

    //# Load Q into register(as dpas-A tile)
    const uint qo_offset = (seq_idx * HEADS_NUM * q_len + head_num_idx) * HEAD_SIZE;

    matrix <half, Q_head_chunk_size, HEAD_SIZE> Qmat = 0;
    cm_svm_block_read<half, Q_head_chunk_size * HEAD_SIZE>((svmptr_t)(query + qo_offset), Qmat.format<half>());

    constexpr uint per_v_block_element_num = KV_BLOCK_SIZE * KV_HEADS_NUM * (HEAD_SIZE + KV_SCALE_ZP_SIZE); // 4 bytes: scale/zp
    #if KV_CACHE_COMPRESSION_BY_TOKEN
        constexpr uint per_k_block_element_num = KV_BLOCK_SIZE * KV_HEADS_NUM * (HEAD_SIZE + KV_SCALE_ZP_SIZE); // 4 bytes: scale/zp
    #else
        constexpr uint per_k_block_element_num = KV_HEADS_NUM * HEAD_SIZE * (KV_BLOCK_SIZE + KV_SCALE_ZP_SIZE); // 4 bytes: scale/zp
    #endif
    uint block_num = KV_PARTITION_SIZE / KV_BLOCK_SIZE;

    uint leftover_size = 0;
    if(kv_partition_idx == kv_partition_num - 1) {
        // last partition
        leftover_size = (kv_len - KV_PARTITION_SIZE * kv_partition_idx) % KV_PARTITION_SIZE;
    }
    if(block_num > total_blocks_num - start_block_idx) {
        block_num = total_blocks_num - start_block_idx;
    }

    //# rS = Q @ Kt
	matrix<float, Q_head_chunk_size, REG_M * KV_PARTITION_STEP_NUM * REG_N> rS = 0;

    // # Each SG can process multiple blocks
    #pragma unroll
    for(uint block_idx = 0, ki = 0; block_idx < block_num; block_idx++) {
        // split kv_partition into multi kv_block
        uint blk_indices = block_indices[start_block_idx + block_idx];
        uint k_base_offset = blk_indices * per_k_block_element_num + kv_head_num_idx * (per_k_block_element_num / KV_HEADS_NUM);
        uint k_scale_zp_offset = k_base_offset + KV_BLOCK_SIZE * HEAD_SIZE; // scale/zp offset

    #if USE_LSC_BLOCK_2D_DESC
        #if KV_CACHE_COMPRESSION
            // Transpose only support dword and qwork
            lsc::block_2d_desc<uint, 1, REG_N, REG_K/4> b2dK(reinterpret_cast<uint*>(key + k_base_offset),  KV_BLOCK_SIZE - 1, HEAD_SIZE*sizeof(uint8_t) - 1, kv_pitch - 1, 0, 0);
        #else
            lsc::block_2d_desc<uint, 1, REG_N, REG_K/2> b2dK(reinterpret_cast<uint*>(key + k_base_offset),  KV_BLOCK_SIZE - 1, HEAD_SIZE*sizeof(half) - 1, kv_pitch - 1, 0, 0);
        #endif
    #else
        uint kv_offset = k_base_offset;
        uint kv_stride = HEAD_SIZE;
        uint kv_x0 = 0, kv_y0 = 0;
        uint kv_x1 = HEAD_SIZE*sizeof(KV_ELEMENT_TYPE);
        uint kv_y1 = KV_BLOCK_SIZE;
    #endif

        uint kv_pos_end = KV_BLOCK_SIZE;
        if(block_idx == block_num - 1 && leftover_size > 0) {
            kv_pos_end = leftover_size % KV_BLOCK_SIZE;
            if(kv_pos_end == 0) kv_pos_end = KV_BLOCK_SIZE;
        }

        #if KV_CACHE_COMPRESSION
            #if KV_CACHE_COMPRESSION_BY_TOKEN
            // load scale/zp
            vector<half, KV_BLOCK_SIZE> scale_vec;
            vector<half, KV_BLOCK_SIZE> zp_vec;
            cm_svm_block_read(reinterpret_cast<svmptr_t>(key + k_scale_zp_offset), scale_vec);
            cm_svm_block_read(reinterpret_cast<svmptr_t>(key + k_scale_zp_offset + KV_BLOCK_SIZE * sizeof(half)), zp_vec);
            if(kv_pos_end < KV_BLOCK_SIZE) {
                // fill leftover with last valid scale/zp
                #pragma unroll
                for(int i = kv_pos_end; i < KV_BLOCK_SIZE; i++) {
                    scale_vec[i] = 0.0;
                    zp_vec[i] = 0.0;
                }
            }
            #else
            // load scale/zp
            vector<half, HEAD_SIZE> scale_vec;
            vector<half, HEAD_SIZE> zp_vec;
            cm_svm_block_read(reinterpret_cast<svmptr_t>(key + k_scale_zp_offset), scale_vec);
            cm_svm_block_read(reinterpret_cast<svmptr_t>(key + k_scale_zp_offset + HEAD_SIZE * sizeof(half)), zp_vec);
            #endif
        #endif

        for(int kv_pos = 0; kv_pos < kv_pos_end; kv_pos += KV_STEP, ki++) {
            #if KV_CACHE_COMPRESSION && KV_CACHE_COMPRESSION_BY_TOKEN
                #if USE_LSC_BLOCK_2D_DESC
                    vector<half, REG_N * 2> temp_scale, temp_zp;
                    temp_scale.select<REG_N,2>(0) = scale_vec.select<REG_N,1>(kv_pos);
                    temp_scale.select<REG_N,2>(1) = scale_vec.select<REG_N,1>(kv_pos);
                    temp_zp.select<REG_N,2>(0) = zp_vec.select<REG_N,1>(kv_pos);
                    temp_zp.select<REG_N,2>(1) = zp_vec.select<REG_N,1>(kv_pos);
                #else
                    matrix<half, REG_N, REG_K> temp_scale;
                    matrix<half, REG_N, REG_K> temp_zp;
                    #pragma unroll
                    for(int kk=0; kk < REG_K; kk++) {
                        temp_scale.select<REG_N, 1, 1, 1>(0, kk) = scale_vec.select<REG_N,1>(kv_pos);
                        temp_zp.select<REG_N, 1, 1, 1>(0, kk) = zp_vec.select<REG_N,1>(kv_pos);
                    }
                #endif
            #endif

            #pragma unroll
            #if KV_CACHE_COMPRESSION
                #if !USE_LSC_BLOCK_2D_DESC && KV_CACHE_COMPRESSION_BY_TOKEN && XE_ARCH==1
                for(int k = 0, ri = 0; k < HEAD_SIZE/4; k += (REG_K/4)*4, ri += 4 ) {
                #else
                for(int k = 0, ri = 0; k < HEAD_SIZE/4; k += REG_K/4, ri ++ ) {
                #endif
            #else
                for(int k = 0, ri = 0; k < HEAD_SIZE/2; k += REG_K/2, ri ++ ) {
            #endif
                // matrix<half, REG_K, REG_N> Kt = 0;
                #if !USE_LSC_BLOCK_2D_DESC && KV_CACHE_COMPRESSION && KV_CACHE_COMPRESSION_BY_TOKEN && XE_ARCH==1
                matrix<uint8_t, REG_N, REG_K*4> temp4; // 8 x 64
                uint cur_kv_offset4 = kv_offset + kv_pos * kv_stride + k * 4;
                #pragma unroll
                for(int kk = 0; kk < REG_N; kk++) {
                    vector<uint, (REG_K*4)/4> tmp_dw;
                    cm_svm_block_read<uint, (REG_K*4)/4>((svmptr_t)(reinterpret_cast<uint*>(key + cur_kv_offset4 + kk * kv_stride)), tmp_dw);
                    temp4[kk].format<uint>() = tmp_dw;
                }
                #pragma unroll
                for(int ur = 0; ur < 4; ur++) {
                    if (ri + ur >= HEAD_SIZE / REG_K) break;
                    matrix<half, REG_K, REG_N> Kt = 0;
                    matrix<uint8_t, REG_N, REG_K> temp; // 8 x 16
                    #pragma unroll
                    for(int kk = 0; kk < REG_N; kk++) {
                        temp[kk] = temp4[kk].select<REG_K,1>(ur*REG_K);
                    }
                    matrix<half, REG_N, REG_K> KtNormal = temp;
                    KtNormal = KtNormal - temp_zp;
                    KtNormal = cm_mul<half>(KtNormal, temp_scale);
                    Transpose_8x8(KtNormal.format<uint, REG_N, REG_K/2>(), Kt.format<uint, REG_K/2, REG_N>());
                    {
                        // matrix<half, Q_head_chunk_size, REG_K> Qmat_data = Qmat.select<Q_head_chunk_size,1,REG_K,1>(0, (ri+ur)*REG_K);
                        matrix<half, Q_head_chunk_size, REG_K> Qmat_data = Qmat.select<Q_head_chunk_size,1,REG_K,1>(0, (ri+ur)*REG_K);
                        matrix<float, Q_head_chunk_size, REG_N> rS_data = 0;
                        rS_data = cm_dpas<CM_PRECISION_HF, CM_PRECISION_HF, SystolicDepth, Q_head_chunk_size>(
                                    rS_data.format<float>(),
                                    Kt.format<int32_t>(),
                                    Qmat_data.format<int32_t>());
                        // rS.select<Q_head_chunk_size, 1, REG_N, 1>(0, (ki+ur)*REG_N) += rS_data;
                        rS.select<Q_head_chunk_size, 1, REG_N, 1>(0, ki*REG_N) += rS_data;
                    }
                }
                continue;
                #endif
                matrix<half, REG_K, REG_N> Kt = 0;
            #if USE_LSC_BLOCK_2D_DESC
                //# Load Kt into register & pack as VNNI(as dpas-B tile)
                //# DWORD transposed load == (transposed + VNNI) load
                b2dK.set_block_x(k);
                #if KV_CACHE_COMPRESSION
                    matrix<uint8_t, REG_K, REG_N> Kt_quant_temp, Kt_quant;
                    cm_load<lsc::Transpose>(Kt_quant_temp.format<uint>(), b2dK.set_block_y(kv_pos));
                    auto quant_src = Kt_quant_temp.format<ushort, REG_K/2, REG_N>();
                    auto quant_dst = Kt_quant.format<ushort, REG_K/2, REG_N>();

                    #pragma unroll
                    for(int r = 0; r < REG_K / 2; r += 2) {
                        quant_dst.row(r  ) = quant_src.select<2,1,8,2>(r,0);
                        quant_dst.row(r+1) = quant_src.select<2,1,8,2>(r,1);
                    }
                    #if KV_CACHE_COMPRESSION_BY_TOKEN
                    #pragma unroll
                    for(int r = 0; r < REG_K; r++) {
                        Kt[r] = Kt_quant[r] - temp_zp.format<half, 2, REG_N>()[r%2]; //vector - vector
                        Kt[r] = cm_mul<half>(Kt[r], temp_scale.format<half, 2, REG_N>()[r%2]);    // vector * vector
                    }
                    #else
                    vector<half, REG_K> temp_scale, temp_zp;
                    temp_scale.select<REG_K, 1>(0) = scale_vec.select<REG_K, 1>(k * 4);
                    temp_zp.select<REG_K, 1>(0) = zp_vec.select<REG_K, 1>(k * 4);

                    auto Kt_dequant_out = Kt.format<half, REG_K/2, 2*REG_N>();
                    auto Kt_dequant_tmp = Kt_quant.format<uint8_t, REG_K/2, 2*REG_N>();
                    #pragma unroll
                    for(int r = 0; r < REG_K/2; r++) {
                        Kt_dequant_out[r].select<REG_N, 2>(0) = Kt_dequant_tmp[r].select<REG_N, 2>(0) - temp_zp[r*2];
                        Kt_dequant_out[r].select<REG_N, 2>(0) = cm_mul<half>(Kt_dequant_out[r].select<REG_N, 2>(0), temp_scale[r*2]);
                        Kt_dequant_out[r].select<REG_N, 2>(1) = Kt_dequant_tmp[r].select<REG_N, 2>(1) - temp_zp[r*2+1];
                        Kt_dequant_out[r].select<REG_N, 2>(1) = cm_mul<half>(Kt_dequant_out[r].select<REG_N, 2>(1), temp_scale[r*2+1]);
                    }
                    #endif
                #else
                    cm_load<lsc::Transpose>(Kt.format<uint>(), b2dK.set_block_y(kv_pos));
                #endif
            #else
                #if KV_CACHE_COMPRESSION
                    matrix<uint8_t, REG_N, REG_K> temp; // 8 x 16
                    uint cur_kv_offset = kv_offset + kv_pos * kv_stride + k * 4;
                    #pragma unroll
                    for(int kk = 0; kk < REG_N; kk++) {
                        cm_svm_block_read<uint8_t, REG_K>((svmptr_t)(key + cur_kv_offset + kk * kv_stride), temp[kk].format<uint8_t>());
                    }
                    #if KV_CACHE_COMPRESSION_BY_TOKEN
                    matrix<half, REG_N, REG_K> KtNormal = temp;
                    KtNormal = KtNormal - temp_zp;
                    KtNormal = cm_mul<half>(KtNormal, temp_scale);
                    #else
                    vector<half, REG_K> temp_scale, temp_zp;
                    temp_scale.select<REG_K, 1>(0) = scale_vec.select<REG_K, 1>(k * 4);
                    temp_zp.select<REG_K, 1>(0) = zp_vec.select<REG_K, 1>(k * 4);
                    matrix<half, REG_N, REG_K> KtNormal;
                    #pragma unroll
                    for(int kk = 0; kk < REG_N; kk++) {
                        KtNormal[kk] = temp[kk] - temp_zp;
                        KtNormal[kk] = cm_mul<half>(KtNormal[kk], temp_scale);
                    }
                    #endif
                #else
                    matrix<uint, REG_N, REG_K/2> KtNormal;
                    uint cur_kv_offset = kv_offset + kv_pos * kv_stride + k * 2;
                    #pragma unroll
                    for(int kk = 0; kk < REG_N; kk++) {
                        cm_svm_block_read<uint, REG_K/2>((svmptr_t)(key + cur_kv_offset + kk * kv_stride), KtNormal[kk].format<uint>());
                    }
                #endif
                #if XE_ARCH==1
                Transpose_8x8(KtNormal.format<uint, REG_N, REG_K/2>(), Kt.format<uint, REG_K/2, REG_N>());
                #else
                Transpose_8x8(KtNormal.format<uint, REG_N, REG_K/2>().select<8,1,8,1>(0,0), Kt.format<uint, REG_K/2, REG_N>().select<8,1,8,1>(0,0));
                Transpose_8x8(KtNormal.format<uint, REG_N, REG_K/2>().select<8,1,8,1>(8,0), Kt.format<uint, REG_K/2, REG_N>().select<8,1,8,1>(0,8));
                #endif
            #endif
            {
                matrix<half, Q_head_chunk_size, REG_K> Qmat_data = Qmat.select<Q_head_chunk_size,1,REG_K,1>(0, ri*REG_K);
                matrix<float, Q_head_chunk_size, REG_N> rS_data = 0;
                rS_data = cm_dpas<CM_PRECISION_HF, CM_PRECISION_HF, SystolicDepth, Q_head_chunk_size>(
                            rS_data.format<float>(),
                            Kt.format<int32_t>(),
                            Qmat_data.format<int32_t>());
                rS.select<Q_head_chunk_size, 1, REG_N, 1>(0, ki*REG_N) += rS_data;
            }
            }
        }
    }

    // online softmax
    vector<float, Q_head_chunk_size> cur_sum = 0.0f;
    vector<float, Q_head_chunk_size> cur_lse = 0.0f;
    matrix<half, Q_head_chunk_size, KV_PARTITION_STEP_NUM * REG_M * REG_N> Pmat = 0;
    #pragma unroll
    for(int qi = 0; qi < Q_head_chunk_size; qi++) {
        auto rS_slice = rS[qi].format<float, KV_PARTITION_STEP_NUM, REG_N>();
        rS_slice = cm_mul<float>(rS_slice, (float)SCALE_FACTOR);  // convert scale_factor into (float), or it will be promoted to double

        if(leftover_size > 0) {
            auto Svec = rS_slice.format<float>();
            for(int i = leftover_size; i < KV_PARTITION_STEP_NUM * REG_N; i++){
                Svec[i] = -3e38f;
            }
        }

        // compute lse
        constexpr float log2e = 1.4426950408889634f;
        constexpr float loge2 = 0.6931471805599453f;

        // compute row_max
        auto rSv = rS_slice.format<float>();
        #if XE_ARCH==1
        float row_max = cm_reduced_max<float>(rSv);
        #else
        float row_max = rSv[0];
        // It is performance hotspot for u8,  must add unroll
        #if KV_CACHE_COMPRESSION
        #pragma unroll
        #endif
        for(int r = 1; r < rSv.n_elems(); r++)
            row_max = cm_max<float>(row_max, rSv[r]);
        #endif

        // compute Pmat = exp(rS_slice - row_max)
        vector<float, KV_PARTITION_STEP_NUM * REG_N> rS_exp_temp = cm_exp((rSv - row_max)*log2e);
        Pmat[qi].format<half, KV_PARTITION_STEP_NUM * REG_M, REG_N>() = rS_exp_temp;

        cur_lse[qi] = cm_sum<float>(rS_exp_temp.format<float>());
        cur_lse[qi] = cm_log<float>(cur_lse[qi]) * loge2 + row_max; // log2(sum(exp(x))) = log2e * log(sum(exp(x)))

        // compute row sum of P
        cur_sum[qi] = cm_sum<float>(Pmat[qi]);
    }
    
    #if !USE_LSC_BLOCK_2D_DESC && KV_CACHE_COMPRESSION
    constexpr uint VALUE_TILE_NUM=2;
    #else
    constexpr uint VALUE_TILE_NUM=1;
    #endif
    //# rO = P * V
    matrix <float, Q_head_chunk_size, HEAD_SIZE/REG_N * REG_M*REG_N> Omat = 0;
    #pragma unroll
    for(uint block_idx = 0, ki = 0; block_idx < block_num; block_idx++) {
        uint blk_indices = block_indices[start_block_idx + block_idx];
        uint v_base_offset = blk_indices * per_v_block_element_num + kv_head_num_idx * (per_v_block_element_num / KV_HEADS_NUM);
        uint v_scale_zp_offset = v_base_offset + KV_BLOCK_SIZE * HEAD_SIZE; // scale/zp offset

    #if USE_LSC_BLOCK_2D_DESC
        //# vector load cannot be used for block_2d_desc
        //# note: candidate template ignored: deduced type 'details::Block2DRefTy<half, 1U, 16U, 1U, (LoadOp)0U>' (aka 'vector_ref<half,32>') of 1st parameter
        #if KV_CACHE_COMPRESSION
        lsc::block_2d_desc<uint8_t, 1, REG_K, REG_N> b2dV(value + v_base_offset,  KV_BLOCK_SIZE - 1, HEAD_SIZE*sizeof(uint8_t) - 1, kv_pitch - 1, 0, 0);
        #else
        lsc::block_2d_desc<half, 1, REG_K, REG_N>   b2dV(value + v_base_offset,  KV_BLOCK_SIZE - 1, HEAD_SIZE * sizeof(half) - 1, kv_pitch - 1, 0, 0);
        #endif
    #else
        uint kv_offset = v_base_offset;
        uint kv_stride = HEAD_SIZE;
        uint kv_x0 = 0, kv_y0 = 0;
        uint kv_x1 = HEAD_SIZE*sizeof(half);
        uint kv_y1 = KV_BLOCK_SIZE;
    #endif
        uint kv_pos_end = KV_BLOCK_SIZE;
        if(block_idx == block_num - 1 && leftover_size > 0) {
            kv_pos_end = leftover_size % KV_BLOCK_SIZE;
            if(kv_pos_end == 0) kv_pos_end = KV_BLOCK_SIZE;
        }

        #if KV_CACHE_COMPRESSION
            // load scale/zp
            vector<half, KV_BLOCK_SIZE> scale_vec;
            vector<half, KV_BLOCK_SIZE> zp_vec;
            cm_svm_block_read(reinterpret_cast<svmptr_t>(value + v_scale_zp_offset), scale_vec);
            cm_svm_block_read(reinterpret_cast<svmptr_t>(value + v_scale_zp_offset + KV_BLOCK_SIZE * sizeof(half)), zp_vec);
            if(kv_pos_end < KV_BLOCK_SIZE) {
                // fill leftover with last valid scale/zp
                #pragma unroll
                for(int i = kv_pos_end; i < KV_BLOCK_SIZE; i++) {
                    scale_vec[i] = 0.0;
                    zp_vec[i] = 0.0;
                }
            }
        #endif
        #pragma unroll
        for(int kv_pos = 0; kv_pos < kv_pos_end; kv_pos += REG_K, ki++) {
            #if KV_CACHE_COMPRESSION
            vector<half, REG_K> temp_scale = scale_vec.select<REG_K, 1>(kv_pos);
            vector<half, REG_K> temp_zp = zp_vec.select<REG_K, 1>(kv_pos);
            matrix<uint8_t, REG_K, REG_N * VALUE_TILE_NUM> Vt_quant;
            #endif
            #pragma unroll
            #if !USE_LSC_BLOCK_2D_DESC && KV_CACHE_COMPRESSION && XE_ARCH==1
            for(int k = 0, ri = 0; k < HEAD_SIZE; k += (REG_N * VALUE_TILE_NUM) * 4, ri += VALUE_TILE_NUM * 4 ) {
            #else
            for(int k = 0, ri = 0; k < HEAD_SIZE; k += REG_N * VALUE_TILE_NUM, ri += VALUE_TILE_NUM ) {
            #endif
                // Load V into register & pack as VNNI(as dpas-B tile)
                matrix<half, REG_K, REG_N * VALUE_TILE_NUM> VmatNormal;
                matrix<half, REG_K, REG_N * VALUE_TILE_NUM> Vmat;
            #if USE_LSC_BLOCK_2D_DESC
                b2dV.set_block_x(k); // x is the column index
                #if KV_CACHE_COMPRESSION
                    cm_load<lsc::Normal>(Vt_quant.format<uint8_t>(), b2dV.set_block_y(kv_pos));
                #else
                    cm_load<lsc::VNNI>(Vmat.format<half>(), b2dV.set_block_y(kv_pos)); // y is the row index
                #endif
            #else
                #if KV_CACHE_COMPRESSION
                    #if XE_ARCH==1
                    matrix<uint8_t, REG_K, REG_N * VALUE_TILE_NUM * 4> Vt_quant4;
                    uint cur_kv_offset4 = kv_offset + kv_pos * kv_stride + k;
                    #pragma unroll
                    for(int kk = 0; kk < REG_K; kk++) {
                        vector<uint, (REG_N * VALUE_TILE_NUM * 4)/4> tmp_dw;
                        cm_svm_block_read<uint, (REG_N * VALUE_TILE_NUM * 4)/4>((svmptr_t)(reinterpret_cast<uint*>(value + cur_kv_offset4 + kk * kv_stride)), tmp_dw);
                        Vt_quant4[kk].format<uint>() = tmp_dw;
                    }
                    #pragma unroll
                    for(int ur = 0; ur < 4; ur++) {
                        if (k + ur * (REG_N * VALUE_TILE_NUM) >= HEAD_SIZE) break;
                        #pragma unroll
                        for(int kk = 0; kk < REG_K; kk++) {
                            Vt_quant[kk] = Vt_quant4[kk].select<REG_N * VALUE_TILE_NUM, 1>(ur * (REG_N * VALUE_TILE_NUM));
                        }
                        int ri_ur = ri + ur * VALUE_TILE_NUM;

                        VmatNormal = Vt_quant;
                        #pragma unroll
                        for(int r = 0; r < REG_K; r++) {
                            VmatNormal[r] = VmatNormal[r] - temp_zp[r]; // vector - scalar
                            VmatNormal[r] = cm_mul<half>(VmatNormal[r], temp_scale[r]); // vector * scalar
                        }

                        if(kv_pos_end - kv_pos < KV_STEP) {
                            #pragma unroll
                            for(int r = kv_pos_end; r < KV_STEP; r++)  {
                                VmatNormal[r] = 0;
                            }
                        }
                        auto Vref = Vmat.format<half, REG_K/2, 2 * REG_N * VALUE_TILE_NUM>();
                        Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 0) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(0, 0);
                        Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 1) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(1, 0);

                        // somtimes KV cache would be filled with random Nan, so need to clean up the unused value data.
                        #if CLEAN_UNUSED_KVCACHE
                            if(kv_pos_end - kv_pos < KV_STEP) {
                                auto VmatRef = Vmat.format<half, REG_K/2, REG_N*2*VALUE_TILE_NUM>();
                                uint valid_rows = kv_pos_end - kv_pos;
                                uint valid_rows_vnni = (valid_rows+1)/2;
                                #pragma unroll
                                for (int r = valid_rows_vnni; r < REG_K/2; r++) {
                                    VmatRef.row(r) = 0.0f;
                                }
                                if (valid_rows % 2 == 1) {
                                    VmatRef.row(valid_rows_vnni-1).select<REG_N*VALUE_TILE_NUM, 2>(1) = 0.0f;
                                }
                            }
                        #endif
                        matrix<half, Q_head_chunk_size, REG_K> Pmat_data = Pmat.select<Q_head_chunk_size,1,REG_K,1>(0, ki*REG_K);
                        #pragma unroll
                        for (int tile=0; tile < VALUE_TILE_NUM; tile ++) {
                            auto Omat_data = Omat.select<Q_head_chunk_size,1,REG_N,1>(0, (ri_ur + tile)*REG_N);
                            Omat_data = cm_dpas<CM_PRECISION_HF, CM_PRECISION_HF, SystolicDepth, Q_head_chunk_size>(
                                        Omat_data.format<float>(),
                                        Vmat.format<half, REG_K/2, REG_N*2*VALUE_TILE_NUM>().select<REG_K/2, 1, REG_N*2, 1>(0, REG_N*2*tile).format<int32_t>(),
                                        Pmat_data.format<int32_t>());
                        }
                    }
                    continue;
                    #else
                    uint cur_kv_offset = kv_offset + kv_pos * kv_stride + k;
                    #pragma unroll
                    for(int kk = 0; kk < REG_K; kk++) {
                        cm_svm_block_read<uint8_t, REG_N * VALUE_TILE_NUM>((svmptr_t)(value + cur_kv_offset + kk * kv_stride), Vt_quant[kk].format<uint8_t>());
                    }
                    #endif
                #else
                    uint cur_kv_offset = kv_offset + kv_pos * kv_stride + k;
                    #pragma unroll
                    for(int kk = 0; kk < REG_K; kk++) {
                        cm_svm_block_read<half, REG_N * VALUE_TILE_NUM>((svmptr_t)(value + cur_kv_offset + kk * kv_stride), VmatNormal[kk].format<half>());
                    }
                    auto Vref = Vmat.format<half, REG_K/2, 2 * REG_N * VALUE_TILE_NUM>();
                    Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 0) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(0, 0);
                    Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 1) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(1, 0);
                #endif
            #endif

            #if KV_CACHE_COMPRESSION
                VmatNormal = Vt_quant;
                #pragma unroll
                for(int r = 0; r < REG_K; r++) {
                    VmatNormal[r] = VmatNormal[r] - temp_zp[r]; // vector - scalar
                    VmatNormal[r] = cm_mul<half>(VmatNormal[r], temp_scale[r]); // vector * scalar
                }

                if(kv_pos_end - kv_pos < KV_STEP) {
                    #pragma unroll
                    for(int r = kv_pos_end; r < KV_STEP; r++)  {
                        VmatNormal[r] = 0;
                    }
                }
                auto Vref = Vmat.format<half, REG_K/2, 2 * REG_N * VALUE_TILE_NUM>();
                Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 0) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(0, 0);
                Vref.select<REG_K/2, 1, REG_N * VALUE_TILE_NUM, 2>(0, 1) = VmatNormal.select<REG_K/2, 2, REG_N * VALUE_TILE_NUM, 1>(1, 0);
            #endif

            // somtimes KV cache would be filled with random Nan, so need to clean up the unused value data.
            #if CLEAN_UNUSED_KVCACHE
                if(kv_pos_end - kv_pos < KV_STEP) {
                    auto VmatRef = Vmat.format<half, REG_K/2, REG_N*2*VALUE_TILE_NUM>();
                    uint valid_rows = kv_pos_end - kv_pos;
                    uint valid_rows_vnni = (valid_rows+1)/2;
                    #pragma unroll
                    for (int r = valid_rows_vnni; r < REG_K/2; r++) {
                        VmatRef.row(r) = 0.0f;
                    }
                    if (valid_rows % 2 == 1) {
                        VmatRef.row(valid_rows_vnni-1).select<REG_N*VALUE_TILE_NUM, 2>(1) = 0.0f;
                    }
                }
            #endif
            matrix<half, Q_head_chunk_size, REG_K> Pmat_data = Pmat.select<Q_head_chunk_size,1,REG_K,1>(0, ki*REG_K);
            #pragma unroll
            for (int tile=0; tile < VALUE_TILE_NUM; tile ++) {
                auto Omat_data = Omat.select<Q_head_chunk_size,1,REG_N,1>(0, (ri + tile)*REG_N);
                Omat_data = cm_dpas<CM_PRECISION_HF, CM_PRECISION_HF, SystolicDepth, Q_head_chunk_size>(
                            Omat_data.format<float>(),
                            Vmat.format<half, REG_K/2, REG_N*2*VALUE_TILE_NUM>().select<REG_K/2, 1, REG_N*2, 1>(0, REG_N*2*tile).format<int32_t>(),
                            Pmat_data.format<int32_t>());
            }
            }
        }
    }

    // save Output
    #pragma unroll
    for (int qi = 0; qi < Q_head_chunk_size; qi++) {
        matrix<float, REG_M, REG_N> cur_O_f32;
        uint o_offset = seq_idx * kv_partition_num * HEADS_NUM * HEAD_SIZE + kv_partition_num * (head_num_idx + qi) * HEAD_SIZE + wg_thread_id * HEAD_SIZE;
        float div_cur_sum = 1.0/cur_sum[qi];
        auto Omat_slice = Omat[qi].format<float, HEAD_SIZE/REG_N * REG_M, REG_N>();
        #pragma unroll
        for(int k = 0, ri=0; k < HEAD_SIZE; k += REG_N, ri++) {
            auto cO = Omat_slice[ri].format<float, REG_M, REG_N>();
            #if XE_ARCH==1
            cur_O_f32 = cm_mul<float>(cO, div_cur_sum);
            #else
            cur_O_f32 = cm_div_ieee(cO, cur_sum[qi]);
            #endif
            cm_svm_block_write<float, REG_N>((svmptr_t)(output + o_offset + k), cur_O_f32.format<float>());
        }
        uint lse_offset = seq_idx * HEADS_NUM * kv_partition_num + (head_num_idx + qi) * kv_partition_num + wg_thread_id;
        lse[lse_offset] = cur_lse[qi];
    }
}

