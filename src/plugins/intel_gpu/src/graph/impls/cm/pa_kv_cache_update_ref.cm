/*******************************************************************************
 * Copyright (c) 2022-2025 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

#include <cm/cm.h>
#include <cm/cmtl.h>

#ifndef ATTR
#define ATTR [[type("svmptr_t")]]
#define ATTR_BUF [[type("buffer_t")]]
#endif

constexpr uint wg_size = WG_SIZE;
#define REG_K 16

// extern "C" _GENX_MAIN_ void pa_kv_cache_update(
extern "C" _GENX_MAIN_ void KERNEL_NAME(
    const half* key [[type("svmptr_t")]],
    const half* value [[type("svmptr_t")]],
    const int32_t* past_lens [[type("svmptr_t")]],
    const int32_t* block_indices [[type("svmptr_t")]],
    const int32_t* block_indices_begins [[type("svmptr_t")]],
    const int32_t* subsequence_begins [[type("svmptr_t")]],
#if KV_CACHE_COMPRESSION_PER_TOKEN
    uint8_t* key_cache [[type("svmptr_t")]],
    uint8_t* value_cache [[type("svmptr_t")]],
#else
    half* key_cache [[type("svmptr_t")]],
    half* value_cache [[type("svmptr_t")]],
#endif   
    uint32_t key_pitch,
    uint32_t key_offset,
    uint32_t value_pitch,
    uint32_t value_offset,
    uint32_t batch_size_in_sequences) {
    // # key:   [batch_size_in_tokens, num_kv_heads * k_head_size]
    // # value  [batch_size_in_tokens, num_kv_heads * v_head_size]
    // # key_cache:   [num_blocks, num_heads, block_size, k_head_size]
    // # value_cache: [num_blocks, num_heads, block_size, v_head_size]
    // 
    // # past_lens: [sequences_num]
    // # subsequence_begins: [sequences_num + 1]
    // # block_indices: [used_blocks_num]
    // # block_indices_begins: [sequences_num + 1]

    // wg_count = aligned_to(batch_size_in_tokens, wg_size) // wg_size
    // # GWS [1, num_heads, wg_count * wg_size]
    // # LWS [1, 1, wg_size]

    const auto head_idx = cm_group_id(1);
    const auto wg_id = cm_group_id(2);
    const auto wg_local_id = cm_local_id(2);
    const auto local_size = cm_local_size(2);

    // static_assert(local_size == wg_size);

    // const uint token_idx = wg_id * local_size + wg_local_id;
    const uint token_idx = cm_global_id(2);

    // token_idx -> subsequence_idx
    // if (token_idx >= subsequence_begins[batch_size_in_sequences]) return;
    uint subsequence_idx = 0;
    for (uint i = 0; i < batch_size_in_sequences; i++) {
        if (token_idx >= subsequence_begins[i] && token_idx < subsequence_begins[i + 1]) {
            subsequence_idx = i;
            break;
        }
    }

    // printf("wg:%d.%d, token_idx: %d, subsequence_idx: %d\n", wg_id, wg_local_id, token_idx, subsequence_idx);

    const uint subsequence_begin_idx = subsequence_begins[subsequence_idx];
    const uint past_len = past_lens[subsequence_idx];
    const uint current_block_idx = (past_len + token_idx - subsequence_begin_idx) / PAGED_ATTENTION_BLOCK_SIZE;
    const uint token_start_pos = (past_len + token_idx - subsequence_begin_idx) % PAGED_ATTENTION_BLOCK_SIZE;
    const uint block_offset = block_indices_begins[subsequence_idx] + current_block_idx;

    if (token_idx >= subsequence_begins[batch_size_in_sequences]) {
        #if KV_CACHE_COMPRESSION_PER_TOKEN
        #else
        // In PTL some V cache are written with NAN or random value due to unknown reason, while PA kernel will leverage lsc cm_load to
        // load V cache by 16x16 block with vnni format, it is hard to exclude the unused V cache when NAN is involved in the same 16x16 block.
        // Once NAN takes part in dpas, the NAN will propagate and cause result become NAN.
        // As a WA, we need to set the unused part(in the same 16 row) of V cache to 0 here.
        const uint last_token_idx = (past_len + 1) % PAGED_ATTENTION_BLOCK_SIZE;
        const uint last_token_idx_aligned = (last_token_idx + REG_K - 1) / REG_K * REG_K;

        // if (token_idx >= last_token_idx && token_idx < PAGED_ATTENTION_BLOCK_SIZE) {
        if (token_idx >= last_token_idx && token_idx < last_token_idx_aligned) {
            uint block_k_base_offset = ((past_len + 1) / PAGED_ATTENTION_BLOCK_SIZE) * KV_HEADS_NUM * ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
            uint key_out_offset = block_k_base_offset + head_idx * ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE + token_idx * ADJUSTED_K_HEAD_SIZE;
            vector<short, V_HEAD_SIZE> zero_data = 0;
            //if(token_idx == last_token_idx_aligned - 1) {
            //    zero_data[18] = 0xFE00; //test NAN
            //}

            // Only reset unused part in the same 16 row for V cache.
            // cm_ptr_store<int, K_HEAD_SIZE / 2>((int*)key_cache, key_out_offset * (int)sizeof(half), zero_data.format<int>());
            cm_ptr_store<int, V_HEAD_SIZE / 2>((int*)value_cache, key_out_offset * (int)sizeof(half), zero_data.format<int>());

            if(0) {
                const uint block_idx = key_out_offset / (ADJUSTED_K_HEAD_SIZE * KV_HEADS_NUM * PAGED_ATTENTION_BLOCK_SIZE);
                const uint head_idx = (key_out_offset % (ADJUSTED_K_HEAD_SIZE * KV_HEADS_NUM * PAGED_ATTENTION_BLOCK_SIZE)) / (ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE);
                const uint block_m = (key_out_offset % (ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE)) / ADJUSTED_K_HEAD_SIZE;
                const uint block_n = (key_out_offset % ADJUSTED_K_HEAD_SIZE);

                if(cm_global_id(0)==0 && cm_global_id(1)==0)
                    printf("token_idx = %d, last_token_idx = %d, subsequence_begins[%d] = %d, past_len = %d, out_token_idx = %d, key_out_offset = %d, reset_block = [%d, %d,%d,%d]\n",
                            token_idx, last_token_idx, batch_size_in_sequences, subsequence_begins[batch_size_in_sequences], past_len,
                            key_out_offset/(ADJUSTED_K_HEAD_SIZE * KV_HEADS_NUM), key_out_offset, block_idx, head_idx, block_m, block_n);
            }
        }
        #endif
        return;
    }

    #if KV_CACHE_COMPRESSION_PER_TOKEN
    // Assume: K_HEAD_SIZE == K_HEAD_SIZE
    auto quantize_and_store = [&](vector<half, K_HEAD_SIZE> data, uchar* out, uint out_offset, uint token_pos) {
            uint scale_offset = out_offset + K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE + token_pos * sizeof(half);
            half max_val = cm_reduced_max<half>(data);
            half min_val = cm_reduced_min<half>(data);
            half scale_val = half(0.0);
            half zp_val = half(0.0);
            if(max_val == min_val) {
                scale_val = half(0.0);
                zp_val = max_val;
            } else {
                scale_val = 255.0 / (max_val - min_val);
                zp_val = (0.0 - min_val) * scale_val;
            }
            vector<half, K_HEAD_SIZE>  dequant_data = cm_mul<half>(data, scale_val) + zp_val;
            vector<uchar, K_HEAD_SIZE> data_u8 = cm_rnde<uchar, K_HEAD_SIZE>(dequant_data);
            cm_ptr_store<uint32_t, K_HEAD_SIZE / 4>((uint32_t*)(out + out_offset + token_pos * K_HEAD_SIZE), 0, data_u8.format<uint32_t>());
            half *out_scale_zp = (half*)(out + scale_offset);
            out_scale_zp[0] = (max_val - min_val) / 255.0;
            out_scale_zp[PAGED_ATTENTION_BLOCK_SIZE] = zp_val;
    };
    #endif

    {
        uint block_k_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * ADJUSTED_K_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
        uint key_out_offset = block_k_base_offset + token_start_pos * K_HEAD_SIZE;
        uint key_in_offset = token_idx * key_pitch + head_idx * K_HEAD_SIZE + key_offset;

        vector<half, K_HEAD_SIZE> key_data;
        key_data.format<int>() = cm_ptr_load<int, K_HEAD_SIZE / 2>((int*)key, key_in_offset * (int)sizeof(half));

        #if KV_CACHE_COMPRESSION_PER_TOKEN
        quantize_and_store(key_data, (uchar*)key_cache, block_k_base_offset, token_start_pos);
        #else
        cm_ptr_store<int, K_HEAD_SIZE / 2>((int*)key_cache, key_out_offset * (int)sizeof(half), key_data.format<int>());
        #endif
    }
    {
        uint block_v_base_offset = (block_indices[block_offset] * KV_HEADS_NUM + head_idx) * ADJUSTED_V_HEAD_SIZE * PAGED_ATTENTION_BLOCK_SIZE;
        uint value_out_offset = block_v_base_offset + token_start_pos * V_HEAD_SIZE;
        uint value_in_offset = token_idx * value_pitch + head_idx * V_HEAD_SIZE + value_offset;

        //if(token_idx==0 && head_idx==0)
        //{
        //    printf("value_pitch = %d, value_in_offset: %d, value_out_offset: %d,V_HEAD_SIZE = %d, ADJUSTED_V_HEAD_SIZE = %d\n",
        //            value_pitch, value_in_offset, value_out_offset, V_HEAD_SIZE, ADJUSTED_V_HEAD_SIZE);
        //}

        vector<half, V_HEAD_SIZE> value_data;
        value_data.format<int>() = cm_ptr_load<int, V_HEAD_SIZE / 2>((int*)value, value_in_offset * (int)sizeof(half));
        #if KV_CACHE_COMPRESSION_PER_TOKEN
        quantize_and_store(value_data, (uchar*)value_cache, block_v_base_offset, token_start_pos);
        #else
        cm_ptr_store<int, V_HEAD_SIZE / 2>((int*)value_cache, value_out_offset * (int)sizeof(half), value_data.format<int>());
        #endif
    }
}
