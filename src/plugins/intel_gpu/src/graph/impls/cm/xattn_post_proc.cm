/*******************************************************************************
 * Copyright (c) 2018-2026 Intel Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *******************************************************************************/

namespace KERNEL_NAME {
#include "estimate.hpp"

extern "C" _GENX_MAIN_ void KERNEL_NAME(svmptr_t block_mask ATTR, svmptr_t merged_block_mask ATTR, uint q_stride_pad, uint q_block_pad, uint k_block_pad) {
    // block_mask:                [b, hq, q_block_pad, k_block_pad]
    // merged_block_mask:         [b, hq, q_block_pad/MERGED_Q_NUM, k_block_pad]
    // global:                    [q_block_pad/MERGED_Q_NUM, hq, b]
    const int TOKEN_IN_BLOCK = BLOCK_SIZE / STRIDE;
    uint m_mereged = cm_group_id(0);
    uint hq = cm_group_id(1);
    uint b = cm_group_id(2);
    block_mask += (b * HQ + hq) * q_block_pad * k_block_pad;
    block_mask += m_mereged * MERGED_Q_NUM * k_block_pad;
    merged_block_mask += (b * HQ + hq) * cm_group_count(0) * k_block_pad;
    merged_block_mask += m_mereged * k_block_pad;

    // mask shape: [div_up(q_len, xattn_block_size), rnd_up(k_stride, WG_N) / sum_per_n_token_in_block]
    // k_block_pad is aligned to 16 or 32, as WG_N of xattn_gemmQK is 128 or 256, and sum_per_n_token_in_block is 8 or 16 w.r.t
    // xattn_block_size 128 or 256 respectively.
    // Hence it is safe to not handle tailing here.
    int j = 0;
    {
        constexpr int STEP = 32;
        for (; j + STEP <= k_block_pad; j += STEP) {
            vector<uchar, STEP> new_mask = cm_ptr_load<int, STEP / 4>((int*)block_mask, j).format<uchar>();
            #pragma unroll
            for (int i = 1; i < MERGED_Q_NUM; i++) {
                if (m_mereged * MERGED_Q_NUM + i < q_stride_pad / TOKEN_IN_BLOCK) {
                    vector<uchar, STEP> cur_mask = cm_ptr_load<int, STEP / 4>((int*)block_mask, j + i * k_block_pad).format<uchar>();
                    new_mask |= cur_mask;
                }
            }
            cm_ptr_store<int, STEP / 4>((int*)merged_block_mask, j, new_mask.format<int>());
        }
    }
    {
        constexpr int STEP = 16;
        for (; j < k_block_pad; j += STEP) {
            vector<uchar, STEP> new_mask = cm_ptr_load<int, STEP / 4>((int*)block_mask, j).format<uchar>();
            #pragma unroll
            for (int i = 1; i < MERGED_Q_NUM; i++) {
                if (m_mereged * MERGED_Q_NUM + i < q_stride_pad / TOKEN_IN_BLOCK) {
                    vector<uchar, STEP> cur_mask = cm_ptr_load<int, STEP / 4>((int*)block_mask, j + i * k_block_pad).format<uchar>();
                    new_mask |= cur_mask;
                }
            }
            cm_ptr_store<int, STEP / 4>((int*)merged_block_mask, j, new_mask.format<int>());
        }
    }
}

}  // NAMESPACE
