{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Sparsification and Quantization of PyTorch models with POT\n",
    "\n",
    "The goal of this tutorial is to demonstrate how to use the Post-training Quantization with POT to optimize a PyTorch model for the high-speed inference via OpenVINO™ Toolkit. The optimization process contains the following steps:\n",
    "\n",
    "1. Evaluate the original model.\n",
    "2. Transform the original model to a quantized one.\n",
    "3. Export optimized and original models to OpenVINO IR.\n",
    "\n",
    "This tutorial uses a ResNet-50 model, pre-trained on Tiny ImageNet, which contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. The tutorial will demonstrate that only a tiny part of the dataset is needed for the post-training quantization, not demanding the fine-tuning of the model.\n",
    "\n",
    "\n",
    "> **NOTE**: This notebook requires that a C++ compiler is accessible on the default binary search path of the OS you are running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from openvino.runtime import Core, serialize\n",
    "from openvino.tools import mo\n",
    "from openvino.tools.pot import load_model, IEEngine, create_pipeline, compress_model_weights, save_model\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "BASE_MODEL_NAME = \"resnet50\"\n",
    "QUANTIZED_MODEL_NAME = BASE_MODEL_NAME + \"_int8\"\n",
    "IMAGE_SIZE = [64, 64]\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where PyTorch and OpenVINO IR models will be stored.\n",
    "fp32_checkpoint_filename = Path(BASE_MODEL_NAME + \"_fp32\").with_suffix(\".pth\")\n",
    "fp32_onnx_path = OUTPUT_DIR / Path(BASE_MODEL_NAME + \"_fp32\").with_suffix(\".onnx\")\n",
    "fp32_ir_path = OUTPUT_DIR / Path(BASE_MODEL_NAME + \"_fp32\").with_suffix(\".xml\")\n",
    "int8_onnx_path = OUTPUT_DIR / Path(BASE_MODEL_NAME + \"_int8\").with_suffix(\".onnx\")\n",
    "sparse_int8_ir_path = OUTPUT_DIR / Path(BASE_MODEL_NAME + \"_int8\").with_suffix(\".xml\")\n",
    "sparse_int8_ir_folder = OUTPUT_DIR\n",
    "sparse_int8_ir_filename = Path(BASE_MODEL_NAME + \"_sparse_int8\").with_suffix(\".xml\")\n",
    "\n",
    "fp32_pth_url = \"https://storage.openvinotoolkit.org/repositories/nncf/openvino_notebook_ckpts/304_resnet50_fp32.pth\"\n",
    "download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Prepare Tiny ImageNet dataset\n",
    "\n",
    "* 100k images of shape 3x64x64,\n",
    "* 200 different classes: snake, spider, cat, truck, grasshopper, gull, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tiny_imagenet_200(\n",
    "    output_dir: Path,\n",
    "    url: str = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n",
    "    tarname: str = \"tiny-imagenet-200.zip\",\n",
    "):\n",
    "    archive_path = output_dir / tarname\n",
    "    download_file(url, directory=output_dir, filename=tarname)\n",
    "    zip_ref = zipfile.ZipFile(archive_path, \"r\")\n",
    "    zip_ref.extractall(path=output_dir)\n",
    "    zip_ref.close()\n",
    "    print(f\"Successfully downloaded and extracted dataset to: {output_dir}\")\n",
    "\n",
    "\n",
    "def create_validation_dir(dataset_dir: Path):\n",
    "    VALID_DIR = dataset_dir / \"val\"\n",
    "    val_img_dir = VALID_DIR / \"images\"\n",
    "\n",
    "    fp = open(VALID_DIR / \"val_annotations.txt\", \"r\")\n",
    "    data = fp.readlines()\n",
    "\n",
    "    val_img_dict = {}\n",
    "    for line in data:\n",
    "        words = line.split(\"\\t\")\n",
    "        val_img_dict[words[0]] = words[1]\n",
    "    fp.close()\n",
    "\n",
    "    for img, folder in val_img_dict.items():\n",
    "        newpath = val_img_dir / folder\n",
    "        if not newpath.exists():\n",
    "            os.makedirs(newpath)\n",
    "        if (val_img_dir / img).exists():\n",
    "            os.rename(val_img_dir / img, newpath / img)\n",
    "\n",
    "DATASET_DIR = OUTPUT_DIR / \"tiny-imagenet-200\"\n",
    "if not DATASET_DIR.exists():\n",
    "    download_tiny_imagenet_200(OUTPUT_DIR)\n",
    "    create_validation_dir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers classes and functions\n",
    "The code below will help to count accuracy and visualize validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, fmt: str = \":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    \"\"\"Displays the progress of validation process\"\"\"\n",
    "\n",
    "    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = \"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch: int):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches: int):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int] = (1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: Union[torch.nn.Module, CompiledModel]):\n",
    "    \"\"\"Compute the metrics using data from val_loader for the model\"\"\"\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    \n",
    "    # Switch to evaluate mode.\n",
    "    val_dataset, val_loader = create_dataloader(batch_size=1)\n",
    "\n",
    "    if isinstance(model, CompiledModel):\n",
    "        def forward_fun(images, target):\n",
    "            output_layer = model.output(0)\n",
    "            output = model(images)[output_layer]\n",
    "            return (torch.from_numpy(output), target)\n",
    "        \n",
    "    else:\n",
    "        def forward_fun(images, target):\n",
    "            return (model(images.to(device)), target.to(device))\n",
    "    \n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, top1, top5], prefix=\"Test: \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            \n",
    "            # Compute the output.     \n",
    "            output, target = forward_fun(images, target)\n",
    "            \n",
    "            # Measure accuracy and record loss.\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # Measure elapsed time.\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 1000\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(\n",
    "            \" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f} Total time: {total_time:.3f}\".format(top1=top1, top5=top5, total_time=end - start_time)\n",
    "        )\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and load original uncompressed model\n",
    "\n",
    "ResNet-50 from the [torchvision repository](https://github.com/pytorch/vision) is pre-trained on ImageNet with more prediction classes than Tiny ImageNet, so the model is adjusted by swapping the last FC layer to one with fewer output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(model_path: Path):\n",
    "    \"\"\"Creates the ResNet-50 model and loads the pretrained weights\"\"\"\n",
    "    model = resnet50()\n",
    "    # Update the last FC layer for Tiny ImageNet number of classes.\n",
    "    NUM_CLASSES = 200\n",
    "    model.fc = torch.nn.Linear(in_features=2048, out_features=NUM_CLASSES, bias=True)\n",
    "    model.to(device)\n",
    "    if model_path.exists():\n",
    "        checkpoint = torch.load(str(model_path), map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"], strict=True)\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(\"There is no checkpoint to load\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = create_model(MODEL_DIR / fp32_checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "val_dataset,": "torch.utils.data.Subset(val_dataset, range(50)),"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(batch_size: int = 1):\n",
    "    \"\"\"Creates train dataloader that is used for quantization initialization and validation dataloader for computing the model accuracy\"\"\"\n",
    "    val_dir = DATASET_DIR / \"val\" / \"images\"\n",
    "\n",
    "    val_dataset = ImageFolder(\n",
    "        val_dir,\n",
    "        transforms.Compose(\n",
    "            [transforms.Resize(IMAGE_SIZE),\n",
    "             transforms.PILToTensor(),\n",
    "             transforms.ConvertImageDtype(torch.float),\n",
    "             transforms.Normalize(\n",
    "                 mean=[0.485, 0.456, 0.406], \n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return val_dataset, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quantization and benchmarking\n",
    "With the validation pipeline, model files, and data-loading procedures for model calibration now prepared, it's time to proceed with the actual post-training quantization using POT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Evaluate the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc1 = validate(model)\n",
    "print(f\"Test accuracy of FP32 model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create and initialize quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load quantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pot_quantized_model(input_filepath,\n",
    "                                 output_folder,\n",
    "                                 output_filename,\n",
    "                                 sparsity_level=0.0):\n",
    "\n",
    "    print(f\"Sparsifying and Quantizing {input_filepath}\")\n",
    "\n",
    "    model_config = {\n",
    "        \"model_name\": \"model\",\n",
    "        \"model\": str(input_filepath),\n",
    "        \"weights\": str(input_filepath).replace(\"xml\", \"bin\")\n",
    "    }\n",
    "\n",
    "    # Engine config.\n",
    "    engine_config = {\"device\": \"CPU\"}\n",
    "    algorithms = []\n",
    "    \n",
    "    if sparsity_level > 0:\n",
    "        algorithms.append(\n",
    "            {\n",
    "                \"name\": \"WeightSparsity\",\n",
    "                \"params\": {\n",
    "                    \"target_device\": \"CPU\",\n",
    "                    \"sparsity_level\": sparsity_level,\n",
    "                    \"stat_subset_size\": 10000,\n",
    "                    \"ignored_scope\": \"images\",\n",
    "                    \"use_fast_bias\": True\n",
    "                }\n",
    "            })\n",
    "           \n",
    "    algorithms.append({\n",
    "        \"name\": \"DefaultQuantization\",\n",
    "        \"params\": {\n",
    "            \"target_device\": \"CPU\",\n",
    "            \"stat_subset_size\": 10000,\n",
    "            \"preset\": \"performance\",\n",
    "            \"use_fast_bias\": True\n",
    "        },\n",
    "    })\n",
    "           \n",
    "    # Step 1: Implement and create a user data loader.\n",
    "    print(\"Step 1\")\n",
    "    val_dataset, val_dataloader = create_dataloader()\n",
    "    \n",
    "    # Step 2: Load a model.\n",
    "    print(\"Step 2\")\n",
    "    model = load_model(model_config=model_config)\n",
    "\n",
    "    # Step 3: Initialize the engine for metric calculation and statistics collection.\n",
    "    print(\"Step 3\")\n",
    "    engine = IEEngine(config=engine_config, data_loader=val_dataset)\n",
    "    \n",
    "    # Step 4: Create a pipeline of compression algorithms and run it.\n",
    "    print(\"Step 4\")                                    \n",
    "    pipeline = create_pipeline(algorithms, engine)\n",
    "    compressed_model = pipeline.run(model=model)\n",
    "\n",
    "    # Step 5 (Optional): Compress model weights to quantized precision\n",
    "    #                     to reduce the size of the final .bin file.\n",
    "    print(\"Step 5\")\n",
    "    compress_model_weights(compressed_model)\n",
    "\n",
    "    # Step 6: Save the compressed model to the desired path.\n",
    "    # Set save_path to the directory where the model should be saved.\n",
    "    print(\"Step 6\")\n",
    "    save_model(\n",
    "        model=compressed_model,\n",
    "        save_path=output_folder,\n",
    "        model_name=str(output_filename).replace(\".xml\", \"\")\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {output_folder}/{output_filename}\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Convert the models to OpenVINO Intermediate Representation (OpenVINO IR)\n",
    "\n",
    "Use Model Optimizer Python API to convert the Pytorch models to OpenVINO IR. The models will be saved to the 'OUTPUT' directory for latter benchmarking.\n",
    "\n",
    "For more information about Model Optimizer, refer to the [Model Optimizer Developer Guide](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html).\n",
    "\n",
    "Before converting models export them to ONNX. Executing the following command may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, *IMAGE_SIZE).to(device)\n",
    "\n",
    "torch.onnx.export(model, dummy_input, fp32_onnx_path)\n",
    "model_ir = mo.convert_model(input_model=fp32_onnx_path)\n",
    "\n",
    "serialize(model_ir, str(fp32_ir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a sparse (50%), quantized model from the pre-trained `FP32` model and the calibration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_pot_quantized_model(input_filepath=fp32_ir_path, \n",
    "                             output_folder=sparse_int8_ir_folder, \n",
    "                             output_filename=sparse_int8_ir_filename, \n",
    "                             sparsity_level=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate the new model on the validation set after initialization of quantization. The accuracy should be close to the accuracy of the floating-point `FP32` model for a simple case like the one being demonstrated now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()\n",
    "\n",
    "original_model = core.read_model(fp32_ir_path)\n",
    "original_model = core.compile_model(original_model, \"CPU\")\n",
    "\n",
    "acc1 = validate(original_model)\n",
    "print(f\"Test accuracy of FP32 model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_quantized_model = core.read_model(f\"{sparse_int8_ir_folder}/{sparse_int8_ir_filename}\")\n",
    "sparse_quantized_model = core.compile_model(sparse_quantized_model, \"CPU\")\n",
    "\n",
    "acc1 = validate(sparse_quantized_model)\n",
    "print(f\"Accuracy of initialized INT8 model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "Small accuracy drop is observed by inference considerably faster"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-50)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu2_temp_env",
   "language": "python",
   "name": "gpu2_temp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
